{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4bXPClAktmZx8T2FzNe3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8f9cb2b35e44e149548d50431a23b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee377c4143c743d581c4db4d27086f35",
              "IPY_MODEL_068d08ccab304b47a124b82f5115ff7b",
              "IPY_MODEL_30881a608e15467eaa3cfc6ea64acb38"
            ],
            "layout": "IPY_MODEL_e03ca9aa203f4c22bcd4c3a261aca09a"
          }
        },
        "ee377c4143c743d581c4db4d27086f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b354dcfee64d7bb6646a6d98284114",
            "placeholder": "​",
            "style": "IPY_MODEL_7ba1a064b72b4c92a58aa2612f29c042",
            "value": "Downloading shards: 100%"
          }
        },
        "068d08ccab304b47a124b82f5115ff7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_744b8dd978034ffe81df1dbc0194e82f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf239e83cad44d96ab00fc7ce614231a",
            "value": 2
          }
        },
        "30881a608e15467eaa3cfc6ea64acb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d6ab8f7656646c9aa2a250d3453769e",
            "placeholder": "​",
            "style": "IPY_MODEL_8dcafbc1fd6740038647371aba4c607b",
            "value": " 2/2 [01:52&lt;00:00, 49.91s/it]"
          }
        },
        "e03ca9aa203f4c22bcd4c3a261aca09a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62b354dcfee64d7bb6646a6d98284114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba1a064b72b4c92a58aa2612f29c042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "744b8dd978034ffe81df1dbc0194e82f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf239e83cad44d96ab00fc7ce614231a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d6ab8f7656646c9aa2a250d3453769e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dcafbc1fd6740038647371aba4c607b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd47cf7391be4c43b71e5b3fd9938f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a5c98f078984dc4a5ccb459e467a9cd",
              "IPY_MODEL_cad63e67805441fca7af6c051936f2bd",
              "IPY_MODEL_9d791d49d4794628aa8a8c1cc835e11d"
            ],
            "layout": "IPY_MODEL_eff76c7f4fba4f8da19658b5a7b682e0"
          }
        },
        "4a5c98f078984dc4a5ccb459e467a9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43c5c3cd0d8f4a469b1aee6154994abb",
            "placeholder": "​",
            "style": "IPY_MODEL_6432bdf4c2e541e29a12f995f202a5f0",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "cad63e67805441fca7af6c051936f2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b814d97acd74171a9181dada5f3a775",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d51d9e867234ebb991a07f31551ef87",
            "value": 9976576152
          }
        },
        "9d791d49d4794628aa8a8c1cc835e11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817b002cb9a249a58a4a11c9626799e0",
            "placeholder": "​",
            "style": "IPY_MODEL_c25c36b4119b4fdab0656a1356f73c46",
            "value": " 9.98G/9.98G [01:33&lt;00:00, 207MB/s]"
          }
        },
        "eff76c7f4fba4f8da19658b5a7b682e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c5c3cd0d8f4a469b1aee6154994abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6432bdf4c2e541e29a12f995f202a5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b814d97acd74171a9181dada5f3a775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d51d9e867234ebb991a07f31551ef87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "817b002cb9a249a58a4a11c9626799e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c25c36b4119b4fdab0656a1356f73c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c6e9ca76a844285aa773e6f9b02b0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16292ece9b584400b66f151a4523fe6a",
              "IPY_MODEL_1183c292648544babe1b032430db9cbd",
              "IPY_MODEL_a0c4e42b15144aa493dd109f8d394357"
            ],
            "layout": "IPY_MODEL_888eefcdca6544b9ab9f4d7df6a76ea2"
          }
        },
        "16292ece9b584400b66f151a4523fe6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a59094d1da7d4aaeb3ff82d227229c13",
            "placeholder": "​",
            "style": "IPY_MODEL_9dd483b5e5f247d3a47ce4512de7215f",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "1183c292648544babe1b032430db9cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4794d480ad884c88a002c0660d4c8583",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62763a1b14cc4f4781a1baf18de43b94",
            "value": 3500296424
          }
        },
        "a0c4e42b15144aa493dd109f8d394357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bef0b440ba043348c57f2bda035f395",
            "placeholder": "​",
            "style": "IPY_MODEL_312882b5050b4cdda0636a9751016e62",
            "value": " 3.50G/3.50G [00:19&lt;00:00, 220MB/s]"
          }
        },
        "888eefcdca6544b9ab9f4d7df6a76ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a59094d1da7d4aaeb3ff82d227229c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd483b5e5f247d3a47ce4512de7215f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4794d480ad884c88a002c0660d4c8583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62763a1b14cc4f4781a1baf18de43b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bef0b440ba043348c57f2bda035f395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312882b5050b4cdda0636a9751016e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29243d0f59e84502a3e0643446d589f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22abe88b308a4de987423b3ea0fe6128",
              "IPY_MODEL_5286134832a546eb950166f950a94e66",
              "IPY_MODEL_3b52a4ec85114618b64a352b3e41bc1c"
            ],
            "layout": "IPY_MODEL_f04cea22cfa54a17af9760648cf09b2d"
          }
        },
        "22abe88b308a4de987423b3ea0fe6128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0577905a762f4412bb4260bfbd1e0e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_79e7b3cfc63b4c00b7d560987f1f2bb0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5286134832a546eb950166f950a94e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97668a9b92c14c2c8ad6ff622810f25e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_654f62c9118344d793b2f736c1e3fede",
            "value": 2
          }
        },
        "3b52a4ec85114618b64a352b3e41bc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0781aceca69140ed966dab4272b71b1f",
            "placeholder": "​",
            "style": "IPY_MODEL_c2aafdcae82440aeaeae09ebf51fdcef",
            "value": " 2/2 [00:59&lt;00:00, 27.09s/it]"
          }
        },
        "f04cea22cfa54a17af9760648cf09b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0577905a762f4412bb4260bfbd1e0e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e7b3cfc63b4c00b7d560987f1f2bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97668a9b92c14c2c8ad6ff622810f25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654f62c9118344d793b2f736c1e3fede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0781aceca69140ed966dab4272b71b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2aafdcae82440aeaeae09ebf51fdcef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3fc4253802d42ed8ecdf91275ecb715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87ee448b639b43d7a980251c571290be",
              "IPY_MODEL_01fe733dfd254bc7a914a079f2db14cd",
              "IPY_MODEL_541abf8e6a614ec08a919c88d74fe947"
            ],
            "layout": "IPY_MODEL_0f9bb3e5892e4b64807f99c1514540d3"
          }
        },
        "87ee448b639b43d7a980251c571290be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cd0e428be8448d8d012ecd35384de9",
            "placeholder": "​",
            "style": "IPY_MODEL_7536299d59a34c8bb70017b08aae482d",
            "value": "generation_config.json: 100%"
          }
        },
        "01fe733dfd254bc7a914a079f2db14cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b0006d41fb42199873fb3cdddf5145",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0370cff182d4415887e61c47483b640c",
            "value": 188
          }
        },
        "541abf8e6a614ec08a919c88d74fe947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b43a7dc05c4b718f24477011a0f537",
            "placeholder": "​",
            "style": "IPY_MODEL_a980963378294f2aacc905054e702ba9",
            "value": " 188/188 [00:00&lt;00:00, 12.9kB/s]"
          }
        },
        "0f9bb3e5892e4b64807f99c1514540d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98cd0e428be8448d8d012ecd35384de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7536299d59a34c8bb70017b08aae482d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1b0006d41fb42199873fb3cdddf5145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0370cff182d4415887e61c47483b640c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59b43a7dc05c4b718f24477011a0f537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a980963378294f2aacc905054e702ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b22f5ff3da3041c1ae9482e42e2b3bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f377580690646969b500b24ecb37edb",
              "IPY_MODEL_1010ab26744643f4b88a904c0784c941",
              "IPY_MODEL_f0c94cdc0bd547f7b32268344013731c"
            ],
            "layout": "IPY_MODEL_2fa5af096e5f430391e048500c36a35d"
          }
        },
        "1f377580690646969b500b24ecb37edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3119f5598f64701b01af690b235223b",
            "placeholder": "​",
            "style": "IPY_MODEL_7c7ce26c9e044398b657e624c1b6a5d7",
            "value": "modules.json: 100%"
          }
        },
        "1010ab26744643f4b88a904c0784c941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e088fe70ac994d6e94ecfafba101a8ab",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d8d15bd031f4b379335b528f315d2cd",
            "value": 349
          }
        },
        "f0c94cdc0bd547f7b32268344013731c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf1f7129fa943f39996ca6f799b7673",
            "placeholder": "​",
            "style": "IPY_MODEL_7b54a04e6d39450391929b07c3dd365e",
            "value": " 349/349 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "2fa5af096e5f430391e048500c36a35d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3119f5598f64701b01af690b235223b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c7ce26c9e044398b657e624c1b6a5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e088fe70ac994d6e94ecfafba101a8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8d15bd031f4b379335b528f315d2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdf1f7129fa943f39996ca6f799b7673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b54a04e6d39450391929b07c3dd365e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c216ba59549d491b9ceb8a42a50e6bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_138397e453474c20a6553bf238a43024",
              "IPY_MODEL_5df041451eb640ee92533558c952fcf6",
              "IPY_MODEL_dcfb39184c6d43008d836944b0ddad34"
            ],
            "layout": "IPY_MODEL_ddebd729c9074bb695c6a91bcbd578b3"
          }
        },
        "138397e453474c20a6553bf238a43024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eadc72a7756f4b52bdb85ecdcbc2d6ed",
            "placeholder": "​",
            "style": "IPY_MODEL_1a44834fd1b945548ef15839f0ad2aaf",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "5df041451eb640ee92533558c952fcf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63bf699a980643e4851dd69b6f4b0b6b",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7253b9ac1764a058259fb7d99f903ec",
            "value": 116
          }
        },
        "dcfb39184c6d43008d836944b0ddad34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaee2f8634434291a2c92151c577d437",
            "placeholder": "​",
            "style": "IPY_MODEL_7c7c0d1b020b40caabfe2252739c90eb",
            "value": " 116/116 [00:00&lt;00:00, 9.72kB/s]"
          }
        },
        "ddebd729c9074bb695c6a91bcbd578b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eadc72a7756f4b52bdb85ecdcbc2d6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a44834fd1b945548ef15839f0ad2aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63bf699a980643e4851dd69b6f4b0b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7253b9ac1764a058259fb7d99f903ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaee2f8634434291a2c92151c577d437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c7c0d1b020b40caabfe2252739c90eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f36014966226472e898207cc0fb65ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eeac7543c8f94f29827e51595a10b476",
              "IPY_MODEL_7a364f1e8224471eab7108221eb5cd46",
              "IPY_MODEL_8fd0b85371284cb497b1f4e42a3c811f"
            ],
            "layout": "IPY_MODEL_b004847dcb6640849625802a052bad0e"
          }
        },
        "eeac7543c8f94f29827e51595a10b476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ed0e2e780f463bbe35ce84be434398",
            "placeholder": "​",
            "style": "IPY_MODEL_b4cb1adf7273413d9a1dc3cbbdb1baa6",
            "value": "README.md: 100%"
          }
        },
        "7a364f1e8224471eab7108221eb5cd46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba4fdf314404cf290f71f98f33a2c6f",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07c4564089b94a01932223b3fad07d51",
            "value": 10571
          }
        },
        "8fd0b85371284cb497b1f4e42a3c811f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d53c60a032425dbfb6058bf0849fd1",
            "placeholder": "​",
            "style": "IPY_MODEL_3be665357b9f43829e41bd2763a741f7",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 560kB/s]"
          }
        },
        "b004847dcb6640849625802a052bad0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ed0e2e780f463bbe35ce84be434398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4cb1adf7273413d9a1dc3cbbdb1baa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba4fdf314404cf290f71f98f33a2c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c4564089b94a01932223b3fad07d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0d53c60a032425dbfb6058bf0849fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be665357b9f43829e41bd2763a741f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "973478734a784aef912738f43551725e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e226bfc1298444d28456652d9b2efa1a",
              "IPY_MODEL_ca8fbec8e5834421bf44a337b773714b",
              "IPY_MODEL_2d82f65fb3b24293b57bc3d44ef482a2"
            ],
            "layout": "IPY_MODEL_3c7d8ca869e840f8bcaacc7447220f7b"
          }
        },
        "e226bfc1298444d28456652d9b2efa1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_837d061475854fae9fbd57c548c494c4",
            "placeholder": "​",
            "style": "IPY_MODEL_a60a523e684b46c4ad2f81313d0b2640",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "ca8fbec8e5834421bf44a337b773714b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea0c97700143494abb4d88a0b19294fb",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ad1479208f848e2b4a46b21b596e377",
            "value": 53
          }
        },
        "2d82f65fb3b24293b57bc3d44ef482a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e462cbe418476bbb677f02a4e4dd78",
            "placeholder": "​",
            "style": "IPY_MODEL_7246c70d878b4048a3d63b775a2a54e9",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.58kB/s]"
          }
        },
        "3c7d8ca869e840f8bcaacc7447220f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837d061475854fae9fbd57c548c494c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60a523e684b46c4ad2f81313d0b2640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea0c97700143494abb4d88a0b19294fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad1479208f848e2b4a46b21b596e377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6e462cbe418476bbb677f02a4e4dd78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7246c70d878b4048a3d63b775a2a54e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "270ea055b2fb4848ab63a62e3a85392a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fabac93ec5fc460790dbaa33b2122717",
              "IPY_MODEL_1580ec29b0a84bd4914ad893c87a5353",
              "IPY_MODEL_8e93eba9bfd34f85bb29609a85b49174"
            ],
            "layout": "IPY_MODEL_087f2d29ac984a678eb4a34921e4f329"
          }
        },
        "fabac93ec5fc460790dbaa33b2122717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edb91ae58b6f4c2b9e95e3ab38b0c19b",
            "placeholder": "​",
            "style": "IPY_MODEL_c7459b927f40463fbd5283a8acd4dc69",
            "value": "config.json: 100%"
          }
        },
        "1580ec29b0a84bd4914ad893c87a5353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_111bc504de2042b499b8a6c76c6d2852",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c52292951dd44354a375da554a492977",
            "value": 571
          }
        },
        "8e93eba9bfd34f85bb29609a85b49174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3460908596d4292bfd90dad5e41dee5",
            "placeholder": "​",
            "style": "IPY_MODEL_a198c4d4c2364b2f85c1cb332f90d948",
            "value": " 571/571 [00:00&lt;00:00, 42.3kB/s]"
          }
        },
        "087f2d29ac984a678eb4a34921e4f329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb91ae58b6f4c2b9e95e3ab38b0c19b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7459b927f40463fbd5283a8acd4dc69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "111bc504de2042b499b8a6c76c6d2852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52292951dd44354a375da554a492977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3460908596d4292bfd90dad5e41dee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a198c4d4c2364b2f85c1cb332f90d948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6afb567b09aa4c19864565ba2ccf0fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4172cf7346114239ae2fa50972fc6c33",
              "IPY_MODEL_ea21e8e9841c48e4bbafbbd041a21d33",
              "IPY_MODEL_0ef976de84914e548a7d2e059017234d"
            ],
            "layout": "IPY_MODEL_bc495d0b824c4045be489eb031d31dde"
          }
        },
        "4172cf7346114239ae2fa50972fc6c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab46b43926b4b049c4c7b245c652cbd",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8ecfadcdce4e968aae7b3c15af1474",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ea21e8e9841c48e4bbafbbd041a21d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e4ba885a4614efaa9862fc0d46dd17f",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff9e44e684804c5cae0aef1de23af464",
            "value": 438011953
          }
        },
        "0ef976de84914e548a7d2e059017234d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3312663d1f945a990039420f05771fc",
            "placeholder": "​",
            "style": "IPY_MODEL_65109c01e55b4c19b8395f92abebde6e",
            "value": " 438M/438M [00:03&lt;00:00, 153MB/s]"
          }
        },
        "bc495d0b824c4045be489eb031d31dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ab46b43926b4b049c4c7b245c652cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8ecfadcdce4e968aae7b3c15af1474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e4ba885a4614efaa9862fc0d46dd17f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9e44e684804c5cae0aef1de23af464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3312663d1f945a990039420f05771fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65109c01e55b4c19b8395f92abebde6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fdb571d5cf14219857669bc8cff59ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2794bc9bc5f4a96842d528f606bfa73",
              "IPY_MODEL_20b551b9cd41492cade95e1b9e0074cb",
              "IPY_MODEL_530cd26d2cb04ad09d5d47eb413dc8e3"
            ],
            "layout": "IPY_MODEL_1e189a54e5124782a8365ff2afc0b1ca"
          }
        },
        "c2794bc9bc5f4a96842d528f606bfa73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_960201e740f845debc988c5ce2d74629",
            "placeholder": "​",
            "style": "IPY_MODEL_d52022dbe13f4f6dbb0f48f501ce6a76",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "20b551b9cd41492cade95e1b9e0074cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95adb1ea43624144b160dd3a864a8cc7",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_810ab011bb8641cf8e3c86bef3605b8f",
            "value": 363
          }
        },
        "530cd26d2cb04ad09d5d47eb413dc8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88eef6b529cd4578a37c1d19161f4ca7",
            "placeholder": "​",
            "style": "IPY_MODEL_58cfe383e88c4f5eb12612c280472e46",
            "value": " 363/363 [00:00&lt;00:00, 14.6kB/s]"
          }
        },
        "1e189a54e5124782a8365ff2afc0b1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "960201e740f845debc988c5ce2d74629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52022dbe13f4f6dbb0f48f501ce6a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95adb1ea43624144b160dd3a864a8cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "810ab011bb8641cf8e3c86bef3605b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88eef6b529cd4578a37c1d19161f4ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58cfe383e88c4f5eb12612c280472e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fb4b330c6a745a48acf4da1c141e1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4612d84e35c42719139e4f008c5d615",
              "IPY_MODEL_81ba92c0a7084ff983371dabf58296cb",
              "IPY_MODEL_3d3da872a21d47b387d9157472c1539e"
            ],
            "layout": "IPY_MODEL_39abb4598e614585b028d8a1bd47fba6"
          }
        },
        "d4612d84e35c42719139e4f008c5d615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30b7851b24704cc6b1bdb0b3c1e5ef73",
            "placeholder": "​",
            "style": "IPY_MODEL_2355b8a45eab4db0ac11c4044c20c29f",
            "value": "vocab.txt: 100%"
          }
        },
        "81ba92c0a7084ff983371dabf58296cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ce683e6a2ca4dbb898b2c32ea924cfc",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5731bc50a9114cca95f6acfef67c97c1",
            "value": 231536
          }
        },
        "3d3da872a21d47b387d9157472c1539e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df4d214f733845dbb1f3bd3bbe08e8ca",
            "placeholder": "​",
            "style": "IPY_MODEL_a5a1e570e1c844d5a5a9d61ddd9c624a",
            "value": " 232k/232k [00:00&lt;00:00, 3.72MB/s]"
          }
        },
        "39abb4598e614585b028d8a1bd47fba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b7851b24704cc6b1bdb0b3c1e5ef73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2355b8a45eab4db0ac11c4044c20c29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ce683e6a2ca4dbb898b2c32ea924cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5731bc50a9114cca95f6acfef67c97c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df4d214f733845dbb1f3bd3bbe08e8ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a1e570e1c844d5a5a9d61ddd9c624a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7817415f639d493683fb8247f87da3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6aefadaa9254db586847458f89b39b3",
              "IPY_MODEL_48335256df304f75b1b4d3e93329b09a",
              "IPY_MODEL_79e007c3ff5d4af8969bc9ce9c616e2a"
            ],
            "layout": "IPY_MODEL_56ec0c13ffa9446a88e8c78e6f175689"
          }
        },
        "b6aefadaa9254db586847458f89b39b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_278dc7d1557846ada54a411c5c0a4c53",
            "placeholder": "​",
            "style": "IPY_MODEL_9303acd941f7402e85b487a9f9ccdfe4",
            "value": "tokenizer.json: 100%"
          }
        },
        "48335256df304f75b1b4d3e93329b09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_201a6043ae004758818f1186f1d2a18e",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dc9a157b7004c478255a2a5be64c959",
            "value": 466021
          }
        },
        "79e007c3ff5d4af8969bc9ce9c616e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba8176d60074d2cbf2e122cda888013",
            "placeholder": "​",
            "style": "IPY_MODEL_5edbcca9151e402b99e9a6b0ae70ec4f",
            "value": " 466k/466k [00:00&lt;00:00, 3.95MB/s]"
          }
        },
        "56ec0c13ffa9446a88e8c78e6f175689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278dc7d1557846ada54a411c5c0a4c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9303acd941f7402e85b487a9f9ccdfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "201a6043ae004758818f1186f1d2a18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc9a157b7004c478255a2a5be64c959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fba8176d60074d2cbf2e122cda888013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5edbcca9151e402b99e9a6b0ae70ec4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72bc32c5075540d788e55cbc187aeeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b179c0f498343628e4535ab4714f6ac",
              "IPY_MODEL_c810d2cfbd964193a42272d82be9805f",
              "IPY_MODEL_2a75ea8d37b842c1b03a9f44903ddb78"
            ],
            "layout": "IPY_MODEL_4b77186c49e84bb1bbc4ab25a74c9f8d"
          }
        },
        "0b179c0f498343628e4535ab4714f6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5568148f20d84882b25e46d460fac08f",
            "placeholder": "​",
            "style": "IPY_MODEL_61923eac48fc4375b9b8e42c26d5c4ca",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c810d2cfbd964193a42272d82be9805f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b0cea596254a43b8e0bb0b62ac7bbe",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_066d3a82e24b4e7aa32ef05502b36145",
            "value": 239
          }
        },
        "2a75ea8d37b842c1b03a9f44903ddb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28d69525baab4b04bc78e350a99ba3fb",
            "placeholder": "​",
            "style": "IPY_MODEL_a5b67502488a4a4bac9c67f4f7097417",
            "value": " 239/239 [00:00&lt;00:00, 12.5kB/s]"
          }
        },
        "4b77186c49e84bb1bbc4ab25a74c9f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5568148f20d84882b25e46d460fac08f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61923eac48fc4375b9b8e42c26d5c4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7b0cea596254a43b8e0bb0b62ac7bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066d3a82e24b4e7aa32ef05502b36145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28d69525baab4b04bc78e350a99ba3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5b67502488a4a4bac9c67f4f7097417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f656eeada4b48979c66ef417b66f434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e68ee4798b81440b810c9b95343b5ee2",
              "IPY_MODEL_29c9784e30d9411896b951b680fa4078",
              "IPY_MODEL_6eae7883f179433bae03cac7e1f2f6ac"
            ],
            "layout": "IPY_MODEL_f3c8ff9ca9004f41b97e742239441528"
          }
        },
        "e68ee4798b81440b810c9b95343b5ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4d0d8fefdd4508a1ca2fd0bd01ba85",
            "placeholder": "​",
            "style": "IPY_MODEL_b482a9bde74c431f9733e21eb66636e1",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "29c9784e30d9411896b951b680fa4078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff559e7feeb4e99a3726022d09caea6",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff441e52ba0f4a5a87264f2b1b60a034",
            "value": 190
          }
        },
        "6eae7883f179433bae03cac7e1f2f6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53b626023ac49e481433db38b53c8d8",
            "placeholder": "​",
            "style": "IPY_MODEL_32494e3ad96344739e6dfc41b18dd100",
            "value": " 190/190 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "f3c8ff9ca9004f41b97e742239441528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4d0d8fefdd4508a1ca2fd0bd01ba85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b482a9bde74c431f9733e21eb66636e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff559e7feeb4e99a3726022d09caea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff441e52ba0f4a5a87264f2b1b60a034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c53b626023ac49e481433db38b53c8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32494e3ad96344739e6dfc41b18dd100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BergerPerkins/LlaMA/blob/main/Llama2_with_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG System Using Llama2 With Hugging Face**"
      ],
      "metadata": {
        "id": "g4HOvywhb7eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLxceAgPb-kf",
        "outputId": "4b108434-3efd-42d4-bcf9-6c1bc640db7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw3M5gjnb_76",
        "outputId": "0eba86c9-6189-495a-c80f-b042f1682619"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding\n",
        "!pip install install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA2XuaNVcJtH",
        "outputId": "3af30520-f397-4d58-a725-d58ec794e621"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfcKgJAQcU7F",
        "outputId": "942e82a6-c468-454d-a557-0f1f3826429b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.9.40-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.6.3)\n",
            "Collecting deprecated>=1.2.9.3 (from llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama_index)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Collecting openai>=1.1.0 (from llama_index)\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama_index)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama_index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
            "Collecting typing-extensions>=4.5.0 (from llama_index)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->llama_index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (3.6)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Installing collected packages: dirtyjson, typing-extensions, h11, deprecated, tiktoken, httpcore, httpx, openai, llama_index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 llama_index-0.9.40 openai-1.10.0 tiktoken-0.5.2 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "OqOcCypJcVpL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Gvu1p7dO3e",
        "outputId": "4e3c7ed2-396f-42f3-bd27-3a6145d28d7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='0c5f9cd2-3a9d-4c39-99e1-b08d581b4b61', embedding=None, metadata={'page_label': '1', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Citation: Yang, H.; Sun, Z.; Liu, J.;\\nZhang, Z.; Zhang, X. The\\nDevelopment of Rubber Tapping\\nMachines in Intelligent Agriculture:\\nA Review. Appl. Sci. 2022 ,12, 9304.\\nhttps://doi.org/10.3390/\\napp12189304\\nAcademic Editors: Claudio De\\nPasquale and Natalija Topi´ c Popovi´ c\\nReceived: 30 July 2022\\nAccepted: 11 September 2022\\nPublished: 16 September 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afﬁl-\\niations.\\nCopyright: © 2022 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\napplied  \\nsciences \\nReview\\nThe Development of Rubber Tapping Machines in Intelligent\\nAgriculture: A Review\\nHui Yang1\\n, Zejin Sun1\\n, Junxiao Liu1,2, Zhifu Zhang1\\nand Xirui Zhang1,2,*\\n1Mechanical and Electrical Engineering College, Hainan University, Haikou 570228, China\\n2Sanya Nanfan Research Institute, Hainan University, Sanya 572025, China\\n*Correspondence: zhangxr@hainanu.edu.cn\\nAbstract: In the past decade, intelligent technologies have advanced rapidly, particularly due to\\nimprovements in automatic control, which have had a signiﬁcant impact on forestry, as well as animal\\nhusbandry and the future of farm management. However, the degree of production and management\\nmechanization in natural rubber plantations is low, especially since the methods of tapping still rely\\nheavily on labor. The decrease of skilled rubber tappers and the increase in labor costs have led to\\nthe development of the mechanization of rubber tapping operations. The application of emerging\\nintelligent agricultural technologies could provide an alternative in order to maximize the potential\\nproductivity of natural rubber. Based on this vision, we reviewed the literature on rubber tapping\\nfrom the past decade for system implementation in rubber plantations. In this review, selected\\nreferences on rubber tapping were categorized into several directions of research, including rubber\\ntapping machines, the key technologies applied in tapping operations, and some related protective\\nresearch, analyzing research works from 2010 to 2022 that focused on tapping methods. The review\\nalso discusses the application of intelligent agricultural technologies, such as the recognition of\\ntapping trajectory and tapping path planning. A summary of challenges and future trends is also\\nprovided in this study. Based on the relevant research, the use of intelligent technologies in rubber\\ntapping machines is still in its initial stage and has broad prospects. Through this study, we aim to\\nprovide a reference for researchers in the ﬁeld of rubber tapping machines and thus to play a positive\\nrole in future rubber tapping.\\nKeywords: rubber plantations; natural rubber; rubber tapping machine; automatic control;\\nintelligent agriculture\\n1. Introduction\\nNatural rubber is an important industrial raw material and strategic resource, the\\nproducts of which are important in the ﬁelds of transportation, national defense, and\\nthe military industry [ 1]. The excellent resilience, electrical insulation properties, wear\\nresistance, airtightness, and ﬂexibility of natural rubber make it widely used in people’s\\ndaily life, in the areas of medicine, hygiene, and so on. There are two types of rubber\\ncurrently used in industry [ 2]. One is natural rubber, produced by natural plants, and the\\nother is synthetic rubber, synthesized by chemical processing. Driven by industrialization,\\nthe use of synthetic rubber has developed rapidly, accounting for more than half of the\\ntotal production capacity in the world [ 3]. Synthetic rubber is superior to natural rubber in\\nterms of some of its special properties, such as chemical resistance and grease resistance [ 4].\\nHowever, the high impact resistance, puncture resistance, and tear resistance of natural\\nrubber are unmatched by synthetic rubber. This facilitates metal bonding, which makes it\\nirreplaceable, especially for aircraft tires and truck tires used in service under complex load\\nconditions [ 5]. For instance, the manufacturing of truck radial tires requires 50% natural\\nrubber, off-the-road (OTR) tires need to be made of 90% natural rubber, and aero tires need\\nAppl. Sci. 2022 ,12, 9304. https://doi.org/10.3390/app12189304 https://www.mdpi.com/journal/applsci', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86f75cd6-6b31-4c56-a3c8-b3b37739b2b3', embedding=None, metadata={'page_label': '2', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 2 of 26\\nto be made of 100% natural rubber. According to statistics, there are more than 70,000 types\\nof products around the world that are made from natural rubber.\\nAs shown in Figure 1, natural rubber is obtained from the laticiferous vessels of\\nthe rubber tree via regular tapping [ 6]. Tapping is the process of creating a path that\\nmust lie 1.2–2.0 mm and at an appropriate angle above the cambium, which is the tissue\\nbetween the wood and laticiferous vessels [ 7]. The thickness consumed in the vertical\\ndirection during each tapping is deﬁned as the bark consumption, and this is required to be\\n1.4–2.1 mm [8] . Thus, it is a skill-oriented job [ 9]. During 2:00–6:00 a.m., the latex output is\\nhigher due to the expansion pressure reaching its highest point, so this operation is usually\\nperformed in the early morning [ 10–12]. Tapping operations may also lead to latex allergy\\nand occupational disease [ 13–16]. With the combination of the development of urbanization\\nand industrialization, as well as great skill requirements and high labor intensity of manual\\ntapping, the problem of dwindling rubber farmers has become increasingly prominent [ 17].\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 2 of 26 \\n \\nrequires 50% natural rubber, off-the-road (OTR ) tires need to be made of 90% natural rub-\\nber, and aero tires need to be made of 100%  natural rubber. According to statistics, there \\nare more than 70,000 types of products around the world that are made from natural rub-ber. \\nAs shown in Figure 1, natural rubber is obta ined from the laticiferous vessels of the \\nrubber tree via regular tapping [6]. Tapping is the process of creating a path that must lie \\n1.2–2.0 mm and at an appropriate angle above the cambium, which is the tissue between \\nthe wood and laticiferous vessels [7]. The th ickness consumed in the vertical direction \\nduring each tapping is defined as the bark cons umption, and this is required to be 1.4–2.1 \\nmm [8]. Thus, it is a skill-oriented job [9]. During 2:00–6:00 a.m., the latex output is higher \\ndue to the expansion pressure reaching its highest point, so this operation is usually per-formed in the early morning [10–12]. Tapping operations may also lead to latex allergy \\nand occupational disease [13–16]. With the combination of the development of urbaniza-\\ntion and industrialization, as well as great skill requirements and high labor intensity of \\nmanual tapping, the problem of dwindling ru bber farmers has become increasingly prom-\\ninent [17]. \\n \\nFigure 1. Partial views of a rubber tree trunk [18]. \\nWith the aim of responding to the above problems, emerging information technolo-\\ngies are important in transforming traditional farming methods into a new type of preci-\\nsion agriculture [19–23]. For example, CIHEVEA Technology Company Limited, Ningbo, China put forward an automatic intelligent ru bber tapping system [24]. This system sends \\ntapping instructions from a remote-controlled terminal and receives data signals through \\na secondary base station. Then the base statio n in the field transmits instructions to the \\nmachines to complete the operation of rubber tapping. This example preliminarily reveals \\nthe application of agricultural information technologies in the production management of rubber plantations. Nevertheless, a critical limitation of the system is the inconvenience \\nof the maintenance process, due to the need to  connect each rubber tree in the orchard to \\na machine. Thus, improving the automation and intelligence level of rubber tapping ma-\\nchines plays an important role in increa sing the output of natural rubber [25]. \\nResearch in the area of rubber tapping has followed several avenues. Early work by \\nMcMullen [26], Ligia Regina et al. [27], and Ramachandran  et al. [28] was concerned with \\nthe effect of tapping frequency and tapping en vironment (the living tree under completely \\naseptic conditions and in the absence of oxyge n) on rubber yield, based on biology (symp-\\ntoms of bark scaling, cracking, drying, necrotic streaking, and browning of the internal \\nbark, leading to the decay of internal tissues). Ru et al. [29] and Wang et al. [30] found that \\ntapping skills and tapping machinery were key factors influencing the low efficiency of \\nlabor tapping. In actual experiments, mechanical tapping can help to resolve the problems related to development. The use of an electrical tapping knife was researched at the end \\nof the 1970s. Walaiporn et al. [31] and Chantuma et al. [32] optimized the mechanical de-\\nvice of the rubber tapping knife, including designs of a manual tapping knife and an \\nFigure 1. Partial views of a rubber tree trunk [18].\\nWith the aim of responding to the above problems, emerging information technologies\\nare important in transforming traditional farming methods into a new type of precision\\nagriculture [ 19–23]. For example, CIHEVEA Technology Company Limited, Ningbo, China\\nput forward an automatic intelligent rubber tapping system [ 24]. This system sends\\ntapping instructions from a remote-controlled terminal and receives data signals through\\na secondary base station. Then the base station in the ﬁeld transmits instructions to the\\nmachines to complete the operation of rubber tapping. This example preliminarily reveals\\nthe application of agricultural information technologies in the production management of\\nrubber plantations. Nevertheless, a critical limitation of the system is the inconvenience\\nof the maintenance process, due to the need to connect each rubber tree in the orchard\\nto a machine. Thus, improving the automation and intelligence level of rubber tapping\\nmachines plays an important role in increasing the output of natural rubber [25].\\nResearch in the area of rubber tapping has followed several avenues. Early work by\\nMcMullen [ 26], Ligia Regina et al. [ 27], and Ramachandran et al. [ 28] was concerned\\nwith the effect of tapping frequency and tapping environment (the living tree under\\ncompletely aseptic conditions and in the absence of oxygen) on rubber yield, based on\\nbiology (symptoms of bark scaling, cracking, drying, necrotic streaking, and browning of\\nthe internal bark, leading to the decay of internal tissues). Ru et al. [ 29] and Wang et al. [ 30]\\nfound that tapping skills and tapping machinery were key factors inﬂuencing the low\\nefﬁciency of labor tapping. In actual experiments, mechanical tapping can help to resolve\\nthe problems related to development. The use of an electrical tapping knife was researched\\nat the end of the 1970s. Walaiporn et al. [ 31] and Chantuma et al. [ 32] optimized the\\nmechanical device of the rubber tapping knife, including designs of a manual tapping\\nknife and an electrical tapping knife, setting the depth and controlling the thickness to\\nrelieve the symptoms of carpal tunnel syndrome among rubber tappers and improve', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16d61e65-7e49-4619-8b37-629f67de32c4', embedding=None, metadata={'page_label': '3', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 3 of 26\\nproduction efﬁciency. The main advantages of the knives are their small volume, light\\nweight, low power, and ease of operation. The types of electric tapping knives designed\\ninclude translational cutting, rotary cutting, and intelligent manual cutting knives. With\\nautomation constraints, these knives have been suggested to be used as an auxiliary tool.\\nIn addition, works by Cheng [33] and Zhou [18] deal with the design of intelligent rubber\\ntapping technology evaluation equipment based on automatic tapping, navigation, and\\nremote control. The emergence of this kind of self-propelled rubber tapping robot means\\nthe beginning of the liberation of labor for rubber tappers.\\nThe application of intelligent agriculture in rubber tapping is still in the early stages of\\ndevelopment, whereas a considerable amount of academic research has accumulated in crop\\nplanting [ 34–37], orchard harvesting [ 38–40], and ﬁeld management [ 41–45], suggesting\\ncomplex multi-dimensional impacts of intelligent agriculture. Moreover, the number of\\nreferences on rubber plantations (including harvesting and protection) is very limited at\\npresent. A comprehensive review of the relevant literature will be especially helpful in\\nsynthesizing the key research insights and unveiling major research trends in this ﬁeld.\\nWe thus hope to help new researchers to grasp the current state of the art by summarizing\\nthe articles available on rubber tapping machinery research. The overall contributions of\\nthis study are set out as follows. We have (1) summarized the articles covering the design\\nand experiments concerning rubber tapping machines from 2010 to 2022; (2) discussed the\\nadvantages and disadvantages of tapping machines based on key technical indicators; (3)\\nexpounded upon the automatic tapping technologies applied in self-propelled robots to\\ncontribute to further research in the area of rubber planting.\\nTo acquire a better understanding of the trends related to rubber tapping machines,\\nin this paper a brief overview of the status study and development is provided ﬁrst.\\nThen the key relevant skills in relation to these machines are introduced (especially for\\nthe operation of self-propelled rubber tapping robots), including tapping technologies,\\nthe recognition of trees and tapping lines, and the perception of obstacle information in\\nrubber plantations. Finally, the ﬁndings of the paper are summarized, followed by the\\ndevelopment of future prospects.\\n2. Materials and Methods\\n2.1. Search Strategy\\nThe strategy used to search for relevant articles was important in this review. In\\nthis study, we followed the Preferred Reporting Items for Systematic Reviews and\\nMeta-Analyses (PRISMA) guidelines [ 46] and referred to the systematic literature\\nreview method [ 47]. We designed a search string and used the following databases as\\nour key resources: IEEE Xplore, MDPI, Web of Science, Engineering Village, Springer, and\\nScienceDirect (as shown in Figure 2). All searches were conducted on 28 August 2022,\\nusing the following Boolean string: “rubber tapping robot” AND “rubber tree protection”\\nOR “tapping machine” based on the titles, abstracts, and keywords. In this study, we used\\nvarious search term combinations according to the criteria or limitations of each database\\n(Table 1). No geographical restrictions were applied to the identiﬁcation process, and the\\nsearch period in the databases was from 1 January 2010 to 28 August 2022. We chose\\nappropriate articles based on the following criteria: (i) closely related to the main idea of\\nrubber tapping machinery and protective work, (ii) not a book, and (iii) not a report. Only\\npeer-reviewed journal articles, conference articles, and dissertations were included.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56e2ea94-2ad5-446c-a496-8bfd05ef5284', embedding=None, metadata={'page_label': '4', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 4 of 26\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 4 of 26 \\n \\n \\nFigure 2. Literature selection methodology, including searching scope, select ion criteria, and the \\nnumber of articles included and excluded. \\nTable 1. The search strategies.  \\nDatabase Search Terms \\nIEEE Xplore Title, abstracts: “Abstract”: rubbe r tree protection OR rubber tap-\\nping AND “Document Title”: rubber tree protection OR rubber tapping. \\nMDPI Title, keywords: rubber tapping OR rubber tree protection. \\nWeb of Science Title, abstracts: (((TS = (rubber tapping (robot OR machine))) OR \\nTS = (rubber tree protection)) OR TI = (rubber tapping (robot OR machine))) OR TI = (rubber tree protection). \\nEngineering Village Title, abstracts, keywords: (((rubber tapping) WN KY) OR ((tap-\\nping machine) OR (rubber tree protection) WN KY)) AND(({ja} OR {ca} OR {cp} OR {ds}) WN DT)). \\nScienceDirect Title, abstract or author-speci fied keywords: “tapping machine” \\nOR “rubber protection”. \\nSpringer Find resources: “rubber tappin g” AND (machine OR technique \\nOR robot) OR “rubber tree protection” within (Article AND Chapter and Conference Paper). \\n2.2. Search Criteria \\nThe full text of each paper that met the above criteria was considered in the decision \\nregarding whether to exclude it according to its relevance to the following detailed crite-\\nria: rubber tapping operations, including ta pping knives, tapping machines, tapping ro-\\nbots, related protection research, recognition of tapping paths, and navigation in rubber \\nplantations. After the final selection, 50 articles were included in this review, and their \\nfindings were then cited and analyzed. We in clude a brief summary and categorization of \\nthe contents of the included research in Table A1 of Appendix A, including (i) protective \\nresearch to increase production in the traditional manual tapping period, (ii) the design \\nof portable electrical tapping devices, (iii) research on fixed tapping machines, and (iv) \\nstudies on self-propelled tapping robots. \\n  \\nFigure 2. Literature selection methodology, including searching scope, selection criteria, and the\\nnumber of articles included and excluded.\\nTable 1. The search strategies.\\nDatabase Search Terms\\nIEEE XploreTitle, abstracts: “Abstract”: rubber tree protection OR rubber\\ntapping AND “Document Title”: rubber tree protection OR\\nrubber tapping.\\nMDPI Title, keywords: rubber tapping OR rubber tree protection.\\nWeb of ScienceTitle, abstracts: (((TS = (rubber tapping (robot OR machine))) OR\\nTS = (rubber tree protection)) OR TI = (rubber tapping (robot OR\\nmachine))) OR TI = (rubber tree protection).\\nEngineering VillageTitle, abstracts, keywords: (((rubber tapping) WN KY) OR\\n((tapping machine) OR (rubber tree protection) WN KY))\\nAND(({ja} OR {ca} OR {cp} OR {ds}) WN DT)).\\nScienceDirectTitle, abstract or author-speciﬁed keywords: “tapping machine”\\nOR “rubber protection”.\\nSpringerFind resources: “rubber tapping” AND (machine OR technique\\nOR robot) OR “rubber tree protection” within (Article AND\\nChapter and Conference Paper).\\n2.2. Search Criteria\\nThe full text of each paper that met the above criteria was considered in the decision\\nregarding whether to exclude it according to its relevance to the following detailed cri-\\nteria: rubber tapping operations, including tapping knives, tapping machines, tapping\\nrobots, related protection research, recognition of tapping paths, and navigation in rubber\\nplantations. After the ﬁnal selection, 50 articles were included in this review, and their\\nﬁndings were then cited and analyzed. We include a brief summary and categorization of\\nthe contents of the included research in Table A1 of Appendix A, including (i) protective', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8cd63c07-11e9-47f1-b045-8d10e76f0f05', embedding=None, metadata={'page_label': '5', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 5 of 26\\nresearch to increase production in the traditional manual tapping period, (ii) the design of\\nportable electrical tapping devices, (iii) research on ﬁxed tapping machines, and (iv) studies\\non self-propelled tapping robots.\\n2.3. Data Extraction\\nInformation about the research in the 50 articles was extracted, including articles\\non (i) rubber tapping machines, (ii) intelligent agricultural technologies (including the\\nrecognition of tapping paths, navigation in the orchard, and tapping methods for the\\nprotection of rubber trees), and some (iii) related tapping studies, such as the inﬂuence\\nof the external environment on the production of natural rubber. Out of the ﬁnal papers\\nincluded for review, the distribution of studies published between 2011–2022 is visualized\\nin Figure 3. The year 2018 and 2019 had the highest numbers of papers, and the number\\nof papers showed an increasing trend year by year. According to the data compiled in\\nTable A1, the development of rubber tapping machines and the key skills used in operations\\nare shown in Figure 4. Due to the use of manual tapping methods before 2016, the articles\\nfrom this period were based on the analysis of occupational diseases caused by long periods\\nof wrist work and how to improve latex production from a pathological perspective. Then,\\na series of portable electric tapping knives and ﬁxed tapping machines were designed in\\norder to relieve wrist fatigue and improve the efﬁciency of tapping. The topic of how to\\nnavigate rubber plantations was proposed at a conference in 2010. Nevertheless, research\\npapers on how to recognize the position of the tapping trajectory, track its preconcerted\\npath, and ﬁnish motion planning did not appear until 2018. At the same time, the structural\\ndesign of the manipulation and motion control of multi-DOF (multi-degree of freedom)\\nsystems were realized, which led to the application of self-propelled rubber tapping robots\\nin recent years.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 5 of 26 \\n \\n2.3. Data Extraction \\nInformation about the research in the 50 arti cles was extracted, including articles on \\n(i) rubber tapping machines, (ii) intelligent agricultural technologies (including the recog-nition of tapping paths, navigation in the or chard, and tapping methods for the protection \\nof rubber trees), and some (iii) related tapping studies, such as the influence of the external \\nenvironment on the production of natural ru bber. Out of the final papers included for \\nreview, the distribution of studies published between 2011–2022 is visualized in Figure 3. \\nThe year 2018 and 2019 had the highest numb ers of papers, and the number of papers \\nshowed an increasing trend year by year. According to the data compiled in Table A1, the \\ndevelopment of rubber tapping machines and the key skills used in operations are shown \\nin Figure 4. Due to the use of manual tapping methods before 2016, the articles from this period were based on the analys is of occupational diseases caused by long periods of wrist \\nwork and how to improve latex production from  a pathological perspective. Then, a series \\nof portable electric tapping knives and fixed tapping machines were designed in order to \\nrelieve wrist fatigue and improve the efficiency  of tapping. The topic of how to navigate \\nrubber plantations was proposed at a conferen ce in 2010. Nevertheless, research papers \\non how to recognize the position of the tapping trajectory, track its preconcerted path, and \\nfinish motion planning did not appear until 2018. At the same time, the structural design \\nof the manipulation and motion control of multi-DOF (multi-degree of freedom) systems \\nwere realized, which led to the application of  self-propelled rubber ta pping robots in re-\\ncent years. \\n \\nFigure 3. Distribution of studies in terms of their year of publication (2011–2022). \\nFigure 3. Distribution of studies in terms of their year of publication (2011–2022).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e2f9c313-6764-406b-b3fc-d91f4f9c8163', embedding=None, metadata={'page_label': '6', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 6 of 26\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 6 of 26 \\n \\n \\nFigure 4. Diagram of the development of rubber tapping  machines and the application of related \\nintelligent technologies according to the research references. \\n3. Development of Rubber Tapping Machines \\n3.1. Manual Tapping Machine \\nAccording to the development stage, two methods of obtaining natural rubber ap-\\npeared first. One is the method of tradit ional manual tapping. Th e other one is micro-\\ntapping with gas stimulation, which involves the use of chemical stimulants to prolong \\nthe discharge time of laticifer on the line of tapping [48,49]. This method has not been \\nwidely applied because there are still some prob lems such as air leakage, bark coarsening, \\nand secant inside shrinking. Therefore, in this section we mainly focus on the former \\nmethod. \\nThe designs of traditional tapping knives, as shown in Figure 5a, include the pull-\\ntype tapping knife and the push-type tapping knife. The use of these knives has the ad-\\nvantage of being low cost and ease of acceptance by workers. However, there are many shortcomings associated with this kind of tapping method. For example, the tool bit can-\\nnot be replaced. In addition, it is difficult to control the quantity and shape of a single \\nconsumption, which has seriously affected the economic benefits of rubber industries [50]. \\nA portable rubber tapping machine is a hand -held device, which can be easily deployed \\nin plantations. As shown in Figure 5b, the use of the \\n4GXJ  (Rubber Research Institute, Chi-\\nnese Academy of Tropical Agricultural Science)  electrical tapping device, a kind of port-\\nable electrical tapping device driven by electr icity, can decrease the labor intensity of rub-\\nber workers [51]. Because of its poor levels of automation, it is preferable to use it as an \\nFigure 4. Diagram of the development of rubber tapping machines and the application of related\\nintelligent technologies according to the research references.\\n3. Development of Rubber Tapping Machines\\n3.1. Manual Tapping Machine\\nAccording to the development stage, two methods of obtaining natural rubber ap-\\npeared ﬁrst. One is the method of traditional manual tapping. The other one is micro-\\ntapping with gas stimulation, which involves the use of chemical stimulants to prolong the\\ndischarge time of laticifer on the line of tapping [ 48,49]. This method has not been widely\\napplied because there are still some problems such as air leakage, bark coarsening, and\\nsecant inside shrinking. Therefore, in this section we mainly focus on the former method.\\nThe designs of traditional tapping knives, as shown in Figure 5a, include the pull-type\\ntapping knife and the push-type tapping knife. The use of these knives has the advantage of\\nbeing low cost and ease of acceptance by workers. However, there are many shortcomings\\nassociated with this kind of tapping method. For example, the tool bit cannot be replaced. In\\naddition, it is difﬁcult to control the quantity and shape of a single consumption, which has\\nseriously affected the economic beneﬁts of rubber industries [ 50]. A portable rubber tapping\\nmachine is a hand-held device, which can be easily deployed in plantations. As shown in\\nFigure 5b, the use of the 4GXJ (Rubber Research Institute, Chinese Academy of Tropical\\nAgricultural Science) electrical tapping device, a kind of portable electrical tapping device\\ndriven by electricity, can decrease the labor intensity of rubber workers [ 51]. Because of its\\npoor levels of automation, it is preferable to use it as an auxiliary device [ 52]. Furthermore,\\nit does not offer a hands-free solution because it still relies on manual handling.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed60afd0-8039-4395-8562-176e410e0059', embedding=None, metadata={'page_label': '7', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 7 of 26\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 7 of 26 \\n \\nauxiliary device [52]. Furthermore, it does not offer a hands-free solution because it still \\nrelies on manual handling. \\n(a) \\n(b) \\nFigure 5. (a) Traditional tapping knife; ( b) portable electrical tapping device. \\n3.2. Fixed Tapping Machine \\nManual rubber tapping is a time-consuming and skill-oriented type of labor. A rub-\\nber tapping machine does not completely rely on manual work, which means that the \\nlabor intensity of the work can be reduced to a certain extent [53–55]. As shown in Figure \\n6a, the bundled profiled tapping machine has the advantages of low cost, a lightweight \\nstructure, and a stable tapping movement. It is attached to the tree by means of straps. \\nEasier control of the tapping depth enables the fixed tapping machine to avoid damaging \\ntrees. This kind of equipment can expediently locate the relative position of the collection \\ncup, which can be prepared  for harvesting [56,57]. \\nNing et al. [58] designed a fixed compos ite track-type rubber tapping machine (as \\nshown in Figure 6b). By selecting the dry rubbe r output as the response value, the second-\\norder regression model of the response valu e and the significant parameters were ob-\\ntained based on the Box–Behnken design test. The results showed that when the speed of \\nthe motor was 21 r/min and the preload force of  the string was 20 N, the tapping machine \\ncould obtain the rubber yield of 6.29 mL in the first 5 min with the optimal parameter \\ncombination. As shown in Figure 6c, a fixed rubber tapping machine driven by a motor \\nwas designed with an intelligent control syst em [30]. When the host computer gave the \\ntapping command to the system, the tapping kn ife realized the presupposed spiral trajec-\\ntory motion according to the designed program in the controller. At the same time, the \\nfeed rate of the tapping knife was controlled in  real time using the feedback data obtained \\nfrom a distance sensor to realize the depth-controlled tapping operation. \\nFigure 5. (a) Traditional tapping knife; ( b) portable electrical tapping device.\\n3.2. Fixed Tapping Machine\\nManual rubber tapping is a time-consuming and skill-oriented type of labor. A rubber\\ntapping machine does not completely rely on manual work, which means that the labor\\nintensity of the work can be reduced to a certain extent [ 53–55]. As shown in Figure 6a, the\\nbundled proﬁled tapping machine has the advantages of low cost, a lightweight structure,\\nand a stable tapping movement. It is attached to the tree by means of straps. Easier control\\nof the tapping depth enables the ﬁxed tapping machine to avoid damaging trees. This kind\\nof equipment can expediently locate the relative position of the collection cup, which can\\nbe prepared for harvesting [56,57].\\nNing et al. [ 58] designed a ﬁxed composite track-type rubber tapping machine (as\\nshown in Figure 6b). By selecting the dry rubber output as the response value, the second-\\norder regression model of the response value and the signiﬁcant parameters were obtained\\nbased on the Box–Behnken design test. The results showed that when the speed of the\\nmotor was 21 r/min and the preload force of the string was 20 N, the tapping machine\\ncould obtain the rubber yield of 6.29 mL in the ﬁrst 5 min with the optimal parameter\\ncombination. As shown in Figure 6c, a ﬁxed rubber tapping machine driven by a motor was\\ndesigned with an intelligent control system [ 30]. When the host computer gave the tapping\\ncommand to the system, the tapping knife realized the presupposed spiral trajectory motion\\naccording to the designed program in the controller. At the same time, the feed rate of the\\ntapping knife was controlled in real time using the feedback data obtained from a distance\\nsensor to realize the depth-controlled tapping operation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f4543f7-079b-4cfb-b46d-47802ce81ccc', embedding=None, metadata={'page_label': '8', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 8 of 26\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 8 of 26 \\n \\n   \\n(a) ( b) ( c) \\nFigure 6. Fixed tapping machine. ( a) Bundled profiled tapping machine [59]. (b) Fixed compound \\nmotion track rubber tapping machine [58]. (c) Rubber forest cutting test site [30]. \\n3.3. Self-Propelled Rubber Tapping Robot \\nWith the development of automation technologies, automatic control, sensor recog-\\nnition, image processing, and other technologi es have been widely applied in rubber tap-\\nping operations, which has promoted the design of self-propelled rubber tapping robots. \\nAs shown in Figure 7a, HARIBIT [60] unve iled a rubber tapping robot based on vision \\nand lidar technology, which used a camera to access two-dimensional code labels attached \\nto rubber trees. The two-dimensional code labe l contained the height and horizontal po-\\nsition information of the line to be tapped. They controlled the movement of the mechan-\\nical arm in relation to the corresponding posi tion of rubber tapping according to the label \\nand used a depth camera to extract the coordinates of knife marks on rubber trees to con-\\nstruct a new cutting track for rubber tapping. \\n  \\n(a) ( b) \\nFigure 7. Self-propelled rubber tapping robot. ( a) Crawler-type tapping robot [60]. (b) Rail-mounted \\ntapping robot [18]. \\nAs shown in Figure 7b, Zhou et al. [18] presented a rubber tapping device with a rail-\\nbased mobile platform. This was an innovati on over previous rubber tapping devices. An \\nupgraded spatial spiral  trajectory was established by analyzing the trunk and manual \\nFigure 6. Fixed tapping machine. ( a) Bundled proﬁled tapping machine [ 59]. (b) Fixed compound\\nmotion track rubber tapping machine [58]. ( c) Rubber forest cutting test site [30].\\n3.3. Self-Propelled Rubber Tapping Robot\\nWith the development of automation technologies, automatic control, sensor recogni-\\ntion, image processing, and other technologies have been widely applied in rubber tapping\\noperations, which has promoted the design of self-propelled rubber tapping robots. As\\nshown in Figure 7a, HARIBIT [ 60] unveiled a rubber tapping robot based on vision and\\nlidar technology, which used a camera to access two-dimensional code labels attached to\\nrubber trees. The two-dimensional code label contained the height and horizontal position\\ninformation of the line to be tapped. They controlled the movement of the mechanical arm\\nin relation to the corresponding position of rubber tapping according to the label and used\\na depth camera to extract the coordinates of knife marks on rubber trees to construct a new\\ncutting track for rubber tapping.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 8 of 26 \\n \\n   \\n(a) ( b) ( c) \\nFigure 6. Fixed tapping machine. ( a) Bundled profiled tapping machine [59]. (b) Fixed compound \\nmotion track rubber tapping machine [58]. (c) Rubber forest cutting test site [30]. \\n3.3. Self-Propelled Rubber Tapping Robot \\nWith the development of automation technologies, automatic control, sensor recog-\\nnition, image processing, and other technologi es have been widely applied in rubber tap-\\nping operations, which has promoted the design of self-propelled rubber tapping robots. \\nAs shown in Figure 7a, HARIBIT [60] unve iled a rubber tapping robot based on vision \\nand lidar technology, which used a camera to access two-dimensional code labels attached \\nto rubber trees. The two-dimensional code labe l contained the height and horizontal po-\\nsition information of the line to be tapped. They controlled the movement of the mechan-\\nical arm in relation to the corresponding posi tion of rubber tapping according to the label \\nand used a depth camera to extract the coordinates of knife marks on rubber trees to con-\\nstruct a new cutting track for rubber tapping. \\n  \\n(a) ( b) \\nFigure 7. Self-propelled rubber tapping robot. ( a) Crawler-type tapping robot [60]. (b) Rail-mounted \\ntapping robot [18]. \\nAs shown in Figure 7b, Zhou et al. [18] presented a rubber tapping device with a rail-\\nbased mobile platform. This was an innovati on over previous rubber tapping devices. An \\nupgraded spatial spiral  trajectory was established by analyzing the trunk and manual \\nFigure 7. Self-propelled rubber tapping robot. ( a) Crawler-type tapping robot [ 60]. (b) Rail-mounted\\ntapping robot [18].\\nAs shown in Figure 7b, Zhou et al. [ 18] presented a rubber tapping device with a\\nrail-based mobile platform. This was an innovation over previous rubber tapping de-\\nvices. An upgraded spatial spiral trajectory was established by analyzing the trunk and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0a103aa-f71e-4b34-a3da-c8ba3b2d4b32', embedding=None, metadata={'page_label': '9', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 9 of 26\\nmanual rubber cutting trajectories for the operation of a six-axis tandem robot arm. A\\ncompact binocular stereo vision system was used to achieve the initial perception of the\\nparameters required for the cutting trajectory. In addition, an integrated end-effector was\\ndeveloped to enable a more precise perception of the cutting trajectory and cutting op-\\nerations. However, the motion of this robot depended on the rail, which greatly limited\\nthe application scenarios for the robot. Moreover, the study did not provide navigational\\noperation information for the crawler chassis. Compared with this kind of ﬁxed automatic\\ntapping machine, a self-propelled rubber tapping robot is more intelligent and relies on\\nless labor. Nevertheless, there is still room for research on these robots’ navigation, control,\\nand their recognition of rubber tapping trajectories. In the context of emerging information\\ntechnology development, the use of self-propelled rubber tapping robots will become a\\ntrend in the future.\\nNotably, an accurate tapping track, tapping depth, tapping thickness, and proﬁling\\nmechanism are the most important evaluation indexes for rubber tapping machines. We\\nhave classiﬁed the above references and summed up the indexes used in each study in\\nTable 2. The ticks and crosses in the table indicate whether the device in the literature\\ncontains the function in the list. In terms of the current development status of agricultural\\nrobots, most of the equipment is still in the laboratory stage and has not reached the\\nstage of industrialization and commercial use. Against this background, the research\\nand development of agricultural robots can make breakthroughs in the following aspects.\\nOne is to develop ﬁxed automatic rubber tapping machines. This kind of research and\\ndevelopment does not need to move the chassis, and it is easy to carry out production tests.\\nThe other is to focus on the research and development of the operating chassis and special\\nmechanical arms with independent positioning and control systems. An operating chassis\\nthat can plan the operating route and avoid obstacles stably would provide the premise for\\nmobile robots to transition from the laboratory to the market.\\nTable 2. Classiﬁcation of rubber cutters and summaries of evaluation indexes used in each study.\\nReferences ClassiﬁcationAccurate\\nTapping TrackTapping DepthBark\\nConsumptionProﬁling\\nMechanismYear of\\nPublication\\n[50] Traditional cutting \\x16 \\x16 \\x16 \\x16 2011\\n[61]Optimized rubber\\ntapping knife\\x16 \\x14 \\x14 \\x16 2018\\n[51]Electrical rubber\\ntapping knife\\x16 \\x14 \\x14 \\x16 2018\\n[56]Fixed rubber\\ntapping knife\\x14 \\x14 \\x14 \\x16 2020\\n[18]Self-propelled rubber\\ntapping robot\\x14 \\x14 \\x14 \\x14 2021\\n[59]Fixed rubber\\ntapping machine\\x16 \\x14 \\x14 \\x14 2022\\n[62]Self-propelled rubber\\ntapping robot\\x14 \\x14 \\x14 \\x14 2022\\n4. Rubber Tapping Technology\\nRubber harvesting stress can lead to tapping panel dryness (TPD) and can inﬂu-\\nence the non-structural carbohydrate (NSC) storage in trunk wood, which affects the\\nproduction of latex [ 63–65]. A tapping machine needs to pay attention to the following\\nimportant indicators:\\n(1) An accurate tapping track: because of the spiral shape used, it is necessary that the\\ntapping tool can move according to the speciﬁed precise track. Moreover, to realize', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47bbbe90-0c33-4285-8368-5fec64febb43', embedding=None, metadata={'page_label': '10', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 10 of 26\\nautomatic rubber tapping and save cost, the mechanical parts for rubber tapping\\nshould be simpliﬁed.\\n(2) A controllable tapping depth: excessively deep rubber tapping will cause damage to\\nthe tree body, and excessively shallow tapping will affect the yield of rubber, so it is\\nnecessary to design a device that can ensure that a uniform detection and limit depth\\nare maintained each time rubber tapping is carried out.\\n(3) Reasonable bark consumption: too much consumption of bark will shorten the total\\ntapping cycle, and too little will reduce the latex yield. Therefore, on the premise of\\nensuring the lactation tube, the amount of skin consumed in a single tap should be\\nreduced as far as possible.\\n(4) A stable proﬁling mechanism: as a rubber tree is not an ideal ellipse, it is necessary to\\ndesign a copying device to make the laticifer partition more even.\\n4.1. Manual Tapping\\nSusanto et al. [ 61] optimized the use of a rubber knife to control tapping depth and\\nused software to select component materials and conduct manufacturing tests on nine\\nrubber trees. The treatment was carried out according to the design plan, keeping the\\ndepth between 1 and 1.5 mm of the cambium, the tapping thickness between 1.5 and 2 mm,\\nand tilting angles between 35◦and 60◦. The results of the functional testing showed that\\nrubber tapping can function properly and using intervals of 09:00–10:00 WIB for slopes\\n60◦, 45◦, and 35◦, they obtained average latex amounts of 1.83 g, 1.11 g, and 0.74 g. The\\nrubber tapping capacity of 5–6 s per tree was an improvement upon conventional rubber\\ntapping, which required between 6–8 s with 5.32 cm3rubber bark consumption. Their\\nblades were connected by means of bolt joints so that they could be easily replaced if a\\nblunt force was encountered.\\nZhang et al. [ 51] designed a portable electric tapping machine. The experimental\\nresults showed that in addition to individual data, the difference in bark consumption\\nat each cut was less than 0.2 mm, and the average bark consumption of each group was\\nalso less than 0.2 mm. This showed that the bark consumption was uniform and could\\nmeet the requirements of the design. Furthermore, the knife body had no extrusion on\\nthe tapping line, which was conducive to rubber discharge. The design’s symmetrical\\nblade structure can be used to tap rubber with an upside and a downside, greatly reducing\\nthe technical difﬁculty and labor intensity of rubber workers, and changing the rubber\\ntapping operation from a professional task to a more accessible one. Chen et al. [ 66]\\ndesigned a series of electric rubber tapping knives called 4GXJ. By adjusting the relative\\nposition of the guide and the circular edge of the blade, precise control of the tapping depth\\nwas realized at a millimeter level. Furthermore, by moving the guide up and down and\\nadjusting its relative position with the horizontal edge, precise control of the millimeter\\nthickness of bark consumption was realized. The results showed that under the conditions\\nof torques of 0.2 N ·m, 0.22 N ·m, and 0.25 N ·m, the movement curve of the transmission\\nmechanism changed smoothly and periodically, and the maximum stress generated by the\\nkey components was 5.147×103MPa , which was much smaller than the yield strength\\nof the material, 2.206 ×108MPa, and there was no obvious change in deformation. A\\nperformance comparison between a traditional tapping knife and an electrical tapping\\nknife is shown in Table 3. Because of the need for a motor, a lithium battery, transmission\\ncomponents, and precision requirements, the cost of an electrical rubber cutter is higher\\nthan that of a traditional one. With the same operation time or labor intensity, the tapping\\narea can be increased by 20–30%. At the same time, the use of an electrical cutter can reduce\\nbark consumption and the damage to trees, which can make up for the higher cost.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2f313b5-713e-4e68-b539-703579ac21e7', embedding=None, metadata={'page_label': '11', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 11 of 26\\nTable 3. Performance comparison between the traditional tapping knife and the electrical tapping\\nknife [67].\\nParameter TypeTraditional Tapping Knife Electrical Tapping Knife\\nPush-Type Pull-Type 4GXJ-1 4GXJ-2\\nPower Manpower Manpower Brushless motor Brushless motor\\nCutting time per tree (s) 12–18 12–18 10–16 5–10\\nBark consumption (mm/year) 110–150 110–150 110–130 110–130\\nBattery capacity (mAh)/Endurance (h) - - 2000/1.5–2.0 4000/3.5–4.5\\nCost of training time for rubber tappers (d) 25–30 25–30 3–5 3–5\\n4.2. Semi-Automatic Tapping with Fixed Machine\\nDeepthi et al. [ 56] proposed the structure of a semi-automated rubber tapping machine,\\nthe depth of which was initially set by manual movement and achieved semi-circular\\nrotation via the motion of the wiper motor carrying a blade along with it. Driven by the\\nmotor, the small circular gear began to rotate, driving the tapping blade to move along with\\nthe large circular gear. The model was held by a circular clamp that could be attached to\\nthe tree and the structure could be adjusted with bolts and nuts according to the diameter\\nof the tree. The model could be easily ﬁxed to the tree and could tap a ﬁxed spiral trajectory.\\nThe system is simple yet cost-efﬁcient, with a one-time investment. It reduces labor costs\\nand assessments which are prone to human error. Zhang et al. [ 68] designed and tested a\\nthree-coordinate linkage natural rubber tapping device based on laser ranging. The artiﬁcial\\ntapping rubber surface was taken as the reference tapping line, and the depth information\\nof the tapping tool was measured at several set collection points through the use of a\\nlaser ranging sensor. Test results showed that the error in terms of bark consumption was\\nabout 5%.\\nHowever, because the surface of a rubber trunk is not ideally cylindrical, its uneven\\nsurface defects increase the difﬁculty of rubber tapping. To overcome the problem of an\\nuneven bark surface, a ﬁxed automatic tapping machine was designed with a proﬁling\\nmechanism [ 59]. In this system, driven by a decelerating stepper motor, the screw nut drives\\nthe end-effector to move in a straight line along the slider bracket, and the elliptical motion\\nis combined with the linear motion to realize the three-dimensional spiral movement of\\nthe rubber tapping actuator along the tree trunk. A cylinder at the back end of the depth-\\nlimiting roller of the model rolls relative to the top surface of the rubber tree to copy the\\ntapping track of the rubber tree. The use of this copying mechanism can ensure a stable\\nspiral angle. According to Figure 8a, combined with the test data, when the motor speed\\nwas 21 r/min and the spiral angle was 25◦, the output of natural rubber was 6.29 mL\\nin the ﬁrst ﬁve minutes, and this was the best parameter combination. Ning et al. [ 58]\\ndesigned a ﬁxed composite track-type rubber tapping machine. As shown in Figure 8b,\\nresponse surface diagrams showed a bell-shaped curve with a downward opening, which\\nindicated that with an increase in the factor level, the yield of rubber ﬁrst increased and\\nthen decreased. Tests showed that when the cutting angle was 28◦, the cutting depth was\\n5.5 mm, and the skin thickness was 1.6 mm, the dry glue output reached the best value of\\n178.4 g. The credibility of the analysis was also veriﬁed.\\n4.3. Automatic Tapping with Self-Propelled Robot\\nZhou et al. [ 18] proposed a rubber tapping robot for natural rubber plantations and\\ndeveloped an integrated end-effector to further accurately perceive the tapping trajectory\\nand tapping operations. The adaptability of the upgraded trajectory and the accuracy, as\\nwell as the efﬁciency of the starting point positioning algorithm, were veriﬁed by ﬁeld\\nexperiments on a rubber plantation. Alignment error and execution time curves are shown\\nin Figure 9. Overall, the alignment error was around 1.0 ±0.1 mm, and the execution time\\nwas 17.01 ±3.65 s. As shown in Figure 10, the accuracy of chip thickness and chip width\\nwas about 1.73 ±0.28 mm, and 5.07 ±0.13 mm, respectively. Furthermore, the accuracy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='845f0f1b-f1f3-4187-adcb-62ec5bf8418d', embedding=None, metadata={'page_label': '12', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 12 of 26\\nof the chip weight was 1.99 ±0.24 mm. The average operating time of the whole tapping\\noperation was 80 ±5 s. That study demonstrated the potential of applying industrial\\nrobotics to the ﬁeld of latex harvesting.\\nMoreover, researchers have also presented a method of tapping with a guide-positioning-\\ntype screw track [ 62]. This model uses a spiral track to realize a spiral tapping track, which\\nis advantageous to the ﬂow of latex. This study demonstrates the potential of applying\\nindustrial robotics to the ﬁeld of latex harvesting. At present, there is growing interest in re-\\nsearch on the autonomous operation of self-propelled rubber tapping machines. Moreover,\\nthis kind of robot has a tremendously marketable value and potential [ 69,70]. As shown in\\nFigure 11, the development of a self-propelled rubber tapping machine usually requires\\nthe fusion and testing of various technologies, such as navigation, sensor perception, and\\nautomatic control. The development of an autonomous navigation technique is of great im-\\nportance to the automatic movement of a self-propelled rubber tapping machine, especially\\nin night-time operations [ 71]. Path planning through rubber forests is necessary in order to\\nmeet the requirements of agricultural norms and to look for reasonable walking tracks on\\nthe premise of nonredundant and non-leaking operations.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 12 of 26 \\n \\n  \\n(a) ( b) \\nFigure 8. Diagram of experiment results. The change in  the color of the graph from blue to red \\nindicates the change in the ordinate from less to more.  (a) Interaction response surface [59]; ( b) re-\\nsponse surface diagram of cutting angle and cutting depth (left), and response surface diagram of cutting angle and bark consumption thickness (right) [58]. \\n4.3. Automatic Tapping with Self-Propelled Robot \\nZhou et al. [18] proposed a rubber tapping robot for na tural rubber plantations and \\ndeveloped an integrated end-effector to further accurately perceive the tapping trajectory and tapping operations. The adaptability of the upgraded trajectory and the accuracy, as \\nwell as the efficiency of the starting point positioning algorithm, were verified by field \\nexperiments on a rubber plantation. Alignment error and execution time curves are shown \\nin Figure 9. Overall, the al ignment error was around 1.0 ± 0.1 mm, and the execution time \\nwas 17.01 ± 3.65 s. As shown in Figure 10, the accuracy of chip thickness and chip width was about 1.73 ± 0.28 mm, and 5.07 ± 0.13 mm, respectively. Furthermore, the accuracy of \\nthe chip weight was 1.99 ± 0.24 mm. The average operating time of the whole tapping \\noperation was 80 ± 5 s. That study demonstrated the potential of applying industrial ro-botics to the field of latex harvesting. \\n \\n(a) ( b) ( c) ( d) \\nFigure 9. Curves of multiple threshold pair test results: ( a) pixel deviation—alignment error results, \\n(b) cutting feed—alignment error results, ( c) pixel deviation—execution results, and ( d) cutting \\nfeed—execution results [18]. \\nFigure 8. Diagram of experiment results. The change in the color of the graph from blue to red indi-\\ncates the change in the ordinate from less to more. ( a) Interaction response surface [ 59]; (b) response\\nsurface diagram of cutting angle and cutting depth (left), and response surface diagram of cutting\\nangle and bark consumption thickness (right) [58].\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 12 of 26 \\n \\n  \\n(a) ( b) \\nFigure 8. Diagram of experiment results. The change in  the color of the graph from blue to red \\nindicates the change in the ordinate from less to more.  (a) Interaction response surface [59]; ( b) re-\\nsponse surface diagram of cutting angle and cutting depth (left), and response surface diagram of cutting angle and bark consumption thickness (right) [58]. \\n4.3. Automatic Tapping with Self-Propelled Robot \\nZhou et al. [18] proposed a rubber tapping robot for na tural rubber plantations and \\ndeveloped an integrated end-effector to further accurately perceive the tapping trajectory and tapping operations. The adaptability of the upgraded trajectory and the accuracy, as \\nwell as the efficiency of the starting point positioning algorithm, were verified by field \\nexperiments on a rubber plantation. Alignment error and execution time curves are shown \\nin Figure 9. Overall, the al ignment error was around 1.0 ± 0.1 mm, and the execution time \\nwas 17.01 ± 3.65 s. As shown in Figure 10, the accuracy of chip thickness and chip width was about 1.73 ± 0.28 mm, and 5.07 ± 0.13 mm, respectively. Furthermore, the accuracy of \\nthe chip weight was 1.99 ± 0.24 mm. The average operating time of the whole tapping \\noperation was 80 ± 5 s. That study demonstrated the potential of applying industrial ro-botics to the field of latex harvesting. \\n \\n(a) ( b) ( c) ( d) \\nFigure 9. Curves of multiple threshold pair test results: ( a) pixel deviation—alignment error results, \\n(b) cutting feed—alignment error results, ( c) pixel deviation—execution results, and ( d) cutting \\nfeed—execution results [18]. \\nFigure 9. Curves of multiple threshold pair test results: ( a) pixel deviation—alignment error results,\\n(b) cutting feed—alignment error results, ( c) pixel deviation—execution results, and ( d) cutting\\nfeed—execution results [18].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88f33b3c-29f6-4dfc-ac52-313f7359546c', embedding=None, metadata={'page_label': '13', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 13 of 26\\nPath planning for navigation in rubber orchards commonly involves global path plan-\\nning and local path planning (Table 4). Local path planning requires real-time feedback.\\nIt is used for local obstacle avoidance in path planning [ 72] and local tracking path plan-\\nning [62,73,74] is based on sensing the external environment through sensors and feeding\\ninformation back to the controller for real-time path control decisions [ 75]. Global path\\nplanning is a planning method that requires the complete path information of a rubber\\nforest as prior data [ 76–78]. In the following section, the identiﬁcation of rubber trees and\\nrubber cutting lines and the application of obstacle information detection in path planning\\nare described.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 13 of 26 \\n \\n   \\n(a) ( b) ( c) \\nFigure 10. Measuring results of the cutting depth test in groups [18]. ( a) Chip  width measurement \\nresults, ( b) chip thickness measurement results, ( c) chip weight measurement results. \\nMoreover, researchers have also presented a method of tapping with a guide-posi-\\ntioning-type screw track [62]. This model uses a spiral track to realize a spiral tapping \\ntrack, which is advantageous to the flow of latex. This study demonstrates the potential of applying industrial robotics to the field of  latex harvesting. At present, there is growing \\ninterest in research on the autonomous op eration of self-propelled rubber tapping ma-\\nchines. Moreover, this kind of robot has a tremendously marketable value and potential \\n[69,70]. As shown in Figure 11, the development of a self-propelled rubber tapping ma-\\nchine usually requires the fusion and testing of various technologies, such as navigation, sensor perception, and automatic control. Th e development of an autonomous navigation \\ntechnique is of great importance to the automatic movement of a self-propelled rubber \\ntapping machine, especially in night-time operations [71]. Path planning through rubber \\nforests is necessary in order to meet the requirements of agricultural norms and to look \\nfor reasonable walking tracks on the premise of nonredundant and non-leaking opera-tions. \\n \\nFigure 11. The structure of a rubber tapping robot. \\nPath planning for navigation in rubber orchards commonly involves global path \\nplanning and local path planning (Table 4). Local path planning requires real-time feed-\\nback. It is used for local obstacle avoidance in path planning [72] and local tracking path \\nplanning [62,73,74] is based on sensing th e external environment through sensors and \\nfeeding information back to the controller for real-time path control decisions [75]. Global \\npath planning is a planning method that requires the complete path information of a rub-\\nber forest as prior data [76–78]. In the following section, the identification of rubber trees \\nFigure 10. Measuring results of the cutting depth test in groups [ 18]. (a) Chip width measurement\\nresults, ( b) chip thickness measurement results, ( c) chip weight measurement results.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 13 of 26 \\n \\n   \\n(a) ( b) ( c) \\nFigure 10. Measuring results of the cutting depth test in groups [18]. ( a) Chip  width measurement \\nresults, ( b) chip thickness measurement results, ( c) chip weight measurement results. \\nMoreover, researchers have also presented a method of tapping with a guide-posi-\\ntioning-type screw track [62]. This model uses a spiral track to realize a spiral tapping \\ntrack, which is advantageous to the flow of latex. This study demonstrates the potential of applying industrial robotics to the field of  latex harvesting. At present, there is growing \\ninterest in research on the autonomous op eration of self-propelled rubber tapping ma-\\nchines. Moreover, this kind of robot has a tremendously marketable value and potential \\n[69,70]. As shown in Figure 11, the development of a self-propelled rubber tapping ma-\\nchine usually requires the fusion and testing of various technologies, such as navigation, sensor perception, and automatic control. Th e development of an autonomous navigation \\ntechnique is of great importance to the automatic movement of a self-propelled rubber \\ntapping machine, especially in night-time operations [71]. Path planning through rubber \\nforests is necessary in order to meet the requirements of agricultural norms and to look \\nfor reasonable walking tracks on the premise of nonredundant and non-leaking opera-tions. \\n \\nFigure 11. The structure of a rubber tapping robot. \\nPath planning for navigation in rubber orchards commonly involves global path \\nplanning and local path planning (Table 4). Local path planning requires real-time feed-\\nback. It is used for local obstacle avoidance in path planning [72] and local tracking path \\nplanning [62,73,74] is based on sensing th e external environment through sensors and \\nfeeding information back to the controller for real-time path control decisions [75]. Global \\npath planning is a planning method that requires the complete path information of a rub-\\nber forest as prior data [76–78]. In the following section, the identification of rubber trees \\nFigure 11. The structure of a rubber tapping robot.\\n4.3.1. Obstacle Information Perception and Path Planning\\nObstacle information perception technology refers to the detection and identiﬁcation\\nof surrounding obstacles in an unstructured operation environment, which provides secu-\\nrity and a guarantee of intelligent operation in a complex environment [ 79–82]. To date,\\nmany technologies, such as sensors, machine vision, Lidar, and ultrasonic geomagnetic\\nposition [ 83,84], have been developed to study the autonomous navigation of robots [ 85].\\nSome researchers have aimed to develop an algorithm of a vision system for the replace-\\nment of the expensive sensors used in typical autonomous vehicles [ 86–88]. The vision\\nsystem model was implemented using only one camera installed on the vehicle to search\\nfor calibrated targets that were put on the trunks of rubber trees.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc43ccd4-555e-4afb-b6a5-eaca960ae851', embedding=None, metadata={'page_label': '14', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 14 of 26\\nTable 4. Key components of articles related to path planning.\\nReferences Sensors Feedback Path Planning ClassiﬁcationYear of\\nPublication\\n[72]Camera, GPS,\\nOdometric sensorsVisual Local tracking path planning 2010\\n[76] Single camera Visual mapLocal obstacle avoidance\\npath planning2018\\n[74]Camera,\\nOdometric sensorsVisual,\\nAuto-steeringFollow-the-past path\\ntracking planning2019\\n[73] Lidar, gyroscope The point cloud mapLocal tracking, global point-to-point\\npath planning2019\\n[62] 3-D Lidar Point cloud Local tracking path planning 2021\\nMany automated systems require the use of cameras for their acquisition devices, with\\na common working principle of applying a vision algorithm to ﬁnd a combination of image\\nformats or point clouds [ 71]. Nissimov et al. [ 89] proposed a method to detect obstacles\\nusing a Kinect 3D sensor, which can be used for the autonomous navigation of robots to\\nclassify pixels of suspicious obstacles through color and texture features. Skoczen et al. [90]\\ndesigned an obstacle detection system for agricultural mobile robots using an RGB-D\\ncamera. Al-kaff et al. [ 91] came up with a method to simulate human behavior and use a\\nmonocular camera to detect the collision states of approaching obstacles. Zhang and Li [ 92]\\nstudied mobile robot recognition and an autonomous obstacle avoidance system based on\\nKinect 3D sensors.\\nHowever, machine vision is greatly affected by the working environment and lighting\\nconditions, as well as the technologies involved in the method, such as image processing,\\nimage analysis, camera calibration, and the extraction of navigation parameters, which\\nmake it rather difﬁcult to apply in an outdoor environment in agriculture [ 93,94]. Rubber-\\ntapping techniques require this activity to be carried out at night. The 3D Lidar SLAM\\nsystem aids in tool mark recognition under low light conditions. Lidar can not only provide\\na lot of accurate information with high frequency but can also meet the requirements\\nof accuracy and speed. In addition, it performs well compared to its cost, providing\\nall-weather services regardless of variations in lighting conditions.\\nZhang et al. [ 73] used a low-cost Lidar system and a gyroscope to extract the sparse\\npoint cloud data of tree trunks by means of clustering method, realizing the autonomous\\nnavigation of an intelligent rubber-tapping platform and collecting information on a rubber\\nforest. The following activities were carried out by applying a fuzzy controller, as shown\\nin Figure 12a,b: walking along a row with a ﬁxed lateral distance, stopping at a ﬁxed\\npoint, turning from one row to another, and collecting information on plant spacing,\\nrow spacing, and trees’ diameters. As shown in Figure 12c, the results of three repeated\\nexperiments showed that the root mean square (RMS) lateral errors were less than 10.32 cm ,\\nand the average stopping error was 12.08 cm, indicating that the proposed navigation\\nmethod provided great path tracking performance. Zhou et al. [ 95] proposed a random\\nforest-based place recognition and navigation framework. Aiming at the problem of place\\nrecognition based on 3D point clouds, two kinds of features were extracted to form multi-\\nmodal feature vectors. The random forest model was applied to the place recognition\\nof a self-propelled robot. They introduced the odometry location relationship output\\ninto the loop discrimination process. Then, the loop detection algorithm was added to\\nS4-SLAM , forming S4-SLAM2, to realize the global localization of the mobile robot in the\\nmap. Multiple experiments were performed in an outdoor environment to conﬁrm the\\nproposed method, with results demonstrating its feasibility and effectiveness.\\nNie et al. [ 62] used a tree trunk recognition system based on the optimized density-\\nbased spatial clustering of applications with noise (DBSCAN) to ensure the accuracy and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c47414d3-9a96-43e4-b3d8-a70072d03817', embedding=None, metadata={'page_label': '15', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 15 of 26\\nrobustness of recognition in different distance scales, in the absence of GPS in forests. The\\nalgorithm (after density ﬁeld correction) could make use of trees with further distances,\\nwhich had been ﬁltered out by the original DBSCAN algorithm due to the low density\\nof point clouds. The original algorithm could only detect trees within 8.9 m on average,\\nwhereas their improved algorithm could successfully detect trees within 23.5 m on average.\\nTo further improve the environmental adaptability and feature stability of their SLAM\\nsystem, the latest research achievements in visual SLAM and semantic SLAM can be\\nintegrated into their system in the future. Zhou et al. [ 96] introduced a sensing approach\\nwhich combined Lidar sensor data with vision sensor data in a self-supervised learning\\nframework to robustly identify a drivable terrain surface for robots operating in forested\\nterrain. In this system, the acquisition frequency of Lidar sensor data is low, and thus it is\\nnot only used for ground recognition but can also be used to automatically supervise the\\ntraining of a classiﬁer based on visual features (color and texture). Experiments showed that\\nthe sensor system revealed good sensing performance in several forested environments.\\nAppl. Sci. 2022 , 12, x FOR PEER REVIEW 15 of 26 \\n \\nmethod provided great path tracking performa nce. Zhou et al. [95] proposed a random \\nforest-based place recognition and navigation framework. Aiming at the problem of place \\nrecognition based on 3D point clouds, two kind s of features were extracted to form multi-\\nmodal feature vectors. The random forest model was applied to the place recognition of a \\nself-propelled robot. They introduced the odomet ry location relationship output into the \\nloop discrimination process. Then, the loop  detection algorithm was added to S4-SLAM, \\nforming S4-SLAM2, to realize the global locali zation of the mobile robot in the map. Mul-\\ntiple experiments were performed in an outdoor environment to confirm the proposed method, with results demonstrating its feasibility and effectiveness. \\n  \\n(a) ( b) \\n  \\n(c) \\nFigure 12. (a) Diagram of navigation paths used in tapping robot tasks [73]; (b) diagram of fuzzy \\ncontrol rules (left) and the distribution  of the membership function (right) [73]; (c) distribution of \\nstopping error and lateral error [73]. \\nNie et al. [62] used a tree trunk recognitio n system based on the optimized density-\\nbased spatial clustering of applications with  noise (DBSCAN) to ensure the accuracy and \\nrobustness of recognition in di fferent distance scales, in the absence of GPS in forests. The \\nalgorithm (after density field correction) coul d make use of trees with further distances, \\nwhich had been filtered out by the original DBSCAN algorithm due to the low density of \\npoint clouds. The original algorithm could only detect trees within 8.9 m on average, whereas their improved algorithm could success fully detect trees within 23.5 m on aver-\\nage. To further improve the environmental ad aptability and feature stability of their \\nSLAM system, the latest research achievements in visual SLAM and semantic SLAM can \\nbe integrated into their system in the future . Zhou et al. [96] introduced a sensing ap-\\nproach which combined Lidar sensor data with  vision sensor data in a self-supervised \\nlearning framework to robustly identify a driv able terrain surface for robots operating in \\nforested terrain. In this system, the acquisition frequency of Lidar sensor data is low, and \\nthus it is not only used for ground recognition but can also be used to automatically su-\\npervise the training of a classifier based on visual features (color and texture). Experiments \\nshowed that the sensor system revealed good sensing performance in several forested en-vironments. \\nFigure 12. (a) Diagram of navigation paths used in tapping robot tasks [ 73]; (b) diagram of fuzzy\\ncontrol rules (left) and the distribution of the membership function (right) [ 73]; (c) distribution of\\nstopping error and lateral error [73].\\nAmong these method, the combination of visual range and artiﬁcial intelligence\\nalgorithms can improve the identiﬁcation accuracy, but it is vulnerable to factors of the\\nexternal environment, such as light interference [ 85]. The ability to lower the cost of\\na camera to increase the distance and scope of measurements is limited. Furthermore,\\nthe 3D laser radar method has high measuring accuracy and good stability, but its cost\\nis high. As a result, information detection based on multi-sensor fusion is a possible\\ndirection of research. In addition, autonomous mobile intelligent rubber tapping robots\\nface some technical difﬁculties, such as poor recognition accuracy, poor adaptive ability,\\nnavigation difﬁculties, and servo control with a weak light source, which need to be solved\\nin the future.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='771c019b-f1d6-4814-b215-dfa0cfea0351', embedding=None, metadata={'page_label': '16', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 16 of 26\\n4.3.2. Recognition of Natural Rubber Trees and Tapping Lines\\nThe identiﬁcation of rubber trees and tapping lines is an important aspect of local\\nnavigation in rubber plantations. Therefore, agricultural robots are required to identify\\na rubber tree body in the process of forest navigation, stop by the rubber tree to be cut,\\nand ﬁnd the relative position of the cutting track to plan the next path for rubber tapping.\\nIn the beginning, some researchers used image processing techniques to preprocess col-\\nlected pictures of cutting parts and segment them according to simple features, such as\\ndiscontinuous pixels, shapes, and characteristics of color. To ensure the objectivity and\\nclassiﬁcation of the classiﬁcation of the tapping panel dryness symptoms, Li et al. [ 97] used\\nimage processing technology to study the segmentation of tapping lines and pictures of\\nlatex. In order to eliminate the interference of other factors in the secant segmentation\\nprocess, they ﬁrst precut the secant image, including clipping, angle, and scale processing.\\nOn this basis, the maximum variance method was applied to segment the secant and the\\nbackground, and the latex and the background, and experiment s were conducted to verify\\nthe effectiveness of this method. However, the secant and latex segmentation algorithms\\nstill need to be improved and optimized further due to the complex illumination conditions\\nin the practical application environment.\\nWith the development of neural network algorithms, the research direction has shifted\\nfrom image processing to supervised learning or unsupervised learning, with the aim of\\nusing a network to recognize the characteristic region. The question of how to improve the\\naccuracy and robustness of such algorithms in different scenarios has become a problem to\\nbe solved. Wongtanawijit and Kaorapapong [ 86] developed a simple tapped line detection\\nalgorithm using the color of wood for segmentation and the shape of the tapped area on\\nthe rubber tree as the features to be recognized by a linear SVM classiﬁer model. Scene\\nimages were acquired by means of a light-assisted smartphone camera. The SVM classiﬁer\\nperformance showed that the features extracted with the linear binary classiﬁer reached\\n70% precision with 60% recall in some conﬁgurations. The performance was worse in some\\nconﬁgurations as well. The identiﬁcation accuracy error of this method was considerable,\\nbut this method can provide an idea for future research on the detection of trees and\\ntapping lines. This feature can be combined with other features such as texture features to\\nenhance the performance of the model in classifying this type of data. Some researchers [ 87]\\nproposed a tapping-path and latex cup detection algorithm using a Faster-RCNN detector,\\nwhich is a state-of-the-art precision detector. Their acquisition tool integrated an RGB-D\\ncamera with assisting lights for the capturing of images under low-light conditions. Fine-\\ntuning this method for tapping-path and harvesting-cup detection, they obtained a mean\\naverage precision (mAP) of 0.95 at 0.5 intersections over union (IoU). They proposed a\\nsimple data-preprocessing step (pixel-wise multiplication) before putting the images into\\nCNN detection networks, which meant that their detection results exhibited higher average\\nprecision. This is a well-known architecture that is lightweight in size and computation\\nthat has displayed high feasibility in regard to its deployment in actual mobile platforms.\\nWongtanawijit and Kaorapapong [ 88] also used the sub-array searching technique to\\ndetect a tapping line consistent with the downward tapping system. The average discrete\\nHausdorff distance was used to measure the detected distance errors of the tapping lines.\\nUsing the intersection-over-union ratio of 0.5 as the evaluation criterion for comparing\\nthe pre-deﬁned ground truth and the detected bounding boxes, they achieved the highest\\ndetection accuracy, up to 90.1%. These techniques feasibly support the generation of a\\ntapping path in future studies. Worawut and Akapot [ 76] developed a model of a vision\\nmapping system which is suitable for rubber tree plantations. The vision model was\\ndesigned for the use of a single camera to capture calibrated targets that were placed on\\nrubber tree trunks. The experiments showed that the percentages of error displayed small\\ndifferences throughout the test range. Moreover, using a higher-resolution camera in the\\nsubsequent step could greatly improve the mapping performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88f5acfc-140a-4d7b-ad89-401e50fe02b3', embedding=None, metadata={'page_label': '17', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 17 of 26\\n5. Conclusions and Future Trends\\nThere is tremendous potential for an increase in interest in the ﬁeld of intelligent\\nrubber tapping machines. In this paper, we ﬁrst expounded on the development of rubber\\ntapping machines in three aspects: manual rubber tapping devices, ﬁxed semi-automatic\\nrubber tapping machines, and self-propelled automatic rubber tapping robots. Then, the\\ntypical tapping method of each kind of tapping machine was represented, indicates that\\nthe development of automatic tapping robots with intelligent technologies at their core, is a\\nsuitable direction for the future research on tapping operations in rubber plantations. In this\\npaper, we also introduced the applications of emerging agricultural technology in relation\\nto automatic tapping robots, mainly including the recognition of tapping trajectories and\\npath planning. Finally, we draw conclusions and identify the future scope of research.\\nAccording to the status of research on the independent operation of rubber tapping\\nmachines, due to the geographical factors affecting rubber growth, the current research on\\nagricultural machinery equipment in rubber plantations is limited, and most automatic\\nrubber cutting robots are in the laboratory stage. At present, the development of intelligent\\nrubber tapping robots for the management of rubber plantations should be accelerated to\\nrealize the management of mechanization, as well as intelligent and automatic directional\\ndevelopment. Moreover, the operating system of agricultural machinery equipment in\\nrubber plantations is standardized, and functions can be called or extended according\\nto the application background and needs of the project, which is convenient for users\\nseeking to standardize management, and thus the work of rubber plantations can be carried\\nout in an orderly manner. Furthermore, with the development of “3S” (remote sensing,\\ngeography information, and global positioning systems) technology, it is inevitable that\\ncomputer science techniques such as learning algorithms, image processing, and so on, will\\nbe applied to the processing of data after obtaining data. In order to overcome the obstacles\\nfacing the use of sensors in the operation of agricultural machinery and equipment, multi-\\nsensor fusion detection is a focus of future research on obstacle perception for equipment\\nin forest operations.\\nIn the future, the optimization of rubber cutting machine robots should not only realize\\nmore efﬁcient and accurate rubber cutting operations, but could also consciously transform\\nrubber cutting from a single operation to a cooperative cluster operation, from an isolated\\ninformation node to an intelligent information system, and from an overall platform to\\na modular platform. Moreover, navigation connected to the Internet of Things (IoT) has\\nbeen widely applied to unmanned ecological farm management; however, there is still a\\nlong way to go in the rubber plantation domain. The integration of the planting, plant\\nprotection, and harvesting of rubber plantations can bring about agricultural beneﬁts and\\naffect the future development trends of rubber plantations.\\nIn conclusion, the use of rubber tapping machines for intelligent agriculture can not\\nonly deliver agricultural beneﬁts but could also alleviate the problem of the shortage of rub-\\nber tapping workers. This review provides a brief reference on the research status of rubber\\ntapping machinery and could thus play a positive role in the sustainable development of\\nnatural rubber in the future.\\nAuthor Contributions: Methodology, H.Y. and Z.Z.; writing—original draft preparation, H.Y. and\\nZ.S.; writing—review and editing, H.Y. and Z.Z.; project administration, X.Z. and J.L.; funding\\nacquisition, X.Z.; visualization, H.Y. and Z.S.; validation, H.Y. and J.L. All authors have read and\\nagreed to the published version of the manuscript.\\nFunding: This research received ﬁnancial support from Academician Innovation Center of Hainan\\nProvince, China, grant number (YSPTZX202008); Key research and development projects of Hainan\\nProvince, China, grant number (ZDYF2021XDNY198); China Agriculture Research System, grant\\nnumber (CARS-33-JX2); Science research projects of Hainan Colleges and Universities, China, grant\\nnumber (Hnky2022-9).\\nInstitutional Review Board Statement: Not applicable.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91b5fbe5-1509-4215-8d05-c2eb5ed8436f', embedding=None, metadata={'page_label': '18', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 18 of 26\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nAppendix A\\nTable A1. A summary of the 50 articles included after the ﬁnal selection stage.\\nPaper ID Information Extraction and Future Work Category Year\\nS01 [72]The topic of a machine which can autonomously navigate\\nrubber plantations with obstacle detection capabilities was\\nraised in the conference.Navigation in rubber plantations 2010\\nS02 [50]Based on different methods and principles, stability analyses of\\n25 superior rubber genotypes showed agreement, indicating\\nstable genotypes. The study was backed up by ample data.Protective research in traditional\\nmanual tapping period2011\\nS03 [32]An innovative tapping system, the double-cut alternative, to\\nimprove the yield of Hevea brasiliensis .Innovative rubber tapping method 2011\\nS04 [49]In this study, a two-stage ﬁeld experiment was conducted to\\nevaluate a wide range of low-intensity harvesting systems\\nbased on ethephon stimulation and the extension and\\napplication of this method was proposed.Innovative rubber tapping method 2012\\nS05 [96]In this paper, a self-supervised sensing approach was\\nintroduced in an attempt to robustly identify a drivable terrain\\nsurface for robots operating in forested terrain. The sensing\\nsystem employed both Lidar and vision sensor data.Tree trunk detection for navigation 2012\\nS06 [54]This study showed that tapping panel diagnosis, used as a\\ndecision support tool, can increase remaining tapping years.\\nThe method formalized here will be a useful support for the\\ninnovating tapping management schemes.Protective research in traditional\\nmanual tapping period2012\\nS07 [13]In this paper, ergonomic factors related to low back pain in\\nrubber tappers was deﬁned. The study aimed to evaluate the\\nprevalence of musculoskeletal disorders.Protective research in traditional\\nmanual tapping period2012\\nS08 [55]This work proved that the use of ultrasound technology, an\\ninnovative stimulation technique, as a preprocess on the\\ntapping cut surface of the rubber trees could increase latex and\\ndry rubber yields.Innovative rubber tapping method 2013\\nS09 [98]Compared the difference between tapped and untapped trees to\\nﬁnd whether tapping operations had an inﬂuence on hevea\\nrubber trees.Protective research in traditional\\nmanual tapping period2013\\nS10 [99]Identifying pathogenicity genes in the rubber tree anthracnose\\nfungus colletotrichum gloeosporioides through random\\ninsertional mutagenesis.Protective research of plant\\ndisease control2013\\nS11 [100]Studied the regulation of MIR genes during latex harvesting\\nand TPD.Protective research 2013\\nS12 [53]The authors of this study selected Hevea brasiliensis as their\\nresearch object. The low-frequency tapping experiment proved\\nthat, compared with the standard tapping technique,\\nlow-frequency tapping technology could make up for the\\nshortage of tapping labor in rubber cultivation.Optimized latex\\nharvesting technologies2014\\nS13 [101]Reviewed and collected the problem of stem and root-rot\\ndisease problems in rubber plantations. Obtained management\\nstrategies based on successes and failures.Rubber-tapping-related work 2014', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c83f5c6f-900a-44de-8deb-f107098f9866', embedding=None, metadata={'page_label': '19', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 19 of 26\\nTable A1. Cont.\\nPaper ID Information Extraction and Future Work Category Year\\nS14 [48]This study set out to assess biochemical and histological\\nchanges, as well as changes in gene expression, in latex and\\nphloem tissues from trees grown under various harvesting\\nsystems. The predicted function for some ethylene response\\nfactor genes suggested that some candidate genes play an\\nimportant role in regulating susceptibility to TPD.Protective research in\\ntapping operations2015\\nS15 [52]This paper discussed a semi-automatic rubber tapping machine\\nwhich was a battery-powered tool with a specially designed\\ncutting blade and guide mechanism, supported by a sensory\\nsystem which assisted the operator in performing tapping of\\nthe required quality and standards on all trees in a plantation.Portable electrical tapping device 2016\\nS16 [5]Questionnaires were administered to rubber tappers to measure\\nmusculoskeletal disorders (MSDs) and potential associated\\nfactors. The tests showed that MSDs were common among\\nrubber tappers.Protective research for rubber tappers 2016\\nS17 [71]This paper presented a novel tree trunk detection algorithm\\nthat used the Viola and Jones detector, along with a proposed\\npreprocessing method, combined with tree trunk detection\\nusing depth information.Tree trunk detection for a\\nself-propelled rubber tapping robot2016\\nS18 [94]This paper described a method of monocular visual recognition\\nto help small vehicles navigate between narrow rows.Navigation technology 2016\\nS19 [69]This paper merged fuzzy visual serving and GPS-based\\nplanning to obtain the proper navigation behavior for a small\\ncrop-inspection robot.Navigation technology 2016\\nS20 [102]Annual growth increment and stability of rubber yields in the\\ntapping phase in rubber tree clones. The results showed that\\nannual girth growth occurred at the expense of rubber yields.Protective research for rubber trees 2016\\nS21 [33]This paper presented the design of an intelligent rubber tapping\\ntechnology evaluation equipment based on a cloud model.Assessment of tapping level 2017\\nS22 [103]Bacillus subtilis B1 was shown to have potential biological\\ncontrol ability against various mildew, decay, and stain fungi in\\nrubber trees.Protective research of plant\\ndisease control2018\\nS23 [51]A portable electrical tapping device was designed in this paper.\\nThe method of image processing was used to verify the bark\\nconsumption of the electric tapping machine.Portable electrical tapping device 2018\\nS24 [76]This study aimed to develop a model of a vision mapping\\nsystem which was suitable for rubber tree plantations based on\\na common farming platform in Thailand.Tree trunk detection for a\\nself-propelled rubber tapping robot2018\\nS25 [86]The authors developed a simple tapped line detection\\nalgorithm using the wood color for segmentation and the shape\\nof the tapped area on a rubber tree as the features for\\nrecognition via a linear SVM classiﬁer model.Tapping path detection for a\\nself-propelled rubber tapping robot2018\\nS26 [97]In this paper, image processing technology was used to\\nseparate the secant and latex to avoid interference factors, and\\nobtain the exact secant and latex binary image. By calculating\\nthe area ratio of the corresponding binary images, the grade of\\nTPD could be classiﬁed accurately.Tree trunk detection for a\\nself-propelled rubber tapping robot2018', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07549509-309f-42f0-b6b2-f44e2300f036', embedding=None, metadata={'page_label': '20', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 20 of 26\\nTable A1. Cont.\\nPaper ID Information Extraction and Future Work Category Year\\nS27 [93]By focusing on the tree canopy and sky of an orchard row, an\\nunmanned ground vehicle was able to extract features that\\ncould be used to autonomously navigate through the center of\\nthe tree rows. The machine vision algorithm developed in this\\nstudy showed the potential to guide small utility vehicles in\\norchards in the future.Navigation technology for a\\nself-propelled rubber tapping robot2018\\nS28 [104]The authors observed a correlation between DNA methylation\\nstatus and rubber yield and related characteristics in Hevea\\nbrasiliensis tapped at different heights. They evaluated the\\neffects of tapping-cut heights on rubber yield and related traits.Protective research of rubber trees 2018\\nS29 [87]This paper presented the detection of rubber tree ( Hevea\\nbrasiliensis ) tapping positions (tapping-paths) and\\ntrunk-mounted harvesting cups in RGB-D images, representing\\nthe machine vision part of an automatic rubber tapping system.Tapping path detection for\\nself-propelled tapping robot2019\\nS30 [105]Using an image segmentation methodology, the ratio of the\\ntapping area to the latex area was calculated to analyze the\\ndegree of TPD.Protective research of rubber trees 2019\\nS31 [61]The design of a ﬂexible rubber tapping tool with settings\\nregalted to depth and thickness control was carried out to\\nincrease the productivity of rubber crops in the study region.\\nThe treatment was carried out as follows: controlling the depth\\nbetween 1–1.5 mm of the cambium, keeping the thickness\\ntapping at 1.5–2 mm, and using tilting angles of 35◦–60◦.Fixed tapping machine 2019\\nS32 [68]A three-coordinate linkage rubber tapping device was designed\\nand tested, and a motion path planning method based on a\\nshort tapping cut was proposed. The planning process for the\\ncutting path fused the information of the tapping cut and the\\ncutting depth. Test results showed that the cutting depth was\\nwell controlled, with no damage to rubber trees and the error in\\nterms of bark consumption was about 5%.Fixed tapping machine 2019\\nS33 [106]This paper introduced the progress and frontiers related to\\ntapping technology, and analyzed and summarized the research\\non semi-automatic tapping machinery and automatic tapping\\nmachinery. The 4GXJ-I tapping knife, which is more suitable for\\nindustrial markets, was also designed by this team.Portable electrical tapping device 2019\\nS34 [3]This study summarized the achievements of the past two\\ndecades in understanding the biosynthesis of natural rubber.Protective research for rubber trees 2019\\nS35 [73]The authors made a robot walk along one row at a ﬁxed lateral\\ndistance, stop at a ﬁxed point, and turn from one row into\\nanother. They discussed a method using a low-cost\\ntwo-dimensional (2D) Lidar and a gyroscope.Navigation in rubber plantations 2019\\nS36 [74]The authors investigated an autonomously guided robotic\\nvehicle platform moving along a rubber tree orchard row. The\\nnavigation of the autonomous vehicle in a rubber tree orchard\\nwas successfully evaluated in terms of the magnitude of errors.Navigation in rubber plantations 2019\\nS37 [56]The authors proposed an automated rubber tree tapping and\\nlatex mixing machine for the high-quality production of natural\\nrubber. During the tapping process, a tapping tool was used to\\nmake a depth of 4.0–4.5 mm.Self-propelled rubber tapping robot 2020', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='936b1f10-27e7-47e9-8b25-4328e66a38b7', embedding=None, metadata={'page_label': '21', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 21 of 26\\nTable A1. Cont.\\nPaper ID Information Extraction and Future Work Category Year\\nS38 [17]This study examined the effect of a high-frequency tapping\\nsystem on latex yield, biochemistry, and tapping panel dryness\\n(TPD). After conducting experiments in three locations, the\\nresults of latex diagnosis showed relatively unhealthy rubber\\ntrees as they were impacted by the high-frequency tapping\\nsystem. The farmer should consider high-frequency tapping\\nand ensure good decision-making in regard to tapping\\nsystem applications.Protective work 2020\\nS39 [107]The authors conﬁrmed that there is a correlation between the\\ntapper height, the tapping postures of tappers, and the\\noccurrence of musculoskeletal disorders among\\ntapping workers.Rubber tapping related work 2021\\nS40 [66]Transmission structure design and motion simulation analysis\\nof a 4GXJ-2 electric rubber cutter. Test results showed that the\\ncutting depth was well controlled, with no damage to rubber\\ntrees and the error in terms of bark consumption was about 5%.Portable electrical tapping device 2021\\nS41 [88]This article presented a near-range machine vision technique\\nfor rubber tapping automation that detected the tapping line in\\nnear-range images. The authors conducted nighttime rubber\\ntapping line detection in near-range images. Their acquisition\\ntool integrated an RGB-D camera with assisting lights in order\\nto capture images under low-light conditions.Tapping path detection for a\\nself-propelled tapping robot2021\\nS42 [62]The authors presented a novel 3-D Lidar SLAM system for\\nrubber tapping robots. The system achieved the same real-time\\nperformance as state-of-the-art algorithms even without\\nIMU information.Navigation in rubber plantations 2021\\nS43 [90]As a representative case, the autonomous mobile robot\\nconsidered in this work was used to determine the working\\narea and to detect obstacles simultaneously, which was a key\\nfeature for its efﬁcient and safe operation.Obstacle detection for navigation 2021\\nS44 [108]The authors recognized the tapped area and untapped area\\nusing an improved YOLOv5-based tapping trajectory\\ndetection method.Tapping path detection 2022\\nS45 [109]Leaf hyperspectral reﬂectance was combined with machine\\nlearning algorithms to detect and classify the level of South\\nAmerican Leaf Blight, as well as predicted disease-induced\\nphotosynthetic changes in rubber trees.Rubber-tapping-related work 2022\\nS46 [18]The authors presented a rubber tapping robot with a six-axis\\ntandem robot arm and a compact binocular stereo vision\\nsystem. The bark consumption-cutting depth settings of 2.0 and\\n5.0 mm were more appropriate for the rubber tapping robot.\\nThe authors suggested that future work could include\\nimprovements in system stiffness and robustness.Self-propelled rubber tapping robot 2022\\nS47 [110]The authors designed an intelligent rubber tapping machine\\n(RTM), and investigated whether the structural vibration level\\nwas suitable for the real tapping process.Rubber tapping machine 2022\\nS48 [111]A self-propelled rubber tapping robot was proposed that could\\nmove along a row of trees according to a predetermined path\\nand tap each rubber tree.Self-propelled rubber tapping robot 2022', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81acd0a0-ae59-4a8c-8f27-44d7598e48af', embedding=None, metadata={'page_label': '22', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 22 of 26\\nTable A1. Cont.\\nPaper ID Information Extraction and Future Work Category Year\\nS49 [112]This paper used two-dimensional light detection and ranging\\n(Lidar) and a ranging sensor to locate a space position. In the\\nﬁeld tests, the lateral error of positioning was less than 8.86 mm,\\nthe height error was less than 0.72 mm, and the average harvest\\nrate was 98.18%.Self-propelled rubber\\nharvesting robot2022\\nS50 [37]The existing problems and perspectives related to pesticide\\napplication sprayers and physical control equipment were\\nsummarized in this study.Protective research of plant\\ndisease control2022\\nReferences\\n1. Mikhaylov, I.A.; Sukhareva, K.V .; Andriasyan, Y.O.; Popov, A.A.; Vorontsov, N.V . Mechanochemical Modiﬁcation of Natural\\nRubber. In Proceedings of the International Conference on Advanced Materials with Hierarchical Structure for New Technologies\\nand Reliable Structures, Tomsk, Russia, 19–26 September 2016.\\n2. Wongsorat, W.; Suppakarn, N.; Jarukumjorn, K. Effects of compatibilizer type and ﬁber loading on mechanical properties and\\ncure characteristics of sisal ﬁber/natural rubber composites. J. Compos. Mater. 2014 ,48, 2401–2411. [CrossRef]\\n3. Men, X.; Wang, F.; Chen, G.Q.; Zhang, H.B.; Xian, M. Biosynthesis of Natural Rubber: Current State and Perspectives. Int. J. Mol.\\nSci.2019 ,20, 50. [CrossRef] [PubMed]\\n4. Ye, S.; Rogan, J.; Sangermano, F. Monitoring rubber plantation expansion using Landsat data time series and a Shapelet-based\\napproach. ISPRS J. Photogramm. Remote Sens. 2018 ,136, 134–143. [CrossRef]\\n5. Stankevitz, K.; Schoenﬁsch, A.; de Silva, V .; Tharindra, H.; Stroo, M.; Ostbye, T. Prevalence and risk factors of musculoskeletal\\ndisorders among Sri Lankan rubber tappers. Int. J. Occup. Environ. Health 2016 ,22, 91–98. [CrossRef] [PubMed]\\n6. Deng, X.M.; Guo, D.; Yang, S.G.; Shi, M.J.; Chao, J.Q.; Li, H.L.; Peng, S.Q.; Tian, W.M. Jasmonate signalling in the regulation of\\nrubber biosynthesis in laticifer cells of rubber tree, Hevea brasiliensis .J. Exp. Bot. 2018 ,69, 3559–3571. [CrossRef]\\n7. Qi, D.; Zhou, J.; Xie, G.; Wu, Z.X. Optimizing Tapping-Tree Density of Rubber ( Hevea brasiliensis ) Plantations in South China.\\nSmall-Scale For. 2016 ,15, 61–72. [CrossRef]\\n8. Zhang, S.; Zhang, C.; Zhang, J.; Yuan, T.; Li, W.; Wang, D.; Zhang, F. Design and experiment of suspension-typed rubber tapping\\ndevice. Int. Agric. Eng. J. 2018 ,27, 110–118.\\n9. Ali, H.; Davies, D.R. The effects of age, sex and tenure on the job performance of rubber tappers. J. Occup. Organ. Psychol. 2003 ,\\n76, 381–391. [CrossRef]\\n10. Ali, M.F.; Abdul Aziz, A.; Williams, A. Assessing Yield and Yield Stability of Hevea clones in the Southern and Central Regions of\\nMalaysia. Agronomy 2020 ,10, 643. [CrossRef]\\n11. Yu, H.Y.; Hammond, J.; Ling, S.H.; Zhou, S.X.; Mortimer, P .E.; Xu, J.C. Greater diurnal temperature difference, an overlooked but\\nimportant climatic driver of rubber yield. Ind. Crops Prod. 2014 ,62, 14–21. [CrossRef]\\n12. An, F.; Lin, W.F.; Cahill, D.; Rookes, J.; Kong, L.X. Variation of phloem turgor pressure in Hevea brasiliensis : An implication for\\nlatex yield and tapping system optimization. Ind. Crops Prod. 2014 ,58, 182–187. [CrossRef]\\n13. Meksawi, S.; Tangtrakulwanich, B.; Chongsuvivatwong, V . Musculoskeletal problems and ergonomic risk assessment in rubber\\ntappers: A community-based study in southern Thailand. Int. J. Ind. Ergon. 2012 ,42, 129–135. [CrossRef]\\n14. Turjanmaa, K.; Alenius, H.; Reunala, T.; Palosuo, T. Recent developments in latex allergy. Curr. Opin. Allergy Clin. Immunol. 2002 ,\\n2, 407–412. [CrossRef]\\n15. Han, P .P .; Chen, J.S.; Han, Y.; Yi, L.; Zhang, Y.N.; Jiang, X.L. Monitoring rubber plantation distribution on Hainan Island using\\nLandsat OLI imagery. Int. J. Remote Sens. 2018 ,39, 2189–2206. [CrossRef]\\n16. Varghese, A.; Panicker, V . Effect of MSDs and scope of ergonomic interventions among rubber processing workers: A systematic\\nreview. Med. Lavoro. 2022 ,68, e2022032. [CrossRef]\\n17. Rukkhun, R.; Iamsaard, K.; Sdoodee, S.; Mawan, N.; Khongdee, N. Effect of high-frequency tapping system on latex yield, tapping\\npanel dryness, and biochemistry of young hillside tapping rubber. Not. Bot. Horti Agrobot. Cluj-Napoca 2020 ,48, 2359–2367.\\n[CrossRef]\\n18. Zhou, H.; Zhang, S.; Zhang, J.; Zhang, C.; Wang, S.; Zhai, Y.; Li, W. Design, development, and ﬁeld evaluation of a rubber tapping\\nrobot. J. Field Robot. 2022 ,39, 28–54. [CrossRef]\\n19. Liu, Z.; Liu, D.; Zhu, D.; Zhang, L.; Zan, X.; Tong, L. Advances and prospects in ﬁne recognition and automatic mapping of crops\\nby remote sensing. Trans. Chin. Soc. Agric. Mach. 2018 ,49, 1–12. [CrossRef]\\n20. Said, M.E.; Belal, A.; Kotb, A.-E.S.; El-Shirbeny, M.A.; Gad, A.; Zahran, M.B. Smart farming for improving agricultural manage-\\nment. Egypt. J. Remote Sens. Space Sci. 2021 ,24, 971–981. [CrossRef]\\n21. Boursianis, A.D.; Papadopoulou, M.S.; Diamantoulakis, P .; Liopa-Tsakalidi, A.; Barouchas, P .; Salahas, G.; Karagiannidis, G.; Wan,\\nS.; Goudos, S.K. Internet of Things (IoT) and Agricultural Unmanned Aerial Vehicles (UAVs) in smart farming: A comprehensive\\nreview. Internet Things 2020 ,18, 100187. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d202722-1b60-4d31-b1a8-8c572fd08716', embedding=None, metadata={'page_label': '23', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 23 of 26\\n22. He, B.; Bai, K.J. Digital twin-based sustainable intelligent manufacturing: A review. Adv. Manuf. 2021 ,9, 1–21. [CrossRef]\\n23. Zhou, G.H.; Zhang, C.; Li, Z.; Ding, K.; Wang, C. Knowledge-driven digital twin manufacturing cell towards intelligent\\nmanufacturing. Int. J. Prod. Res. 2020 ,58, 1034–1051. [CrossRef]\\n24. Ningbo CIHEVEA Technology Co., Ltd. Available online: http://www.cihevea.com/ (accessed on 2 July 2022).\\n25. Lan, Y.; Zhao, D.; Zhang, Y.; Zhu, J. Exploration and development prospect of ecological unmanned farm model. Trans. Chin. Soc.\\nAgric. Eng. 2021 ,37, 312–327.\\n26. McMullen, A.I. Extraction of latex of Hevea brasiliensis under sterile conditions. Nature 1949 ,164, 715. [CrossRef] [PubMed]\\n27. Gouvea, L.R.L.; Silva, G.A.P .; Verardi, C.K.; Silva, J.Q.; Scaloppi, E.J.; Goncalves, P .D. Temporal stability of vigor in rubber tree\\ngenotypes in the pre- and post-tapping phases using different methods. Euphytica 2012 ,186, 625–634. [CrossRef]\\n28. Ramachandran, P .; Mathur, S.; Francis, L.; Varma, A.; Mathew, J.; Mathew, N.M.; Sethuraj, M.R. Evidence for Association of a\\nViroid with Tapping Panel Dryness Syndrome of Rubber ( Hevea brasiliensis ).Plant Dis. 2000 ,84, 1155. [CrossRef]\\n29. Ru, S.; Li, Z.; Liang, D.; Zhang, Y. Design and analysis of track type rubber tapping machine based on Pro/Mechanica. Manuf.\\nAutom. 2019 ,41, 48–52.\\n30. Wang, X.; Geng, G.; Li, F.; Zhou, X.; Guan, Z. Design of ﬁxed automatic intelligent control rubber tapping machine. Agric. Eng.\\n2020 ,10, 79–84.\\n31. Pramchoo, W.; Geater, A.F.; Harris-Adamson, C.; Tangtrakulwanich, B. Ergonomic rubber tapping knife relieves symptoms of\\ncarpal tunnel syndrome among rubber tappers. Int. J. Ind. Ergon. 2018 ,68, 65–72. [CrossRef]\\n32. Chantuma, P .; Lacote, R.; Leconte, A.; Gohet, E. An innovative tapping system, the double cut alternative, to improve the yield of\\nHevea brasiliensis in Thai rubber plantations. Field Crops Res. 2011 ,121, 416–422. [CrossRef]\\n33. Cheng, J.R.; Cai, K.Q.; Liu, B.Y.; Tang, X.Y. Design and Test of the Intelligent Rubber Tapping Technology Evaluation Equipment\\nBased on Cloud Model. In Cloud Computing and Security ; Springer: Cham, Switzerland, 2017; Volume 10602. [CrossRef]\\n34. Mitchell, S.; Weersink, A.; Erickson, B. Adoption of precision agriculture technologies in Ontario crop production. Can. J. Plant\\nSci.2018 ,98, 1384–1388. [CrossRef]\\n35. Monaghan, J.M.; Daccache, A.; Vickers, L.H.; Hess, T.M.; Weatherhead, E.K.; Grove, I.G.; Knox, J.W. More ‘crop per drop’:\\nConstraints and opportunities for precision irrigation in European agriculture. J. Sci. Food Agric. 2013 ,93, 977–980. [CrossRef]\\n[PubMed]\\n36. Gebbers, R.; Adamchuk, V .I. Precision Agriculture and Food Security. Science 2010 ,327, 828–831. [CrossRef]\\n37. Wang, S.L.; Xu, T.; Li, X. Development Status and Perspectives of Crop Protection Machinery and Techniques for Vegetables.\\nHorticulturae 2022 ,8, 166. [CrossRef]\\n38. Yang, J.; Chen, S.-W.; Zhang, B.; Tu, Q.; Wang, J.; Yuan, M.-S. Non-biological ﬂuorescent chemosensors for pesticides detection.\\nTalanta 2022 ,240, 123200. [CrossRef]\\n39. Zhang, C.L.; Valente, J.; Kooistra, L.; Guo, L.F.; Wang, W.S. Orchard management with small unmanned aerial vehicles: A survey\\nof sensing and analysis approaches. Precis. Agric. 2021 ,22, 2007–2052. [CrossRef]\\n40. Farber, C.; Mahnke, M.; Sanchez, L.; Kurouski, D. Advanced spectroscopic techniques for plant disease diagnostics. A review.\\nTrac-Trends Anal. Chem. 2019 ,118, 43–49. [CrossRef]\\n41. Huang, Y.B.; Chen, Z.X.; Yu, T.; Huang, X.Z.; Gu, X.F. Agricultural remote sensing big data: Management and applications.\\nJ. Integr. Agric. 2018 ,17, 1915–1931. [CrossRef]\\n42. Carrow, R.N.; Krum, J.M.; Flitcroft, I.; Cline, V . Precision turfgrass management: Challenges and ﬁeld applications for mapping\\nturfgrass soil and stress. Precis. Agric. 2010 ,11, 115–134. [CrossRef]\\n43. Huang, Y.B.; Reddy, K.N.; Fletcher, R.S.; Pennington, D. UAV Low-Altitude Remote Sensing for Precision Weed Management.\\nWeed Technol. 2018 ,32, 2–6. [CrossRef]\\n44. Afsah-Hejri, L.; Akbari, E.; Toudeshki, A.; Homayouni, T.; Alizadeh, A.; Ehsani, R. Terahertz spectroscopy and imaging: A review\\non agricultural applications. Comput. Electron. Agric. 2020 ,177, 105628. [CrossRef]\\n45. Quy, V .K.; Hau, N.V .; Anh, D.V .; Quy, N.M.; Ban, N.T.; Lanza, S.; Randazzo, G.; Muzirafuti, A. IoT-Enabled Smart Agriculture:\\nArchitecture, Applications, and Challenges. Appl. Sci. 2022 ,12, 3396. [CrossRef]\\n46. Moher, D.; Liberati, A.; Tetzlaff, J.; Altman, D.G.; Grp, P . Preferred Reporting Items for Systematic Reviews and Meta-Analyses:\\nThe PRISMA Statement (Reprinted from Annals of Internal Medicine). Phys. Ther. 2009 ,89, 873–880. [CrossRef]\\n47. Pavel, M.I.; Tan, S.Y.; Abdullah, A. Vision-Based Autonomous Vehicle Systems Based on Deep Learning: A Systematic Literature\\nReview. Appl. Sci. 2022 ,12, 6831. [CrossRef]\\n48. Putranto, R.A.; Herlinawati, E.; Rio, M.; Leclercq, J.; Piyatrakul, P .; Gohet, E.; Sanier, C.; Oktavia, F.; Pirrello, J.; Montoro, P .\\nInvolvement of Ethylene in the Latex Metabolism and Tapping Panel Dryness of Hevea brasiliensis .Int. J. Mol. Sci. 2015 ,16,\\n17885–17908. [CrossRef]\\n49. Rodrigo, V .H.L.; Kudaligama, K.; Fernando, K.; Yapa, P .A.J. Replacing traditional half spiral cut by a quarter cut with Ethephon;\\na simple approach to solve current issues related to latex harvesting in rubber industry. J. Natl. Sci. Found. Sri Lanka 2012 ,40,\\n283–291. [CrossRef]\\n50. Gouvea, L.R.L.; Silva, G.A.P .; Scaloppi, E.J.; Goncalves, P .D. Different methods to assess yield temporal stability in rubber. Pesqui.\\nAgropecu. Bras. 2011 ,46, 491–498. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='96834c84-f0b9-463f-bea1-24859b4e6d7a', embedding=None, metadata={'page_label': '24', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 24 of 26\\n51. Zhang, C.; Sheng, X.; Zhang, S.; Zhang, F.; Zhang, W.; Zhang, J.; Yuan, T.; Zhang, S.; Yong, L.; Li, W. Design and experiment\\nof portable electric tapping machine. In Proceedings of the ASABE 2018 Annual International Meeting, Detroit, MI, USA,\\n29 July–1 August 2018 ; American Society of Agricultural and Biological Engineers (ASABE): St. Joseph, MI, USA, 2018. [CrossRef]\\n52. Arjun, R.N.; Soumya, S.J.; Vishnu, R.S.; Bhavani, R.R. Semi Automatic Rubber Tree Tapping Machine. In Proceedings of\\nthe 2016 International Conference on Robotics and Automation for Humanitarian Applications (RAHA), Amritapuri, India,\\n18–20 December 2016; pp. 92–96.\\n53. Atsin, G.J.O.; Soumahin, E.F.; Kouakou, H.T.; Coulibaly, L.F.; Traore, S.M.; Alle, J.Y.; N’Guessan, A.E.B.; Kouame, C.; Obouayeba, S.\\nImpact of Reduced Tapping Frequency on Agronomic, Physiological and Biochemical Aspects of Clone PB 260 of Hevea brasiliensis\\nin the Centre West of Cote D’Ivoire. J. Rubber Res. 2014 ,17, 45–56.\\n54. Michels, T.; Eschbach, J.M.; Lacote, R.; Benneveau, A.; Papy, F. Tapping panel diagnosis, an innovative on-farm decision support\\nsystem for rubber tree tapping. Agron. Sustain. Dev. 2012 ,32, 791–801. [CrossRef]\\n55. She, F.H.; Zhu, D.M.; Kong, L.X.; Wang, J.; An, F.; Lin, W.F. Ultrasound-assisted tapping of latex from Para rubber tree Hevea\\nbrasiliensis .Ind. Crops Prod. 2013 ,50, 803–808. [CrossRef]\\n56. Deepthi, S.R.; Dsouza, R.M.D.; Shri, K.A. Automated Rubber tree tapping and latex mixing machine for quality production\\nof natural rubber. In Proceedings of the 2020 IEEE-HYDCON International Conference on Engineering in the 4th Industrial\\nRevolution, Hyderabad, India, 11–12 September 2020. [CrossRef]\\n57. Gao, K.; Sun, J.; Gao, F.; Jiao, J. Tapping error analysis and precision control of ﬁxed tapping robot. Nongye Gongcheng Xuebao\\nTrans. Chin. Soc. Agric. Eng. 2021 ,37, 44–50. [CrossRef]\\n58. Ning, T.; Liang, D.; Zhang, Y.; Fu, W.; Ru, S. Design and experimental research of ﬁxed compound motion track rubber tapping\\nmachine. J. Southwest Univ. 2022 ,44, 100–109. [CrossRef]\\n59. Zhang, X.; Cao, C.; Zhang, L.; Xing, J.; Liu, J.; Dong, X. Design and experiment of copying advanced natural rubber tapping\\nmachine. Trans. Chin. Soc. Agric. Mach. 2022 ,53, 99–108. [CrossRef]\\n60. The Solution of Intelligent Tapping System. Available online: http://www.haribit.com/index.php?c=show&m=view&id=64\\n(accessed on 10 August 2022).\\n61. Susanto, H.; Hanif, S.A. The Design of Flexible Rubber Tapping Tool with Settings the Depth and Thickness Control. In\\nProceedings of the 1st South Aceh International Conference on Engineering and Technology (SAICOET), Politeknik Aceh Selatan,\\nTapak Tuan, Indonesia, 8–9 December 2018.\\n62. Nie, F.; Zhang, W.Y.; Wang, Y.; Shi, Q.H. A Forest 3-D Lidar SLAM System for Rubber-Tapping Robot Based on Trunk Center\\nAtlas. IEEE ASME Trans. Mechatron. 2021 . [CrossRef]\\n63. Chantuma, P .; Lacointe, A.; Kasemsap, P .; Thanisawanyangkura, S.; Gohet, E.; Clement, A.; Guilliot, A.; Ameglio, T.; Thaler, P .\\nCarbohydrate storage in wood and bark of rubber trees submitted to different level of C demand induced by latex tapping. Tree\\nPhysiol. 2009 ,29, 1021–1031. [CrossRef] [PubMed]\\n64. Kanpanon, N.; Kasemsap, P .; Thaler, P .; Kositsup, B.; Gay, F.; Lacote, R.; Epron, D. Carbon isotope composition of latex does not\\nreﬂect temporal variations of photosynthetic carbon isotope discrimination in rubber trees ( Hevea brasiliensis ).Tree Physiol. 2015 ,\\n35, 1166–1175. [CrossRef] [PubMed]\\n65. Gebelin, V .; Leclercq, J.; Kuswanhadi; Argout, X.; Chaidamsari, T.; Hu, S.N.A.; Tang, C.R.; Sarah, G.; Yang, M.; Montoro, P . The\\nsmall RNA proﬁle in latex from Hevea brasiliensis trees is affected by tapping panel dryness. Tree Physiol. 2013 ,33, 1084–1098.\\n[CrossRef] [PubMed]\\n66. Chen, W.; Xiao, S.; Jia, Q.; Deng, X.; Huang, C.; Zheng, Y. Transmission structure design and motion simulation analysis of 4GXJ-2\\nElectric rubber cutter. J. Yancheng Inst. Technol. (JCR-SCI) 2021 ,34, 24–31. [CrossRef]\\n67. Cao, J.; Zhang, Y.; Wang, W.; Xiao, S.; Wu, S.; Xiao, H. Research on portable tapping machine for natural rubber harvesting. J. Chin.\\nAgric. Mech. 2020 ,41, 20–27. [CrossRef]\\n68. Zhang, C.; Li, D.; Zhang, S.; Shui, Y.; Tan, Y.; Li, W. Design and experiment of 3-coordinate linkage rubber cutting device based on\\nlaser ranging. Trans. Chin. Soc. Agric. Mach. 2019 ,50, 121–127.\\n69. Bengochea-Guevara, J.M.; Conesa-Munoz, J.; Andujar, D.; Ribeiro, A. Merge Fuzzy Visual Servoing and GPS-Based Planning to\\nObtain a Proper Navigation Behavior for a Small Crop-Inspection Robot. Sensors 2016 ,16, 276. [CrossRef] [PubMed]\\n70. He, W.; Li, Z.J.; Chen, C.L.P . A Survey of Human-centered Intelligent Robots: Issues and Challenges. IEEE-CAA J. Autom. Sin.\\n2017 ,4, 602–609. [CrossRef]\\n71. Juman, M.A.; Wong, Y.W.; Rajkumar, R.K.; Goh, L.J. A novel tree trunk detection method for oil-palm plantation navigation.\\nComput. Electron. Agric. 2016 ,128, 172–180. [CrossRef]\\n72. Simon, S. Autonomous navigation in rubber plantations. In Proceedings of the ICMLC 2010—The 2nd International Conference\\non Machine Learning and Computing, Bangalore, India, 9–11 February 2010; pp. 309–312. [CrossRef]\\n73. Zhang, C.L.; Yong, L.Y.; Chen, Y.; Zhang, S.L.; Ge, L.Z.; Wang, S.; Li, W. A Rubber-Tapping Robot Forest Navigation and\\nInformation Collection System Based on 2D LiDAR and a Gyroscope. Sensors 2019 ,19, 2136. [CrossRef] [PubMed]\\n74. Kunghun, W.; Tantrapiwat, A.; Chaidilokpattanakul, P . Navigation of autonomous vehicle for rubber tree orchard. In Proceed-\\nings of the 5th International Conference on Engineering, Applied Sciences and Technology (ICEAST), Luang Prabang, Laos,\\n2–5 July 2019.\\n75. Terentev, A.; Dolzhenko, V .; Fedotov, A.; Eremenko, D. Current State of Hyperspectral Remote Sensing for Early Plant Disease\\nDetection: A Review. Sensors 2022 ,22, 757. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ebe4dcf1-2ad1-485e-9315-435084aff70f', embedding=None, metadata={'page_label': '25', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 25 of 26\\n76. Kunghun, W.; Tantrapiwat, A. Development of a vision based mapping in rubber tree orchard. In Proceedings of the ICEAST\\n2018—4th International Conference on Engineering, Applied Sciences and Technology: Exploring Innovative Solutions for Smart\\nSociety, Phuket, Thailand, 4–7 July 2018. [CrossRef]\\n77. Tsai, C.-C.; Huang, H.-C.; Chan, C.-K. Parallel Elite Genetic Algorithm and Its Application to Global Path Planning for Au-\\ntonomous Robot Navigation. IEEE Trans. Ind. Electron. 2011 ,58, 4813–4821. [CrossRef]\\n78. Liu, L.; Yao, J.; He, D.; Chen, J.; Huang, J.; Xu, H.; Wang, B.; Guo, J. Global Dynamic Path Planning Fusion Algorithm Combining\\nJump-A* Algorithm and Dynamic Window Approach. IEEE Access 2021 ,9, 19632–19638. [CrossRef]\\n79. Kobayashi, M.; Motoi, N. Local Path Planning Method Based on Virtual Manipulators and Dynamic Window Approach for a\\nWheeled Mobile Robot. In Proceedings of the 2021 IEEE/SICE International Symposium on System Integration (SII), Iwaki, Japan,\\n11–14 January 2021; pp. 499–504. [CrossRef]\\n80. Ling, F.; Du, C.; Chen, J.; Yuan, Z. An Improved Geometrical Path Planning Algorithm for UAV in Irregular-obstacle Environment.\\nIn Proceedings of the 2019 IEEE 8th Joint International Information Technology and Artiﬁcial Intelligence Conference (ITAIC\\n2019), Chongqing, China, 24–26 May 2019; pp. 972–976.\\n81. Lin, P .; Choi, W.Y.; Chung, C.C. Local Path Planning Using Artiﬁcial Potential Field for Waypoint Tracking with Collision\\nAvoidance. In Proceedings of the 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), Rhodes,\\nGreece, 20–23 September 2020.\\n82. Wang, N.; Xu, H. Dynamics-Constrained Global-Local Hybrid Path Planning of an Autonomous Surface Vehicle. IEEE Trans. Veh.\\nTechnol. 2020 ,69, 6928–6942. [CrossRef]\\n83. Tafreshi, F.A.; Fatahi, Z.; Ghasemi, S.F.; Taherian, A.; Esfandiari, N. Ultrasensitive ﬂuorescent detection of pesticides in real sample\\nby using green carbon dots. PLoS ONE 2020 ,15, e0230646. [CrossRef]\\n84. Khudri, N.A.F.R.; Mohd Masri, M.M.; Maidin, M.S.T.; Kamarudin, N.; Hussain, M.H.; Abd Ghani, I.; Jalinas, J. Preliminary\\nevaluation of acoustic sensors for early detection of red palm weevil, Rhynchophorus ferrugineus incidence on oil palm and coconut\\nin Malaysia. Intern. J. Trop. Insect Sci. 2021 ,41, 3287–3292. [CrossRef]\\n85. Liu, S.; Atia, M.M.; Karamat, T.B.; Noureldin, A. A LiDAR-Aided Indoor Navigation System for UGVs. J. Navig. 2015 ,68, 253–273.\\n[CrossRef]\\n86. Wongtanawijit, R.; Kaorapapong, T. Rubber Tapped Path Detection using K-means Color Segmentation and Distance to Boundary\\nFeature. In Proceedings of the 2018 15th International Conference on Electrical Engineering/Electronics, Computer, Telecommu-\\nnications and Information Technology (ECTI-Con), Chiang Rai, Thailand, 18–21 July 2018; pp. 130–133.\\n87. Wongtanawijit, R.; Khaorapapong, T. Rubber Tapping Position and Harvesting Cup Detection Using Faster-RCNN with Mo-\\nbileNetV2. In Proceedings of the ICSEC 2019—23rd International Computer Science and Engineering Conference, Phuket,\\nThailand, 30 October–1 November 2019; pp. 335–339. [CrossRef]\\n88. Wongtanawijit, R.; Khaorapapong, T. Nighttime rubber tapping line detection in near-range images Near-Range tapping line\\nshadow acquisition technique with tapping line detection algorithm for automatic rubber tapping robot in nighttime. Multimed.\\nTools Appl. 2021 ,80, 29401–29422. [CrossRef]\\n89. Nissimov, S.; Goldberger, J.; Alchanatis, V . Obstacle detection in a greenhouse environment using the Kinect sensor. Comput.\\nElectron. Agric. 2015 ,113, 104–115. [CrossRef]\\n90. Skoczen, M.; Ochman, M.; Spyra, K.; Nikodem, M.; Krata, D.; Panek, M.; Pawlowski, A. Obstacle Detection System for Agricultural\\nMobile Robot Application Using RGB-D Cameras. Sensors 2021 ,21, 5292. [CrossRef] [PubMed]\\n91. Al-Kaff, A.; Garcia, F.; Martin, D.; De la Escalera, A.; Maria Armingol, J. Obstacle Detection and Avoidance System Based on\\nMonocular Camera and Size Expansion Algorithm for UAVs. Sensors 2017 ,17, 1061. [CrossRef]\\n92. Zhang, L.; Li, D. Research on Mobile Robot Target Recognition and Obstacle Avoidance Based on Vision. J. Internet Technol. 2018 ,\\n19, 1879–1892. [CrossRef]\\n93. Radcliffe, J.; Cox, J.; Bulanon, D.M. Machine vision for orchard navigation. Comput. Ind. 2018 ,98, 165–171. [CrossRef]\\n94. Liu, L.; Mei, T.; Niu, R.X.; Wang, J.; Liu, Y.B.; Chu, S. RBF-Based Monocular Vision Navigation for Small Vehicles in Narrow Space\\nbelow Maize Canopy. Appl. Sci. 2016 ,6, 182. [CrossRef]\\n95. Zhou, B.; He, Y.; Huang, W.C.; Yu, X.; Fang, F.; Li, X.M. Place recognition and navigation of outdoor mobile robots based on\\nrandom Forest learning with a 3D LiDAR. J. Intell. Robot. Syst. 2022 ,104, 72. [CrossRef]\\n96. Zhou, S.Y.; Xi, J.Q.; McDaniel, M.W.; Nishihata, T.; Salesses, P .; Iagnemma, K. Self-Supervised Learning to Visually Detect Terrain\\nSurfaces for Autonomous Robots Operating in Forested Terrain. J. Field Robot. 2012 ,29, 277–297. [CrossRef]\\n97. Li, S.T.; Zhang, J.; Sun, L.; Liu, Y.N. Study on the Secant Segmentation Algorithm of Rubber Tree. In Proceedings of the 2nd\\nInternational Conference on Machine Vision and Information Technology (CMVIT), Hong Kong, China, 23–25 February 2018.\\n98. Severo, E.T.D.; Oliveira, E.F.; Sansigolo, C.A.; Rocha, C.D.; Calonego, F.W. Properties of juvenile and mature woods of Hevea\\nbrasiliensis untapped and with tapping panels. Eur. J. Wood Wood Prod. 2013 ,71, 815–818. [CrossRef]\\n99. Cai, Z.Y.; Li, G.H.; Lin, C.H.; Shi, T.; Zhai, L.G.; Chen, Y.P .; Huang, G.X. Identifying pathogenicity genes in the rubber tree\\nanthracnose fungus Colletotrichum gloeosporioides through random insertional mutagenesis. Microbiol. Res. 2013 ,168, 340–350.\\n[CrossRef] [PubMed]\\n100. Gebelin, V .; Leclercq, J.; Hu, S.N.; Tang, C.R.; Montoro, P . Regulation of MIR Genes in Response to Abiotic Stress in Hevea\\nbrasiliensis .Int. J. Mol. Sci. 2013 ,14, 19587–19604. [CrossRef] [PubMed]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='35a7025f-86aa-4e02-9deb-6b13824e5e73', embedding=None, metadata={'page_label': '26', 'file_name': 'applsci-12-09304-v2.pdf', 'file_path': '/content/data/applsci-12-09304-v2.pdf', 'file_type': 'application/pdf', 'file_size': 8630817, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appl. Sci. 2022 ,12, 9304 26 of 26\\n101. Mohammed, C.L.; Rimbawanto, A.; Page, D.E. Management of basidiomycete root- and stem-rot diseases in oil palm, rubber and\\ntropical hardwood plantation crops. For. Pathol. 2014 ,44, 428–446. [CrossRef]\\n102. Silva, G.A.P .; Gouvea, L.R.L.; Verardi, C.K.; de Oliveira, A.L.B.; Goncalves, P .D. Annual growth increment and stability of rubber\\nyield in the tapping phase in rubber tree clones: Implications for early selection. Ind. Crops Prod. 2014 ,52, 801–808. [CrossRef]\\n103. Sajitha, K.L.; Dev, S.A.; Florence, E.J. Biocontrol potential of Bacillus subtilis B1 against sapstain fungus in rubber wood. Eur. J.\\nPlant Pathol. 2018 ,150, 237–244. [CrossRef]\\n104. Wu, C.T.; Ban, S.; Gao, X.S.; Li, W.G. Correlation between DNA methylation status and rubber yield and related characteristics in\\nHevea brasiliensis tapped at different heights. Ind. Crops Prod. 2018 ,111, 563–572. [CrossRef]\\n105. Zhang, J.; Liu, Y.; Xing, H. Application of Improved 2-D Entropy Algorithm in Rubber Tree Image Segmentation. In proceeding\\nof the International Conference of Safety Produce Informatization, Chongqing, China, 28–30 November 2019.\\n106. Wang, L.; Cao, J.; Zheng, Y.; Huang, C.; Wu, S. The Theoretical Research on Technical Advance and Innovation Integration of\\nTapping Machinery. IOP Conf. Ser. Mater. Sci. Eng. 2019 ,592, 012068. [CrossRef]\\n107. Varghese, A.; Panicker, V .V . Computer-Aided Ergonomic Analysis for Rubber Tapping Workers. In Advanced Manufacturing\\nSystems and Innovative Product Design ; Deepak, B.B.V .L., Parhi, D.R.K., Biswal, B.B., Eds.; Springer: Singapore, 2021; pp. 293–302.\\n[CrossRef]\\n108. Sun, Z.; Yang, H.; Zhang, Z.; Liu, J.; Zhang, X. An Improved YOLOv5-Based Tapping Trajectory Detection Method for Natural\\nRubber Trees. Agriculture 2022 ,12, 1309. [CrossRef]\\n109. Sterling, A.; Di Rienzo, J.A. Prediction of South American Leaf Blight and Disease-Induced Photosynthetic Changes in Rubber\\nTree, Using Machine Learning Techniques on Leaf Hyperspectral Reﬂectance. Plants 2022 ,11, 329. [CrossRef]\\n110. Chong, Z.C.; Ali, W.M.A.; Mazlan, A.Z.A. Structural Vibration Study of a New Concept Intelligent Rubber Tapping Machine.\\nInSymposium on Intelligent Manufacturing and Mechatronics ; Ali Mokhtar, M.N., Jamaludin, Z., Abdul Aziz, M.S., Maslan, M.N.,\\nRazak, J.A., Eds.; Springer: Singapore, 2022; pp. 305–312. [CrossRef]\\n111. Angel, T.S.; Amrithesh, K.; Krishna, K.; Ashok, S.; Vignesh, M. Artiﬁcial Intelligence-Based Rubber Tapping Robot. In Inventive\\nCommunication and Computational Technologies ; Ranganathan, G., Fernando, X., Shi, F., Eds.; Springer: Singapore, 2022; Volume 311,\\npp. 427–438. [CrossRef]\\n112. Wang, S.; Zhou, H.; Zhang, C.L.; Ge, L.Z.; Li, W.; Yuan, T.; Zhang, W.Q.; Zhang, J.X. Design, development and evaluation of latex\\nharvesting robot based on ﬂexible Toggle. Robot. Auton. Syst. 2022 ,147, 103906. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da6f1611-debd-4191-a3fd-39df3ac73610', embedding=None, metadata={'page_label': '1', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Citation: Hussain, M. YOLO-v1 to\\nYOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward\\nDigital Manufacturing and Industrial\\nDefect Detection. Machines 2023 ,11,\\n677. https://doi.org/10.3390/\\nmachines11070677\\nAcademic Editor: Sang Do Noh\\nReceived: 30 May 2023\\nRevised: 15 June 2023\\nAccepted: 21 June 2023\\nPublished: 23 June 2023\\nCopyright: © 2023 by the author.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nmachines\\nReview\\nYOLO-v1 to YOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward Digital Manufacturing and\\nIndustrial Defect Detection\\nMuhammad Hussain\\nDepartment of Computer Science, School of Computing and Engineering, University of Huddersﬁeld,\\nQueensgate, Huddersﬁeld HD1 3DH, UK; m.hussain@hud.ac.uk\\nAbstract: Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has\\nrapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned\\nby the principle of real-time and high-classiﬁcation performance, based on limited but efﬁcient\\ncomputational parameters. This principle has been found within the DNA of all YOLO variants\\nwith increasing intensity, as the variants evolve addressing the requirements of automated quality\\ninspection within the industrial surface defect detection domain, such as the need for fast detection,\\nhigh accuracy, and deployment onto constrained edge devices. This paper is the ﬁrst to provide an\\nin-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from\\nthe perspective of industrial manufacturing. The review explores the key architectural advancements\\nproposed at each iteration, followed by examples of industrial deployment for surface defect detection\\nendorsing its compatibility with industrial requirements.\\nKeywords: industrial defect detection; object detection; smart manufacturing; quality inspection\\n1. Introduction\\nHumans via the visual cortex, a primary cortical region of the brain responsible for\\nprocessing visual information [ 1], are able to observe, recognize [ 2], and differentiate\\nbetween objects instantaneously [ 3]. Studying the inner workings of the visual cortex and\\nthe brain in general has paved the way for artiﬁcial neural networks (ANNs) [ 4] along\\nwith a myriad of computational architectures residing under the deep learning umbrella.\\nIn the last decade, owing to rapid and revolutionary advancements in the ﬁeld of deep\\nlearning [5], researchers have exerted their efforts on providing efﬁcient simulation of the\\nhuman visual system to computers, i.e., enabling computers to detect objects of interest\\nwithin static images and video [ 6], a ﬁeld known as computer vision (CV) [ 7]. CV is\\na prevalent research area for deep learning researchers and practitioners in the present\\ndecade. It is composed of subﬁelds consisting of image classiﬁcation [ 8], object detection [ 9],\\nand object segmentation [ 10]. All three ﬁelds share a common architectural theme, namely,\\nmanipulation of convolutional neural networks (CNNs) [ 11]. CNNs are accepted as the de\\nfacto when dealing with image data. In comparison with conventional image processing\\nand artiﬁcial defection methods, CNNs utilize multiple convolutional layers coupled with\\naggregation, i.e., pooling structures aiming to unearth deep semantic features hidden away\\nwithin the pixels of the image [12].\\nArtiﬁcial intelligence (AI) has found opportunities in industries across the spectrum\\nfrom renewable energy [ 13,14] and security to healthcare [ 15] and the education sector.\\nHowever, one industry that is poised for signiﬁcant automation through CV is the manu-\\nfacturing industry. Quality inspection (QI) is an integral part of any manufacturing domain\\nproviding integrity and conﬁdence to the clients on the quality of the manufactured prod-\\nucts [ 16]. Manufacturing has wide scope for automation; however, when dealing with\\nsurface inspection [ 17], defects can take sophisticated forms [ 18], making human-based\\nMachines 2023 ,11, 677. https://doi.org/10.3390/machines11070677 https://www.mdpi.com/journal/machines', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='323de817-942d-4045-a732-f38ce9ed57bf', embedding=None, metadata={'page_label': '2', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 2 of 25\\nquality inspection a cumbersome task with manifold inefﬁciencies linked to human bias,\\nfatigue, cost, and downtime [ 19]. These inefﬁciencies provide an opportunity for CV-based\\nsolutions to present automated quality inspection that can be integrated within existing\\nsurface defect inspection processes, increasing efﬁciency whilst overcoming bottlenecks\\npresented via conventional inspection methodologies [20].\\nHowever, for success, CV-based architectures must conform to a stringent set of\\ndeployment requirements that can vary from one manufacturing sector to another [ 21]. In\\nthe majority of applications, the focus is not only on the determination of the defect, but also\\non multiple defects along with the locality details of each [ 22]. Therefore, object detection\\nis preferred over image classiﬁcation since the latter only focuses on determination of\\nobject within the image without providing any locality information. Architectures within\\nthe object detection domain can be classiﬁed into single-stage or two-stage detectors [ 23].\\nTwo-stage detectors split the detection process into two stages: Feature extraction/proposal\\nfollowed by regression and classiﬁcation for acquiring the output [ 24]. Although this can\\nprovide high accuracy, it comes with a high computational demand making it inefﬁcient for\\nreal-time deployment onto constrained edge devices. Single-stage detectors, on the other\\nhand, merge the two processes into one, enabling the classiﬁcation and regression via a\\nsingle pass, signiﬁcantly reduce the computational demand, and provide a more compelling\\ncase for production-based deployment [ 25]. Although many single-stage detectors have\\nbeen introduced, such as single shot detector (SSD) [ 26], deconvolutional single shot\\ndetector (D-SSD) [ 27], and RetinaNet [ 28], the YOLO (You Only Look Once) [ 29] family of\\narchitectures seems to be gaining high traction due to its high compatibility with industrial\\nrequirements, such as accuracy, lightweight, and edge-friendly deployment conditions.\\nThe last half-a-decade has been dominated by the introduction of YOLO variants, with the\\nmost recent variant introduced in 2022 as YOLO-v8.\\nTo the best of our knowledge, there is no cohesive review of the advancing YOLO\\nvariants, benchmarking technical advancements, and their implications on industrial\\ndeployment. This paper reviews the YOLO variants released to the present date, focusing\\non presenting the key technical contributions of each YOLO iteration and its impact on key\\nindustrial metrics required for deployment, such as accuracy, speed, and computational\\nefﬁcacy. As a result, the aim is to provide researchers and practitioners with a better\\nunderstanding of the inner workings of each variant, enabling them to select the most\\nrelevant architecture based on their industrial requirements. Additionally, literature on\\nthe deployment of YOLO architectures for various industrial surface defect detection\\napplications is presented.\\nThe subsequent structure of the review is as follows. The ﬁrst section provides an\\nintroduction to single- and two-stage detectors and the anatomy for single-stage object\\ndetectors. Next, the evolution of YOLO variants is presented, detailing the key contributions\\nfrom YOLO-v1 to YOLO-v8, followed by a review of the literature focused on YOLO-based\\nimplementation of industrial surface defect detection. Finally, the discussion section\\nfocuses on summarizing the reviewed literature, followed by extracted conclusions, future\\ndirections, and challenges are presented.\\nObject Detection\\nCNNs can be categorized as convolution-based feed forward neural networks for\\nclassiﬁcation purposes [ 30]. The input layer is followed by multiple convolutional layers\\nto acquire an increased set of smaller-scale feature maps. These feature maps post further\\nmanipulation are transformed into one-dimensional feature vectors before being used as\\ninput to the fully connected layer(s). The process of feature extraction and feature map\\nmanipulation is vital to the overall accuracy of the network; therefore, this can involve the\\nstacking of multiple convolutional and pooling layers for richer feature maps. Popular\\narchitectures for feature extraction include AlexNet [ 31], VGGNet [ 32], GoogleNet [ 33], and\\nResNet [ 34]. AlexNet is proposed in 2012 and consists of ﬁve convolutional, three pooling,\\nand three fully connected layers primarily utilized for image classiﬁcation tasks. VGGNet', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dcfc4093-f182-4b9b-976f-91a454e456df', embedding=None, metadata={'page_label': '3', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 3 of 25\\nfocused on performance enhancement by increasing the internal depth of the architecture,\\nintroducing several variants with increased layers, VGG-16/19. GoogleNet introduced the\\ncascading concept by cascading multiple ‘inception’ modules, whilst ResNet introduced\\nthe concept of skip-connections for preserving information and making it available from\\nthe earlier to the later layers of the architecture.\\nThe motive for an object detector is to infer whether the object(s) of interest are\\nresiding in the image or present the frame of a video. If the object(s) of interest are\\npresent, the detector returns the respective class and locality, i.e., location dimensions\\nof the object(s). Object detection can be further divided into two sub-categories: Two-\\nstage methods and one-stage methods as shown in Figure 1. The former initiates the\\nﬁrst stage with the selection of numerous proposals, then in the second stage, performs\\nprediction on the proposed regions. Examples of two-stage detectors include the famous\\nR-CNN [ 35] variants, such as Fast R-CNN [ 36] and Faster R-CNN [ 37], boasting high\\naccuracies but low computational efﬁciency. The latter transforms the task into a regression\\nproblem, eliminating the need for an initial stage dedicated to selecting candidate regions;\\ntherefore, the candidate selection and prediction is achieved in a single pass. As a result,\\narchitectures falling into this category are computationally less demanding, generating\\nhigher FPS and detection speed, but in general the accuracy tends to be inferior with respect\\nto two-stage detectors.\\nMachines 2023 , 11, x FOR PEER REVIEW 3 of 26 \\n \\n and ResNet [34]. AlexNet is prop osed in 2012 and consists of ﬁve convolutional, three \\npooling, and three fully connected layers primarily utilized for image classi ﬁcation tasks. \\nVGGNet focused on performance enhancement by increasing the internal depth of the \\narchitecture, introducing several variants with increased layers, VGG-16/19. GoogleNet \\nintroduced the cascading concept by cascading multiple ‘inception’ modules, whilst Res-\\nNet introduced the concept of skip-connections  for preserving information and making it \\navailable from the earlier to the later layers of the architecture. \\nThe motive for an object detector is to infe r whether the object(s) of interest are resid-\\ning in the image or present the frame of a video. If the object(s) of interest are present, the \\ndetector returns the respective class and locality , i.e., location dimensions of the object(s). \\nObject detection can be further divided into two sub-categories: Two-stage methods and one-stage methods as shown in Figure 1. The former initiates the ﬁrst stage with the se-\\nlection of numerous proposals, then in the second stage, performs prediction on the pro-\\nposed regions. Examples of two-stage detect ors include the famous R-CNN [35] variants, \\nsuch as Fast R-CNN [36] and Faster R-CNN [37], boasting high accuracies but low com-\\nputational e ﬃciency. The la tter transforms the task into a regression problem, eliminating \\nthe need for an initial stage dedicated to selecting candidate regions; therefore, the candi-\\ndate selection and prediction is achieved in a single pass. As a result, architectures falling \\ninto this category are computationally less demanding, generating higher FPS and detec-\\ntion speed, but in general the accuracy tends to be inferior with respect to two-stage de-\\ntectors. \\n \\nFigure 1. Object detector anatomy. \\n2. Original YOLO Algorithm \\nYOLO was introduced to the computer vision community via a paper release in 2015 \\nby Joseph Redmon et al. [29] titled ‘You Only Look Once: Uni ﬁed, Real-Time Object De-\\ntection’. The paper reframed object detection, presenting it essentially as a single pass re-\\ngression problem, initiating with image pixels and moving to bounding box and class \\nprobabilities. The proposed approach based on the ‘uni ﬁed’ concept enabled the simulta-\\nneous prediction of multiple bounding boxe s and class probabilities, improving both \\nspeed and accuracy. \\nSince its inception in 2016 until the present year (2023), the YOLO family has contin-\\nued to evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further \\nwork within the computer vision domain at YOLO-v3 [38], the e ﬀectiveness and potential \\nof the core ‘uni ﬁed’ concept have been further developed by several authors, with the \\nlatest addition to the YOLO family coming in  the form of YOLO-v8. Figure 2 presents the \\nYOLO evolution timeline. \\nFigure 1. Object detector anatomy.\\n2. Original YOLO Algorithm\\nYOLO was introduced to the computer vision community via a paper release in 2015 by\\nJoseph Redmon et al. [ 29] titled ‘You Only Look Once: Uniﬁed, Real-Time Object Detection’.\\nThe paper reframed object detection, presenting it essentially as a single pass regression\\nproblem, initiating with image pixels and moving to bounding box and class probabilities.\\nThe proposed approach based on the ‘uniﬁed’ concept enabled the simultaneous prediction\\nof multiple bounding boxes and class probabilities, improving both speed and accuracy.\\nSince its inception in 2016 until the present year (2023), the YOLO family has continued\\nto evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further work\\nwithin the computer vision domain at YOLO-v3 [ 38], the effectiveness and potential of\\nthe core ‘uniﬁed’ concept have been further developed by several authors, with the latest\\naddition to the YOLO family coming in the form of YOLO-v8. Figure 2 presents the YOLO\\nevolution timeline.\\n2.1. Original YOLO\\nThe core principle proposed by YOLO-v1 was the imposing of a grid cell with dimen-\\nsions of s×s onto the image. In the case of the center of the object of interest falling into one\\nof the grid cells, that particular grid cell would be responsible for the detection of that object.\\nThis permitted other cells to disregard that object in the case of multiple appearances.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34ca3b18-3a82-47ea-8364-0efe4781c93e', embedding=None, metadata={'page_label': '4', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 4 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 4 of 26 \\n \\n  \\nFigure 2. YOLO evolution timeline. \\n2.1. Original YOLO \\nThe core principle proposed by YOLO-v1 wa s the imposing of a grid cell with di-\\nmensions of s×s onto the image. In the case of  the center of the object of interest falling \\ninto one of the grid cells, that particular grid cell would be responsible for the detection \\nof that object. This permi tted other cells to disregard that object in the case of multiple \\nappearances. \\nFor implementation of object detection, each grid cell would predict B bounding \\nboxes along with the dimensions and con ﬁdence scores. The con ﬁdence score was indic-\\native of the absence or presence of an object within the bounding box. Therefore, the con-\\nﬁdence score can be expressed as Equation (1): \\n𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒 𝑠𝑐𝑜𝑟𝑒 = 𝑝 (𝑜𝑏𝑗𝑒𝑐𝑡 )∗𝐼 𝑜 𝑈\\u0be3\\u0be5\\u0bd8ௗ௧\\u0be5௨௧\\u0bdb  (1)\\nwhere 𝑝(𝑜𝑏𝑗𝑒𝑐𝑡 ) signi ﬁed the probability of the object being present, with a range of 0–1 \\nwith 0 indicating that the object is not present and 𝐼𝑜𝑈\\u0be3\\u0be5\\u0bd8ௗ௧\\u0be5௨௧\\u0bdb represented the intersection-\\nover-union with the predicted bounding box with respect to the ground truth bounding \\nbox. \\nEach bounding box consisted of ﬁve components ( x, y, w, h, and the  conﬁdence score) \\nwith the ﬁrst four components  corresponding to center coordinates ( x, y, width, and height ) \\nof the respective bounding box as shown in Figure 3.  \\nFigure 2. YOLO evolution timeline.\\nFor implementation of object detection, each grid cell would predict Bbounding boxes\\nalong with the dimensions and conﬁdence scores. The conﬁdence score was indicative of\\nthe absence or presence of an object within the bounding box. Therefore, the conﬁdence\\nscore can be expressed as Equation (1):\\ncon f idence score =p(object )∗IoUtruth\\npred(1)\\nwhere p(object )signiﬁed the probability of the object being present, with a range of 0–1 with\\n0 indicating that the object is not present and IoUtruth\\npredrepresented the intersection-over-\\nunion with the predicted bounding box with respect to the ground truth bounding box.\\nEach bounding box consisted of ﬁve components ( x,y,w,h,and the conﬁdence score)\\nwith the ﬁrst four components corresponding to center coordinates ( x,y,width, and height )\\nof the respective bounding box as shown in Figure 3.\\nMachines 2023 , 11, x FOR PEER REVIEW 5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture. \\nAs alluded to earlier, the input image is split into s × s grid cells (default = 7 × 7), with \\neach cell predicting B bounding boxes, each containing ﬁve parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore, the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠× (5∗𝐵+𝐶 )  (2)\\nConsidering the example of YOLO network wi th each cell boundi ng box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-eter output would be give n as expressed in (3): \\n7×7×( 5∗2+8 0 )   (3)\\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Ther efore, two sets of bounding box vectors are \\nrequired, i.e., vector y is the representative of ground truth and vector 𝑦ሶ is the predicted \\nvector. To address multiple bounding boxes containing no object or the same object, \\nYOLO opts for non-maximum suppression (NMS). By de ﬁning a threshold value for \\nNMS, all overlapping predic ted bounding boxes with an IoU lower than the de ﬁned NMS \\nvalue are eliminated. \\nThe original YOLO based on the Darknet framework consisted of two sub-variants. \\nThe ﬁrst architecture comprised of 24 convolutional layers with the ﬁnal layer providing \\na connection into the ﬁrst of the two fully connected layers. Whereas the ‘Fast YOLO’ var-\\niant consisted of only nine co nvolutional layers hosting fewer ﬁlters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1   convolutional layers was imple-\\nmented for reducing the resultant feature sp ace from the preceding layers. The prelimi-\\nnary architecture for YOLO-v1 is presented in Figure 3. \\nTo address the issue of multiple bounding boxes for the same object or with a con ﬁ-\\ndence score of zero, i.e., no object, the authors decided to greatly penalize predictions from bounding boxes containing objects ( 𝛾\\n\\u0bd6\\u0be2\\u0be2\\u0be5ௗ =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾\\u0be1\\u0be2\\u0be2\\u0bd5\\u0bdd =0 . 5 ). The authors calculated the loss function by taking the \\nsum of all bounding  box parameters ( x, y, width , height , conﬁdence score, and class prob-\\nability). As a result, the ﬁrst part of the equation computes the loss of the bounding box \\nprediction with respect to the ground tr uth bounding box based on the coordinates \\n𝑥\\u0bd6\\u0bd8\\u0be1௧\\u0bd8\\u0be5 , 𝑦\\u0bd6\\u0bd8\\u0be1௧\\u0bd8\\u0be5 . 𝕝\\u0bdc\\u0bdd\\u0be2\\u0bd5\\u0bdd is set as 1 in the case of the object residing within  𝑗௧\\u0bdb bounding box \\nprediction in 𝑖௧\\u0bdb cell; otherwise, it is set as 0. The selected, i.e., predicted bounding box \\nwould be tasked with predicting an object wi th the greatest IoU, as expressed in (4): \\n𝛾\\u0bd6\\u0be2\\u0be2\\u0be5ௗ ∑∑ 𝕝\\u0bdc\\u0bdd\\u0be2\\u0bd5\\u0bdd[(𝑥\\u0bdc−𝑥 పෝ)ଶ+(𝑦\\u0bdc−𝑦 పෝ)ଶ]\\u0bbb\\n\\u0bddୀ\\u0b34ௌమ\\n\\u0bdcୀ\\u0b34   (4)\\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding compon ent. However, the scale of \\nerror in the large boxes has lesser impact co mpared to the small boxes. The normalization \\nof width and height between the range 0 and 1 indicates that their square roots increase \\nFigure 3. YOLO-v1 preliminary architecture.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec2d46e7-89b0-4616-a3a0-a0a17608c2d7', embedding=None, metadata={'page_label': '5', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 5 of 25\\nAs alluded to earlier, the input image is split into s ×s grid cells (default = 7 ×7),\\nwith each cell predicting Bbounding boxes, each containing ﬁve parameters and sharing\\nprediction probabilities of classes ( C). Therefore, the parameter output would take the\\nfollowing form, expressed in (2):\\ns×s×(5∗B+C) (2)\\nConsidering the example of YOLO network with each cell bounding box prediction\\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter\\noutput would be given as expressed in (3):\\n7×7×(5∗2+80) (3)\\nThe fundamental motive of YOLO and object detection in general is the object detection\\nand localization via bounding boxes. Therefore, two sets of bounding box vectors are\\nrequired, i.e., vector y is the representative of ground truth and vector.yis the predicted\\nvector. To address multiple bounding boxes containing no object or the same object, YOLO\\nopts for non-maximum suppression (NMS). By deﬁning a threshold value for NMS, all\\noverlapping predicted bounding boxes with an IoU lower than the deﬁned NMS value\\nare eliminated.\\nThe original YOLO based on the Darknet framework consisted of two sub-variants.\\nThe ﬁrst architecture comprised of 24 convolutional layers with the ﬁnal layer providing\\na connection into the ﬁrst of the two fully connected layers. Whereas the ‘Fast YOLO’\\nvariant consisted of only nine convolutional layers hosting fewer ﬁlters each. Inspired\\nby the inception module in GoogleNet, a sequence of 1×1convolutional layers was\\nimplemented for reducing the resultant feature space from the preceding layers. The\\npreliminary architecture for YOLO-v1 is presented in Figure 3.\\nTo address the issue of multiple bounding boxes for the same object or with a conﬁ-\\ndence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\\nbounding boxes containing objects ( γcoord =5) and the lowest penalization for prediction\\ncontaining no object ( γnoobj =0.5). The authors calculated the loss function by taking\\nthe sum of all bounding box parameters ( x,y,width ,height , conﬁdence score, and class\\nprobability). As a result, the ﬁrst part of the equation computes the loss of the bounding box\\nprediction with respect to the ground truth bounding box based on the coordinates xcenter ,\\nycenter .\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nijis set as 1 in the case of the object residing within jthbounding box prediction in\\nithcell; otherwise, it is set as 0. The selected, i.e., predicted bounding box would be tasked\\nwith predicting an object with the greatest IoU, as expressed in (4):\\nγcoord∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij[\\n(xi−ˆxi)2+(yi−ˆyi)2]\\n(4)\\nThe next component of the loss function computes the prediction error in width and\\nheight of the bounding box, similar to the preceding component. However, the scale of\\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\\nof width and height between the range 0 and 1 indicates that their square roots increase\\nthe differences for smaller values to a higher degree compared to that of larger values,\\nexpressed as (5):\\nγcoord∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij[(√wi−√\\nˆwi)2\\n+(√\\nhi−√\\nˆhi)2]\\n(5)\\nNext, the loss of the conﬁdence score is computed based on whether the object is\\npresent or absent with respect to the bounding box. Penalization of the object conﬁdence\\nerror is only executed by the loss function if that predictor was responsible for the ground', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be1378ee-3633-45bd-8d32-eac08248969b', embedding=None, metadata={'page_label': '6', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 6 of 25\\ntruth bounding box.\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nijis set to 1 when the object is present in the cell; otherwise, it is set\\nas 0, whilst\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nnoobj\\nijworks in the opposite way, as shown in (6):\\n∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij(ci−ˆci)2+γnoobj∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nnoobj\\nij(xi−ˆxi)2+(ci−ˆci)2(6)\\nThe last component of the loss function, similar to the normal classiﬁcation loss,\\ncalculates the class (c) probability loss, except for the\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nijpart, expressed in (7):\\n∑S2\\ni=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij∑c∈classes\\n.(pi(c)−\\nMachines 2023 , 11, x FOR PEER REVIEW  6 of 26 \\n \\n the differences for smaller values to a higher degree compared to that of larger values, \\nexpressed as (5):  \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(√𝑤𝑖−√𝑤𝑖̂)2+(√ℎ𝑖−√ℎ𝑖̂)2\\n]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (5) \\nNext, the loss of the confidence score is computed based on whether the object is \\npresent or absent with respect to the bounding box. Penalization of the object confidence \\nerror is only executed by the loss function if that predictor was responsible for the ground \\ntruth bounding box. 𝕝𝑖𝑗𝑜𝑏𝑗 is set to 1 when the object is present in the cell ; otherwise , it is \\nset as 0, whilst 𝕝ijnoobj works in the opposite way, as shown in (6):  \\n \\n∑ ∑ 𝕝ijobj(ci−cî)2+γnoobj ∑ ∑ 𝕝ijnoobj(xi−xî)2+(ci−cî)2 B\\nj=0S2\\ni=0B\\nj=0S2\\ni=0   (6) \\n  \\nThe last component of the loss function, similar to the normal classification loss, cal-\\nculates the class ( c) probability loss, except for the 𝕝𝑖𝑗𝑜𝑏𝑗 part, expressed in (7):  \\n \\n∑ 𝕝𝑖𝑗𝑜𝑏𝑗 𝑆2\\n𝑖=0 ∑ (𝑝𝑖(𝑐)− 𝑐∈𝑐𝑙𝑎𝑠𝑠𝑒𝑠.𝑝𝑖(𝑐)̂ )2  (7) \\n  \\nPerformance wise, the simple YOLO (24 conv olutional  layers) when trained on the \\nPASCAL VOC dataset (2007 and 2012) [39,40] achieved a mean average precision (refer-\\nring to cross -class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved \\n52.7% mAP at an impressive 155 FPS. Although the performance was better than real -time \\ndetectors , such as DPM -v5 [41] (33% mAP), it was lower than the state -of-the-art (SOTA) \\nat the time , i.e., Faster R -CNN (71% mAP).  \\nThere were some clear loopholes that required attention , such as the architecture hav-\\ning comparatively low recall and higher localization error compared to Faster R -CNN. \\nAdditionally, the architecture struggled to detect close proximity objec ts due to the fact \\nthat each grid cell was capped to two bounding box proposals.  The loopholes attributed \\nto the original YOLO provided inspiration for the following variants of YOLO.  \\n2.2. YOLO-v2/9000  \\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was to \\nremove or at least mitigate the inefficiencies observed with the original YOLO whil e main-\\ntaining the impressive speed factor. Several enhancements were claimed through the im-\\nplementation of various techniques. Batch normalization [43 ] was introduced with the in-\\nternal architecture to improve model convergence, leading to faster training. This intro-\\nduction eliminated the need for other regularization techniques , such as dropout [44] \\naimed at reducing overfitting [45]. Its effectiveness can be gauged by the fact that simply \\nintroducing batch normalization improved the mAP by 2% compared to the original \\nYOLO.  \\nThe original YOLO worked with an input image size of 224  × 224 pixels during the \\ntraining stage, whilst for the detection phase , input images could be scaled up to 448  × 448 \\npixels, enforcing the architecture to adjust to the varying image resolution , which in turn \\ndecrease the mAP. To address this, the authors trained the architecture on 448  × 448 pixel \\nimages for 10 epochs on the  ImageNet [46] dataset, providing the architecture with the \\ncapacity to adjust the internal filters when dealing with higher resolution images, result-\\ning in an increased mAP of 4%. Whilst architectures , such as Fast and Faster R -CNN pre-\\ndict coordinates dir ectly from the convolutional network, the original YOLO utilized fully  (7)\\nPerformance wise, the simple YOLO (24 convolutional layers) when trained on the\\nPASCAL VOC dataset (2007 and 2012) [ 39,40] achieved a mean average precision (referring\\nto cross-class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved 52.7%\\nmAP at an impressive 155 FPS. Although the performance was better than real-time\\ndetectors, such as DPM-v5 [ 41] (33% mAP), it was lower than the state-of-the-art (SOTA) at\\nthe time, i.e., Faster R-CNN (71% mAP).\\nThere were some clear loopholes that required attention, such as the architecture\\nhaving comparatively low recall and higher localization error compared to Faster R-CNN.\\nAdditionally, the architecture struggled to detect close proximity objects due to the fact that\\neach grid cell was capped to two bounding box proposals. The loopholes attributed to the\\noriginal YOLO provided inspiration for the following variants of YOLO.\\n2.2. YOLO-v2/9000\\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was\\nto remove or at least mitigate the inefﬁciencies observed with the original YOLO while\\nmaintaining the impressive speed factor. Several enhancements were claimed through\\nthe implementation of various techniques. Batch normalization [ 43] was introduced with\\nthe internal architecture to improve model convergence, leading to faster training. This\\nintroduction eliminated the need for other regularization techniques, such as dropout [ 44]\\naimed at reducing overﬁtting [ 45]. Its effectiveness can be gauged by the fact that simply\\nintroducing batch normalization improved the mAP by 2% compared to the original YOLO.\\nThe original YOLO worked with an input image size of 224 ×224 pixels during\\nthe training stage, whilst for the detection phase, input images could be scaled up to\\n448×448 pixels, enforcing the architecture to adjust to the varying image resolution,\\nwhich in turn decrease the mAP . To address this, the authors trained the architecture on\\n448×448 pixel images for 10 epochs on the ImageNet [ 46] dataset, providing the architec-\\nture with the capacity to adjust the internal ﬁlters when dealing with higher resolution\\nimages, resulting in an increased mAP of 4%. Whilst architectures, such as Fast and Faster\\nR-CNN predict coordinates directly from the convolutional network, the original YOLO\\nutilized fully connected layers to serve this purpose. YOLO-v2 replaced the fully connected\\nlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\\nbox predictions. Anchor boxes [ 47] are essentially a list of predeﬁned dimensions (boxes)\\naimed at best matching the objects of interest. Rather than manual determination of best-ﬁt\\nanchor boxes, the authors utilized k-means clustering [ 48] on the training set bounding\\nboxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\\naverage IoU with respect to the closest centroid as shown in Figure 4. YOLO-v2 was trained\\non different architectures, namely, VGG-16 and GoogleNet, in addition to the authors\\nproposing the Darknet-19 [ 49] architecture due to characteristics, such as reduced process-\\ning requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on VGG-16\\nand GoogleNet, respectively. In terms of performance, YOLO-v2 provided 76.8 mAP at\\n67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures’ superiority\\nover SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000 utilized', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ac68a86-62c4-45d1-9c84-2367dc30e7c8', embedding=None, metadata={'page_label': '7', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 7 of 25\\nYOLO-v2 architecture, aimed at real-time detection of more than 9000 different objects;\\nhowever, at a signiﬁcantly reduced mAP of 19.7%.\\nMachines 2023 , 11, x FOR PEER REVIEW 7 of 26 \\n \\n connected layers to serve this purpose. YO LO-v2 replaced the fully connected layer re-\\nsponsible for predicting bounding boxes by adding anchor boxes for bounding box pre-\\ndictions. Anchor boxes [47] are essentially a list of prede ﬁned dimensions (boxes) aimed \\nat best matching the objects of interest. Rather than manual determination of best- ﬁt an-\\nchor boxes, the authors utilized k-means clus tering [48] on the training set bounding \\nboxes, inclusive of the ground truth boundi ng boxes, grouping similar shapes and plo tting \\naverage IoU with respect to the closest cent roid as shown in Figure 4. YOLO-v2 was \\ntrained on di ﬀerent architectures, namely, VGG-16 and GoogleNet, in addition to the au-\\nthors proposing the Darknet-19 [49] architectu re due to characteristics, such as reduced \\nprocessing requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on \\nVGG-16 and GoogleNet, respectively. In term s of performance, YOLO-v2 provided 76.8 \\nmAP at 67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures’ supe-\\nriority over SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000 \\nutilized YOLO-v2 architecture, aimed at real-time detection of more than 9000 di ﬀerent \\nobjects; however, at a signi ﬁcantly reduced mAP of 19.7%. \\n \\nFigure 4. Dimension cluste rs vs. mAP. \\n2.3. YOLO-v3 \\nArchitectures, such as VGG, focused their development work around the concept \\nthat deeper networks, i.e., more internal laye rs, equated to higher accuracy. YOLO-v2 also \\nhad higher number of convolutional layers compared to its predecessor. \\nHowever, as the image progressed throug h the network, the progressive down sam-\\npling resulted in the loss of ﬁne-grained features; therefore, YOLO-v2 often struggled with \\ndetecting smaller objects. At the time research was active in addressing this issue, as evi-\\ndent by the deployment of skip connection s [50] embedded within the proposed ResNet \\narchitecture, the focus was on addressing the vanishing gradient issue by facilitating in-\\nformation propagation via skip connection, as presented in Figure 5. \\nFigure 4. Dimension clusters vs. mAP .\\n2.3. YOLO-v3\\nArchitectures, such as VGG, focused their development work around the concept that\\ndeeper networks, i.e., more internal layers, equated to higher accuracy. YOLO-v2 also had\\nhigher number of convolutional layers compared to its predecessor.\\nHowever, as the image progressed through the network, the progressive down sam-\\npling resulted in the loss of ﬁne-grained features; therefore, YOLO-v2 often struggled\\nwith detecting smaller objects. At the time research was active in addressing this issue,\\nas evident by the deployment of skip connections [ 50] embedded within the proposed\\nResNet architecture, the focus was on addressing the vanishing gradient issue by facilitating\\ninformation propagation via skip connection, as presented in Figure 5.\\nMachines 2023 , 11, x FOR PEER REVIEW 8 of 26 \\n \\n  \\nFigure 5. Skip-connection con ﬁguration. \\nYOLO-v3 proposed a hybrid architecture fa ctoring in aspects of YOLO-v2, Darknet-\\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of \\nﬁne-grained features by a llowing for the gradient ﬂow from shallow layers to deeper lay-\\ners. \\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 \\nadditional layers was added for the detection head, totaling 106 convolutional layers for \\nthe YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the archi-\\ntecture made predictions at three di ﬀerent scales of granularity for outpu tting better per-\\nformance, increasing the probability of small object detection. \\n2.4. YOLO-v4 \\nYOLO-v4 was the ﬁrst variant of the YOLO family after the original author discon-\\ntinued further work that was introduced to the computer vi sion community in April 2020 \\nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite \\nof object detection techniques, tested and en hanced for providing a real-time, lightweight \\nobject detector. \\nThe backbone of an object detector has a critical role in the quality of features ex-\\ntracted. In-line with the experimental spirit, the authors experimented with three di ﬀerent \\nbackbones: CSPResNext-50, CSPDarknet-53, and E ﬃcientNet-B3 [53]. The ﬁrst was based \\non DenseNet [54] aimed at alleviating the vanishing gradient problem and bolstering fea-ture propagation and reuse, resulting in reduced number of network parameters. E ﬃ-\\ncientNet was proposed by Google Brain. The paper posits that an optima selection for \\nparameters when scaling CNNs can be asce rtained through a search mechanism. After \\nexperimenting with the above feature extractors, the authors based on their intuition and \\nbacked by their experimental result s selected CSPDarknet-53 as the o ﬃcial backbone for \\nYOLO-v4. \\nFor feature aggregation, the authors experi mented with several techniques for inte-\\ngration at the neck level including feature pyramid network (FPN) [55] and path aggrega-tion network (PANet) [56]. Ultimately, the authors opted for PANet as the feature aggre-\\ngator. The modi ﬁed PANet, as shown in Figure 6, utilized the concatenation mechanism. \\nPANet can be seen as an advanced version of FPN, namely, PANet proposed a bo ttom-up \\naugmentation path along with the top-down path (FPN), adding a ‘shortcut’ connection \\nfor linking ﬁne-grained features from high- and low-level layers. Additionally, the authors \\nintroduced a SPP [57] block post CSPDarknet-53 aimed at increasing the receptive ﬁeld \\nand separation of the important features arriving from the backbone. \\nFigure 5. Skip-connection conﬁguration.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f8ec7f8d-7d5c-4f43-b4ff-47285aba9c5f', embedding=None, metadata={'page_label': '8', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 8 of 25\\nYOLO-v3 proposed a hybrid architecture factoring in aspects of YOLO-v2, Darknet-\\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of\\nﬁne-grained features by allowing for the gradient ﬂow from shallow layers to deeper layers.\\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 addi-\\ntional layers was added for the detection head, totaling 106 convolutional layers for the\\nYOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the architecture\\nmade predictions at three different scales of granularity for outputting better performance,\\nincreasing the probability of small object detection.\\n2.4. YOLO-v4\\nYOLO-v4 was the ﬁrst variant of the YOLO family after the original author discon-\\ntinued further work that was introduced to the computer vision community in April 2020\\nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite\\nof object detection techniques, tested and enhanced for providing a real-time, lightweight\\nobject detector.\\nThe backbone of an object detector has a critical role in the quality of features extracted.\\nIn-line with the experimental spirit, the authors experimented with three different back-\\nbones: CSPResNext-50, CSPDarknet-53, and EfﬁcientNet-B3 [ 53]. The ﬁrst was based on\\nDenseNet [ 54] aimed at alleviating the vanishing gradient problem and bolstering feature\\npropagation and reuse, resulting in reduced number of network parameters. EfﬁcientNet\\nwas proposed by Google Brain. The paper posits that an optima selection for parameters\\nwhen scaling CNNs can be ascertained through a search mechanism. After experimenting\\nwith the above feature extractors, the authors based on their intuition and backed by their\\nexperimental results selected CSPDarknet-53 as the ofﬁcial backbone for YOLO-v4.\\nFor feature aggregation, the authors experimented with several techniques for integra-\\ntion at the neck level including feature pyramid network (FPN) [ 55] and path aggregation\\nnetwork (PANet) [ 56]. Ultimately, the authors opted for PANet as the feature aggregator.\\nThe modiﬁed PANet, as shown in Figure 6, utilized the concatenation mechanism. PANet\\ncan be seen as an advanced version of FPN, namely, PANet proposed a bottom-up augmen-\\ntation path along with the top-down path (FPN), adding a ‘shortcut’ connection for linking\\nﬁne-grained features from high- and low-level layers. Additionally, the authors introduced\\na SPP [ 57] block post CSPDarknet-53 aimed at increasing the receptive ﬁeld and separation\\nof the important features arriving from the backbone.\\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26 \\n \\n  \\nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi ﬁed PAN. \\nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \\na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the \\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage \\noverlap if in close proximity. \\nIn addition to the bag-of-freebies, the auth ors introduced ‘bag-of-specials’, with the \\nauthors claiming that although this set of op timization techniques presented in Figure 7 \\nwould marginally impact the inference time, they would signi ﬁcantly improve the overall \\nperformance. One of the components within th e ‘bag-of-specials’ was the Mish [59] acti-\\nvation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \\nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\\ndem. \\n \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\\ncials. \\n2.5. YOLO-v5 \\nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-\\nture extraction, neck focused on feature aggregation, and the head for consuming output \\nFigure 6. Path aggregation. ( a) Original PAN, ( b) modiﬁed PAN.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d9aecb24-f373-4698-bb67-66f1d6d88a37', embedding=None, metadata={'page_label': '9', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 9 of 25\\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consist-\\ning of augmentations, such as Mosaic aimed at improving performance without introducing\\nadditional baggage onto the inference time. CIoU loss [ 58] was also introduced as a freebie,\\nfocused on the overlap of the predicted and ground truth bounding box. In the case of no\\noverlap, the idea was to observe the closeness of the two boxes and encourage overlap if in\\nclose proximity.\\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26 \\n \\n  \\nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi ﬁed PAN. \\nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \\na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the \\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage \\noverlap if in close proximity. \\nIn addition to the bag-of-freebies, the auth ors introduced ‘bag-of-specials’, with the \\nauthors claiming that although this set of op timization techniques presented in Figure 7 \\nwould marginally impact the inference time, they would signi ﬁcantly improve the overall \\nperformance. One of the components within th e ‘bag-of-specials’ was the Mish [59] acti-\\nvation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \\nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\\ndem. \\n \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\\ncials. \\n2.5. YOLO-v5 \\nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-\\nture extraction, neck focused on feature aggregation, and the head for consuming output \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-specials.\\nIn addition to the bag-of-freebies, the authors introduced ‘bag-of-specials’, with the\\nauthors claiming that although this set of optimization techniques presented in Figure 7\\nwould marginally impact the inference time, they would signiﬁcantly improve the overall\\nperformance. One of the components within the ‘bag-of-specials’ was the Mish [ 59] acti-\\nvation function aimed at moving feature creations toward their respective optimal points.\\nCross mini-batch normalization [ 60] was also presented facilitating the running on any\\nGPU as many batch normalization techniques involve multiple GPUs operating in tandem.\\n2.5. YOLO-v5\\nThe YOLO network in essence consists of three key pillars, namely, backbone for\\nfeature extraction, neck focused on feature aggregation, and the head for consuming\\noutput features from the neck as input and generating detections. YOLO-v5 [ 61] similar to\\nYOLO-v4, with respect to contributions, focus on the conglomeration and reﬁnement of\\nvarious computer vision techniques for enhancing performance. In addition, in less than\\n2 months after the release of YOLO-v4, Glenn Jocher open-sourced an implementation of\\nYOLO-v5 [61].\\nA notable mention is that YOLO-v5 was the ﬁrst native release of architectures be-\\nlonging to the YOLO clan, to be written in PyTorch [ 62] rather than Darknet. Although\\nDarknet is considered as a ﬂexible low-level research framework, it was not purpose built\\nfor production environments with a signiﬁcantly smaller number of subscribers due to\\nconﬁgurability challenges. PyTorch, on the other hand, provided an established eco-system,\\nwith a wider subscription base among the computer vision community and provided the\\nsupporting infrastructure for facilitating mobile device deployment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c0d7857b-63a1-4bab-9581-14e868aaa822', embedding=None, metadata={'page_label': '10', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 10 of 25\\nIn addition, another notable proposal was the ‘automated anchor box learning’ concept.\\nIn YOLO-v2, the anchor box mechanism was introduced based on selecting anchor boxes\\nthat closely resemble the dimensions of the ground truth boxes in the training set via\\nk-means. The authors select the ﬁve close-ﬁt anchor boxes based on the COCO dataset [ 63]\\nand implement them as default boxes. However, the application of this methodology to a\\nunique dataset with signiﬁcant object differentials compared to those present in the COCO\\ndataset can quickly expose the inability of the predeﬁned boxes to adapt quickly to the\\nunique dataset. Therefore, authors in YOLO-v5 integrated the anchor box selection process\\ninto the YOLO-v5 pipeline. As a result, the network would automatically learn the best-ﬁt\\nanchor boxes for the particular dataset and utilize them during training to accelerate the\\nprocess. YOLO-v5 comes in several variants with respect to the computational parameters\\nas presented in Table 1.\\nTable 1. YOLO-v5 internal variant comparison.\\nModel Average Precision (@50) Parameters FLOPs\\nYOLO-v5s 55.8% 7.5 M 13.2B\\nYOLO-v5m 62.4% 21.8 M 39.4B\\nYOLO-v5l 65.4% 47.8 M 88.1B\\nYOLO-v5x 66.9% 86.7 M 205.7B\\nYOLO-v5 comprised of a weight ﬁle equating to 27 MB compared to YOLO-v5l at 192\\nMB. Figure 8 demonstrates the superiority of YOLO-v5 over EfﬁcientDet [64].\\nMachines 2023 , 11, x FOR PEER REVIEW 10 of 26 \\n \\n features from the neck as input and generating detections. YOLO-v5 [61] similar to YOLO-\\nv4, with respect to contributions, focus on the conglomeration and re ﬁnement of various \\ncomputer vision techniques for enhancing perfor mance. In addition, in less than 2 months \\nafter the release of YOLO-v4, Glenn Jocher open-sourced an implementation of YOLO-v5 \\n[61]. \\nA notable mention is that YOLO-v5 was the ﬁrst native release of architectures be-\\nlonging to the YOLO clan, to be wri tten in PyTorch [62] rather than Darknet. Although \\nDarknet is considered as a ﬂexible low-level research framework, it was not purpose built \\nfor production environments with a signi ﬁcantly smaller number of subscribers due to \\nconﬁgurability challenges. PyTorch, on the othe r hand, provided an established eco-sys-\\ntem, with a wider subscription base among the computer vision community and provided \\nthe supporting infrastructure for facilitating mobile device deployment. \\nIn addition, another notable proposal was the ‘automated anchor box learning’ con-\\ncept. In YOLO-v2, the anchor box mechanism was introduced based on selecting anchor \\nboxes that closely resemble the dimensions of  the ground truth boxes in the training set \\nvia k-means. The authors select the ﬁve close- ﬁt anchor boxes based on the COCO dataset \\n[63] and implement them as default boxes. However, the application of this methodology \\nto a unique dataset with signi ﬁcant object di ﬀerentials compared to those present in the \\nCOCO dataset can quickly expose the inability of the prede ﬁned boxes to adapt quickly \\nto the unique dataset. Therefore, authors in  YOLO-v5 integrated the anchor box selection \\nprocess into the YOLO-v5 pipeline. As a result, the network would automatically learn the best- ﬁt anchor boxes for the particular dataset and utilize them during training to ac-\\ncelerate the process. YOLO-v5 comes in severa l variants with respect to the computational \\nparameters as presented in Table 1. \\nTable 1. YOLO-v5 internal variant comparison. \\nModel Average Precision (@50) Parameters FLOPs \\nYOLO-v5s 55.8% 7.5 M 13.2B \\nYOLO-v5m 62.4% 21.8 M 39.4B \\nYOLO-v5l 65.4% 47.8 M 88.1B \\nYOLO-v5x 66.9% 86.7 M 205.7B \\nYOLO-v5 comprised of a weight ﬁle equating to 27 MB compared to YOLO-v5l at 192 \\nMB. Figure 8 demonstrates the superiority of YOLO-v5 over E ﬃcientDet [64]. \\n \\nFigure 8. YOLO-v5 variant comparison vs. E ﬃcientDet [61]. \\nFigure 8. YOLO-v5 variant comparison vs. EfﬁcientDet [61].\\n2.6. YOLO-v6\\nThe initial codebase for YOLO-v6 [ 65] was released in June 2022 by the Meituan\\nTechnical Team based in China. The authors focused their design strategy on producing an\\nindustry-orientated object detector.\\nTo meet industrial application requirements, the architecture would need to be highly\\nperformant on a range of hardware options, maintaining high speed and accuracy. To\\nconform with the diverse set of industrial applications, YOLO-v6 comes in several variants\\nstarting with YOLO-v6-nano as the fastest with the least number of parameters and reaching\\nYOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69cffe9f-7665-4cc2-96e4-1a549f8dde03', embedding=None, metadata={'page_label': '11', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 11 of 25\\nTable 2. YOLO-v6 variant comparison.\\nVariantmAP 0.5:0.95\\n(COCO-val)FPS Tesla T4 Parameters (Million)\\nYOLO-v6-N 35.9 (300 epochs) 802 4.3\\nYOLO-v6-T 40.3 (300 epochs) 449 15.0\\nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2\\nYOLO-v6-S 43.5 (300 epochs) 495 17.2\\nYOLO-v6-M 49.7 233 34.3\\nYOLO-v6-L-ReLU 51.7 149 58.5\\nThe impressive performance presented in Table 2 is a result of several innovations\\nintegrated into the YOLO-v6 architecture. The key contributions can be summed into four\\npoints. First, in contrast to its predecessors, YOLO-v6 opts for an anchor-free approach,\\nmaking it 51% faster when compared to anchor-based approaches.\\nSecond, the authors introduced a revised reparametrized backbone and neck, proposed\\nas EfﬁcientRep backbone and Rep-PAN neck [ 66], namely, up to and including YOLO-v5,\\nthe regression and classiﬁcation heads shared the same features. Breaking the convention,\\nYOLO-v6 implements the decoupled head as shown in Figure 9. As a result, the architecture\\nhas additional layers separating features from the ﬁnal head, as empirically shown to\\nimprove the performance. Third, YOLO-v6 mandates a two-loss function. Varifocal loss\\n(VFL) [ 67] is used as the classiﬁcation loss and distribution focal loss (DFL) [ 68], along with\\nSIoU/GIoU [ 69] as regression loss. VFL being a derivative of focal loss, treats positive\\nand negative samples at varying degrees of importance, helping in balancing the learning\\nsignals from both sample types. DFL is deployed for box regression in YOLO-v6 medium\\nand large variants, treating the continuous distribution of the box locations as discretized\\nprobability distribution, which is shown to be particularly efﬁcient when the ground truth\\nbox boundaries are blurred.\\nMachines 2023 , 11, x FOR PEER REVIEW 11 of 26 \\n \\n 2.6. YOLO-v6 \\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan Tech-\\nnical Team based in China. The authors focu sed their design strategy on producing an \\nindustry-orientated object detector. \\nTo meet industrial application requirements, the architecture would need to be \\nhighly performant on a range of hardware op tions, maintaining high speed and accuracy. \\nTo conform with the diverse set of industrial applications, YOLO-v6 comes in several var-\\niants starting with YOLO-v6-nano as the fastest with the least number of parameters and reaching YOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2. \\nTable 2. YOLO-v6 variant comparison. \\nVariant mAP 0.5:0.95 \\n(COCO-val) FPS Tesla T4 Parameters (Million) \\nYOLO-v6-N 35.9 (300 epochs) 802 4.3 \\nYOLO-v6-T 40.3 (300 epochs) 449 15.0 \\nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2 \\nYOLO-v6-S 43.5 (300 epochs) 495 17.2 \\nYOLO-v6-M 49.7 233 34.3 \\nYOLO-v6-L-ReLU 51.7 149 58.5 \\nThe impressive performance presented in Ta ble 2 is a result of several innovations \\nintegrated into the YOLO-v6 ar chitecture. The key contributi ons can be summed into four \\npoints. First, in contrast to its predecesso rs, YOLO-v6 opts for an anchor-free approach, \\nmaking it 51% faster when compared to anchor-based approaches. \\nSecond, the authors introduced a revised reparametrized backbone and neck, pro-\\nposed as E ﬃcientRep backbone and Rep-PAN neck [66], namely, up to and including \\nYOLO-v5, the regression and classi ﬁcation heads shared the same features. Breaking the \\nconvention, YOLO-v6 implements the decoupled head as shown in Figure 9. As a result, \\nthe architecture has additional layers separating features from the ﬁnal head, as empiri-\\ncally shown to improve the performance. Thir d, YOLO-v6 mandates a two-loss function. \\nVarifocal loss (VFL) [67] is used as the classi ﬁcation loss and distribution focal loss (DFL) \\n[68], along with SIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss, \\ntreats positive and negative samples at varying degrees of importance, helping in balanc-\\ning the learning signals from both sample types. DFL is deployed for box regression in YOLO-v6 medium and large variants, treating the continuous distribution of the box lo-\\ncations as discretized probability distribution, which is shown to be particularly e ﬃcient \\nwhen the ground truth box boundaries are blurred. \\n \\nFigure 9. YOLO-v6 model base architecture.\\nAdditional improvements focused on industrial applications include the use of knowl-\\nedge distillation [70], involving a teacher model used for training a student model, where\\nthe predictions of the teacher are used as soft labels along with the ground truth for training\\nthe student. This comes without fueling the computational cost as essentially the aim is\\nto train a smaller (student) model to replicate the high performance of the larger (teacher)\\nmodel. Comparing the performance of YOLO-v6 with its predecessors, including YOLO-v5\\non the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 achieves a higher\\nmAP at various FPS.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f177723c-df80-4e7a-8942-dd4d186b9fc4', embedding=None, metadata={'page_label': '12', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 12 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 12 of 26 \\n \\n Figure 9. YOLO-v6 model base architecture. \\nAdditional improvements focused on indu strial applications include the use of \\nknowledge distillation [70], involving a teacher model used for training a student model, \\nwhere the predictions of the teacher are used as soft labels along with the ground truth \\nfor training the student. This comes without fueling the computational cost as essentially the aim is to train a smaller (student) model to replicate the high performance of the larger \\n(teacher) model. Comparing the performance of YOLO-v6 with its predecessors, includ-\\ning YOLO-v5 on the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 \\nachieves a higher mAP at various FPS. \\n \\nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71]. \\n2.7. YOLO-v7 \\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [72]. \\nAlthough other variants have been releas ed in between, including YOLO-X [73] and \\nYOLO-R [74], these focused more on GPU speed  enhancements with respect to inferenc-\\ning. YOLO-v7 proposes several architectural reforms for improving the accuracy and \\nmaintaining high detection speeds. The proposed reforms can be split into two categories: \\nArchitectural reforms and Trainable BoF (bag-o f-freebies). Architectural reforms included \\nthe implementation of the E-ELAN (extended e ﬃcient layer aggregation network) [75] in \\nthe YOLO-v7 backbone, taking inspiration from research advancements in network e ﬃ-\\nciency. The design of the E-ELAN was guided by the analysis of factors that impact accu-\\nracy and speed, such as memory access cost , input/output channel ratio, and gradient \\npath. \\nThe second architectural reform was presented as compound model scaling, as \\nshown in Figure 11. The aim was to cater for a wider scope of application requirements, \\ni.e., certain applications require accuracy to be prioritized, whilst others may prioritize speed. Although NAS (network architecture se arch) [76] can be used for parameter-spe-\\nciﬁc scaling to ﬁnd the best factors, the scaling factors are independent [77]. Whereas the \\ncompound-scaling mechanism a llows for the width and depth to be scaled in coherence \\nfor concatenation-based networks, maintaining optimal network architecture while scal-\\ning for di ﬀerent sizes. \\nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71].\\n2.7. YOLO-v7\\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [ 72].\\nAlthough other variants have been released in between, including YOLO-X [ 73] and YOLO-\\nR [74], these focused more on GPU speed enhancements with respect to inferencing. YOLO-\\nv7 proposes several architectural reforms for improving the accuracy and maintaining high\\ndetection speeds. The proposed reforms can be split into two categories: Architectural\\nreforms and Trainable BoF (bag-of-freebies). Architectural reforms included the implemen-\\ntation of the E-ELAN (extended efﬁcient layer aggregation network) [ 75] in the YOLO-v7\\nbackbone, taking inspiration from research advancements in network efﬁciency. The design\\nof the E-ELAN was guided by the analysis of factors that impact accuracy and speed, such\\nas memory access cost, input/output channel ratio, and gradient path.\\nThe second architectural reform was presented as compound model scaling, as shown\\nin Figure 11. The aim was to cater for a wider scope of application requirements, i.e., certain\\napplications require accuracy to be prioritized, whilst others may prioritize speed. Although\\nNAS (network architecture search) [ 76] can be used for parameter-speciﬁc scaling to ﬁnd\\nthe best factors, the scaling factors are independent [ 77]. Whereas the compound-scaling\\nmechanism allows for the width and depth to be scaled in coherence for concatenation-\\nbased networks, maintaining optimal network architecture while scaling for different sizes.\\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26 \\n \\n  \\nFigure 11. YOLO-v7 compound scaling. \\nRe-parameterization planning is based on averaging a set of model weights to obtain \\na more robust network [78,79]. Expanding further, module level re-parameterization ena-\\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7 \\nutilizes gradient ﬂow propagation paths with the aim to observe which internal network \\nmodules should deploy re-parameterization strategies. \\nThe auxiliary head coarse-to- ﬁne concept is proposed on  the premise that the net-\\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \\nlayers to assist in the training proces s. However, this would not train as e ﬃciently as the \\nlead head, due to the former not having access to the complete network. \\nFigure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \\nsurpassed the compared object detectors in a ccuracy and speed in the range of 5–160 FPS. \\nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of \\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-\\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \\n \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \\nFigure 11. YOLO-v7 compound scaling.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e96b8183-eb49-4f92-88cb-154ce603a0a7', embedding=None, metadata={'page_label': '13', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 13 of 25\\nRe-parameterization planning is based on averaging a set of model weights to obtain a\\nmore robust network [ 78,79]. Expanding further, module level re-parameterization enables\\nsegments of the network to regulate their own parameterization strategies. YOLO-v7\\nutilizes gradient ﬂow propagation paths with the aim to observe which internal network\\nmodules should deploy re-parameterization strategies.\\nThe auxiliary head coarse-to-ﬁne concept is proposed on the premise that the network\\nhead is quite far downstream; therefore, the auxiliary head is deployed at the middle layers\\nto assist in the training process. However, this would not train as efﬁciently as the lead\\nhead, due to the former not having access to the complete network.\\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\\nsurpassed the compared object detectors in accuracy and speed in the range of 5–160 FPS .\\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-\\nv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,\\nrespectively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26 \\n \\n  \\nFigure 11. YOLO-v7 compound scaling. \\nRe-parameterization planning is based on averaging a set of model weights to obtain \\na more robust network [78,79]. Expanding further, module level re-parameterization ena-\\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7 \\nutilizes gradient ﬂow propagation paths with the aim to observe which internal network \\nmodules should deploy re-parameterization strategies. \\nThe auxiliary head coarse-to- ﬁne concept is proposed on  the premise that the net-\\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \\nlayers to assist in the training proces s. However, this would not train as e ﬃciently as the \\nlead head, due to the former not having access to the complete network. \\nFigure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \\nsurpassed the compared object detectors in a ccuracy and speed in the range of 5–160 FPS. \\nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of \\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-\\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \\n \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72].\\nInternal variant comparison of YOLO-v7 is presented in Table 3. As evident, there is a\\nsigniﬁcant performance gap with respect to mAP when comparing YOLO-v7-tiny with the\\ncomputationally demanding YOLO-v7-D6. However, the latter would not be suitable for\\nedge deployment onto a computationally constrained device.\\nTable 3. Variant comparison of YOLO-v7.\\nModel Size (Pixels) mAP (@50) Parameters FLOPs\\nYOLO-v7-tiny 640 52.8% 6.2 M 5.8G\\nYOLO-v7 640 69.7% 36.9 M 104.7G\\nYOLO-v7-X 640 71.1% 71.3 M 189.9G\\nYOLO-v7-E6 1280 73.5% 97.2 M 515.2G\\nYOLO-v7-D6 1280 73.8% 154.7 M 806.8G', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4605a099-42d1-4459-b8b5-1abd3ae30f5f', embedding=None, metadata={'page_label': '14', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 14 of 25\\n2.8. YOLO-v8\\nThe latest addition to the family of YOLO was conﬁrmed in January 2023 with the\\nrelease of YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper release\\nis impending and many features are yet to be added to the YOLO-v8 repository, initial\\ncomparisons of the newcomer against its predecessors demonstrate its superiority as the\\nnew YOLO state-of-the-art.\\nFigure 13 demonstrates that when comparing YOLO-v8 against YOLO-v5 and YOLO-\\nv6 trained on 640 image resolution, all YOLO-v8 variants output better throughput with a\\nsimilar number of parameters, indicating toward hardware-efﬁcient, architectural reforms.\\nThe fact that YOLO-v8 and YOLO-v5 are presented by Ultralytics with YOLO-v5 providing\\nimpressive real-time performance and based on the initial benchmarking results released\\nby Ultralytics, it is strongly assumed that the YOLO-v8 will be focusing on constrained\\nedge device deployment at high-inference speed.\\nMachines 2023 , 11, x FOR PEER REVIEW 15 of 26 \\n \\n  \\nFigure 13. YOLO-v8 comparison with predecessors [80]. \\n3. Industrial Defect Detection Via YOLO \\nThe previous section demonstrates the rapid evolution of the YOLO ‘clan’ of object \\ndetectors amongst the computer vision community. This section of the review focuses on \\nthe implementation of YOLO variants for the detection of surface defects within the in-dustrial se tting. The selection of ‘industrial se tting’ is due to its varying and stringent re-\\nquirements alternating between accuracy and speed, a theme which is found through \\nDNA of the YOLO variants. \\n3.1. Industrial Fabric Defect Detection \\nRui Jin et al. [81] in their premise state the ine ﬃciencies of manual inspection in the \\ntextile manufacturing domain as high cost of  labor, human-related fatigue, and reduced \\ndetection speed (less than 20 m/min). The authors aim to address these ine ﬃciencies by \\nproposing a YOLO-v5-based architecture, coupled with a spatial a ttention mechanism for \\naccentuation of smaller defective regions. Th e proposed approach involved a teacher net-\\nwork trained on the fabric dataset. Post tr aining of the teacher network, the learned \\nweights were distilled to the student network, which was compatible for deployment onto \\na Jetson TX2 [82] via TensorRT [83]. The results presented by the authors show, as ex-\\npected, that the teacher network reported hi gher performance with an AUC of 98.1% com-\\npared to 95.2% (student network). However, as the student network was computationally \\nsmaller, the inference time was signi ﬁcantly less at 16 ms for the student network in con-\\ntrast to the teacher network at 35 ms on th e Jetson TX2. Based on the performance, the \\nFigure 13. YOLO-v8 comparison with predecessors [80].\\n3. Industrial Defect Detection via YOLO\\nThe previous section demonstrates the rapid evolution of the YOLO ‘clan’ of object\\ndetectors amongst the computer vision community. This section of the review focuses\\non the implementation of YOLO variants for the detection of surface defects within the\\nindustrial setting. The selection of ‘industrial setting’ is due to its varying and stringent\\nrequirements alternating between accuracy and speed, a theme which is found through\\nDNA of the YOLO variants.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66192ea1-96e7-494f-a6b6-8d248972f7bf', embedding=None, metadata={'page_label': '15', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 15 of 25\\n3.1. Industrial Fabric Defect Detection\\nRui Jin et al. [ 81] in their premise state the inefﬁciencies of manual inspection in the\\ntextile manufacturing domain as high cost of labor, human-related fatigue, and reduced\\ndetection speed (less than 20 m/min). The authors aim to address these inefﬁciencies by\\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism\\nfor accentuation of smaller defective regions. The proposed approach involved a teacher\\nnetwork trained on the fabric dataset. Post training of the teacher network, the learned\\nweights were distilled to the student network, which was compatible for deployment onto\\na Jetson TX2 [ 82] via TensorRT [ 83]. The results presented by the authors show, as expected,\\nthat the teacher network reported higher performance with an AUC of 98.1% compared to\\n95.2% (student network). However, as the student network was computationally smaller,\\nthe inference time was signiﬁcantly less at 16 ms for the student network in contrast to the\\nteacher network at 35 ms on the Jetson TX2. Based on the performance, the authors claim\\nthat the proposed solution provides high accuracy and real-time inference speed, making it\\ncompatible for deployment via the edge device.\\nSifundvoleshile Dlamini et al. [ 84] propose a production environment fabric defect\\ndetection framework focused on real-time detection and accurate classiﬁcation on-site,\\nas shown in Figure 14. The authors embed conventional image processing at the onset\\nof their data enhancement strategy, i.e., ﬁltering to denoise feature enhancement. Post\\naugmentations and data scaling, the authors train the YOLO-v4 architecture based on\\npretrained weights. The reported performance was respectable with an F1-score of 93.6%,\\nat an impressive detection speed of 34 fps and prediction speed of 21.4 ms. The authors\\nclaim that the performance is evident to the effectiveness of the selected architecture for the\\ngiven domain.\\nMachines 2023 , 11, x FOR PEER REVIEW 16 of 26 \\n \\n authors claim that the proposed solution prov ides high accuracy and real-time inference \\nspeed, making it compatible for deployment via the edge device. \\nSifundvoleshile Dlamini et al. [84] propos e a production environment fabric defect \\ndetection framework focused on real-time detection and accurate classi ﬁcation on-site, as \\nshown in Figure 14. The authors embed conventional image processing at the onset of \\ntheir data enhancement strategy, i.e., ﬁltering to denoise feature enhancement. Post aug-\\nmentations and data scaling, the authors tr ain the YOLO-v4 architecture based on pre-\\ntrained weights. The reported performance was respectable with an F1-score of 93.6%, at \\nan impressive detection speed of 34 fps and pr ediction speed of 21.4 ms. The authors claim \\nthat the performance is evident to the e ﬀectiveness of the selected architecture for the \\ngiven domain. \\n \\nFigure 14. Inspection machine integration [84]. \\nRestricted by the available computing reso urces for edge deploy ment, Guijuan Lin et \\nal. [85] state problems with quality inspection  in the fabric production domain, including \\nminute scale of defects, extreme unbalance wi th the aspect ratio of certain defects, and \\nslow defect detection speeds. To address thes e issues, the authors proposed a sliding-win-\\ndow, self-a ttention (multihead) mechanism calibrated  for small defect targets. Addition-\\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the \\noriginal YOLO-v5 architecture for the extraction  of hierarchical features. Furthermore, the \\ngeneralized focal loss is implemented with the architecture aimed at improving the learn-ing process for positive target instances, whilst lowering the rate of missed detections. The \\nauthors report the accuracy of the proposed so lution on a real-world fabric dataset, reach-\\ning 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection require-ments for detection via embedded devices. \\nFigure 14. Inspection machine integration [84].\\nRestricted by the available computing resources for edge deployment, Guijuan Lin et al. [ 85]\\nstate problems with quality inspection in the fabric production domain, including minute\\nscale of defects, extreme unbalance with the aspect ratio of certain defects, and slow defect\\ndetection speeds. To address these issues, the authors proposed a sliding-window, self-\\nattention (multihead) mechanism calibrated for small defect targets. Additionally, the Swin\\nTransformer [ 86] module as depicted in Figure 15 was integrated into the original YOLO-v5\\narchitecture for the extraction of hierarchical features. Furthermore, the generalized focal\\nloss is implemented with the architecture aimed at improving the learning process for\\npositive target instances, whilst lowering the rate of missed detections. The authors report\\nthe accuracy of the proposed solution on a real-world fabric dataset, reaching 76.5% mAP', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2fce131c-8a14-41ae-8584-7d74ef47b4c5', embedding=None, metadata={'page_label': '16', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 16 of 25\\nat 58.8 FPS, making it compatible with the real-time detection requirements for detection\\nvia embedded devices.\\nMachines 2023 , 11, x FOR PEER REVIEW 17 of 26 \\n \\n  \\nFigure 15. Backbone for Swin Transformer network [85]. \\n3.2. Solar Cell Surface Defect Detection \\nSetting their premise, the authors [87] state that human-led Photovoltaic (PV) inspec-\\ntion has many drawbacks including the re quirement of operation and maintenance \\n(O&M) engineers, cell-by-cell inspec tion, high workload, and reduced e ﬃciency. The au-\\nthors propose an improved architecture based on YOLO-v5 for the characterization of complex solar cell surface textures and defect ive regions. The proposal is based on the \\nintegration of deformable convolution within  the CSP module with the aim of achieving \\nan adaptive learning scale. Additionally, an a ttention mechanism is incorporated for en-\\nhanced feature extraction. Moreover, the authors optimize the original YOLO-v5 architec-\\nture further via K-means++ clustering for an chor box determination algorithm. Based on \\nthe presented results, the improved architectu re achieved a respectable mAP of 89.64% on \\nan EL-based solar cell image dataset, 7.85% higher compared to mAP for the original ar-\\nchitecture, with detection speed reaching 36.24 FPS, which can be translated as a more accurate detection while remaining compatible with the real-time requirements. \\nAmran Binomairah et al. [88]  highlight two frequent defects encountered during the \\nmanufacturing process of crystalline solar cell s as dark spot/region and microcracks. The \\nlatter can have a detrimental impact on the performance of the module, which is a major \\ncause for PV module failures. The authors su bscribe to the YOLO architecture, comparing \\nthe performance of their methodology on YOLO-v4 and an improved YOLO-v4-tiny inte-\\ngrated with a spatial pyramid pooling mechanism. Based on the presented results, YOLO-\\nv4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with 91% \\nmAP at 28.2 ms. The authors claim that although the la tter is less accurate, it is notably \\nfaster than the former. \\nTianyi Sun et al. [89] focus on automated hot-spot detection within PV cells based a \\nmodi ﬁed version of the YOLO-v5 architecture. The ﬁrst improvement comes in the form \\nof enhanced anchors and detection heads for the respective architecture. To improve the \\ndetection precision at varying scales, k-means clustering [48] is deployed for clustering \\nthe length–width ratio with respect to the data  annotation frame. Additionally, a set of the \\nanchors consisting of smaller values were adde d to cater for the detection of small defects \\nby optimizing the cluster numb er. The reported performance of the improved architecture \\nwas reported as 87.8% mAP, with the average recall rate of 89.0% and F1-score reaching \\n88.9%. The reported FPS was impressive reaching  98.6 FPS, with the authors claiming that \\nthe proposed solution would provide intellige nt monitoring at PV power stations. Infer-\\nencing output presented in Figure 16 sh ows the proposed AP-YOLO-v5 architecture, \\nproviding inferences at a higher con ﬁdence level compared to the original YOLO-v5. \\nFigure 15. Backbone for Swin Transformer network [85].\\n3.2. Solar Cell Surface Defect Detection\\nSetting their premise, the authors [ 87] state that human-led Photovoltaic (PV) inspec-\\ntion has many drawbacks including the requirement of operation and maintenance (O&M)\\nengineers, cell-by-cell inspection, high workload, and reduced efﬁciency. The authors\\npropose an improved architecture based on YOLO-v5 for the characterization of complex\\nsolar cell surface textures and defective regions. The proposal is based on the integration\\nof deformable convolution within the CSP module with the aim of achieving an adaptive\\nlearning scale. Additionally, an attention mechanism is incorporated for enhanced feature\\nextraction. Moreover, the authors optimize the original YOLO-v5 architecture further via\\nK-means++ clustering for anchor box determination algorithm. Based on the presented\\nresults, the improved architecture achieved a respectable mAP of 89.64% on an EL-based\\nsolar cell image dataset, 7.85% higher compared to mAP for the original architecture, with\\ndetection speed reaching 36.24 FPS, which can be translated as a more accurate detection\\nwhile remaining compatible with the real-time requirements.\\nAmran Binomairah et al. [ 88] highlight two frequent defects encountered during\\nthe manufacturing process of crystalline solar cells as dark spot/region and microcracks.\\nThe latter can have a detrimental impact on the performance of the module, which is\\na major cause for PV module failures. The authors subscribe to the YOLO architecture,\\ncomparing the performance of their methodology on YOLO-v4 and an improved YOLO-v4-\\ntiny integrated with a spatial pyramid pooling mechanism. Based on the presented results,\\nYOLO-v4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with\\n91% mAP at 28.2 ms. The authors claim that although the latter is less accurate, it is notably\\nfaster than the former.\\nTianyi Sun et al. [ 89] focus on automated hot-spot detection within PV cells based a\\nmodiﬁed version of the YOLO-v5 architecture. The ﬁrst improvement comes in the form\\nof enhanced anchors and detection heads for the respective architecture. To improve the\\ndetection precision at varying scales, k-means clustering [ 48] is deployed for clustering the\\nlength–width ratio with respect to the data annotation frame. Additionally, a set of the\\nanchors consisting of smaller values were added to cater for the detection of small defects\\nby optimizing the cluster number. The reported performance of the improved architecture\\nwas reported as 87.8% mAP , with the average recall rate of 89.0% and F1-score reaching', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5aaaaa60-b6b3-400c-8581-594f74ed9e10', embedding=None, metadata={'page_label': '17', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 17 of 25\\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming\\nthat the proposed solution would provide intelligent monitoring at PV power stations.\\nInferencing output presented in Figure 16 shows the proposed AP-YOLO-v5 architecture,\\nproviding inferences at a higher conﬁdence level compared to the original YOLO-v5.\\nMachines 2023 , 11, x FOR PEER REVIEW 18 of 26 \\n \\n  \\nFigure 16. inference/con ﬁdence comparison [89]. \\n3.3. Steel Surface Defect Detection \\nDinming Yang et al. [90] set the premise of  their research by stating the importance \\nof steel pipe quality inspection, citing the growing demand in countries, such as China. \\nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive \\ntesting (NDT), the authors state that it still requires human assistance for the determina-\\ntion, classi ﬁcation, and localization of the defect s. The authors propose the implementa-\\ntion of YOLO-v5 for production-based weld st eel defect detection based on X-ray images \\nof the weld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% \\n(IoU-0.5), whilst meeting the real-time detect ion requirements of steel pipe production \\nwith a single image detection rate of 0.12 s. \\nZhuxi MA et al. [91] address the issu e of large-scale computation and speci ﬁc hard-\\nware requirements for automated defect detection in aluminum strips. The authors select \\nYOLO-v4 as the architecture, whilst the back bone is constructed to make use of depth-\\nwise separable convolutions along with a parallel dual a ttention mechanism for feature \\nenhancement, as shown in Figure 17. The propos ed network is tested on real data from a \\ncold-rolling workshop, providing impressive results on real data achieving an mAP of \\n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-\\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of \\nthree. The increase in performance was pa rtly due to the custom anchor approach, \\nwhereby due to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 \\nwhich is in-line with the defect characteristics, such as scratch marks. \\nFigure 16. Inference/conﬁdence comparison [89].\\n3.3. Steel Surface Defect Detection\\nDinming Yang et al. [ 90] set the premise of their research by stating the importance\\nof steel pipe quality inspection, citing the growing demand in countries, such as China.\\nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive\\ntesting (NDT), the authors state that it still requires human assistance for the determination,\\nclassiﬁcation, and localization of the defects. The authors propose the implementation of\\nYOLO-v5 for production-based weld steel defect detection based on X-ray images of the\\nweld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% (IoU-0.5),\\nwhilst meeting the real-time detection requirements of steel pipe production with a single\\nimage detection rate of 0.12 s.\\nZhuxi MA et al. [ 91] address the issue of large-scale computation and speciﬁc hard-\\nware requirements for automated defect detection in aluminum strips. The authors select\\nYOLO-v4 as the architecture, whilst the backbone is constructed to make use of depth-\\nwise separable convolutions along with a parallel dual attention mechanism for feature\\nenhancement, as shown in Figure 17. The proposed network is tested on real data from\\na cold-rolling workshop, providing impressive results on real data achieving an mAP of\\n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-\\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of\\nthree. The increase in performance was partly due to the custom anchor approach, whereby\\ndue to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 which is\\nin-line with the defect characteristics, such as scratch marks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='073d12bd-fd4a-44a1-bb09-b836f2fa109b', embedding=None, metadata={'page_label': '18', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 18 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 19 of 26 \\n \\n  \\nFigure 17. Proposed parallel network structure [91]. \\nJianting Shi et al. [92] cite the manufacturin g process of steel production as the reason \\nfor various defects originating on the steel surface, such as rolling scale and patches. The \\nauthors state that the small dimensions of the defects as well as the stringent detection \\nrequirements make the quality inspection process a challenging task. Therefore, the au-\\nthors present an improved version of YOLO-v5 by incorporating an a ttention mechanism \\nfor facilitating the transmission of shallow features from the backbone to the neck, pre-\\nserving the defective regions, in addition to k-means clustering of anchor boxes for ad-\\ndressing the extreme aspect ratios of defectiv e targets within the dataset. The authors state \\nthat the improved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the original architecture achieved 81.78% mAP at 52 FPS. \\n3.4. Pallet Racking Defect Inspection \\nA promising application with signi ﬁcant deployment scope in the warehousing and \\ngeneral industrial storage centers is automated pallet racking inspection. Warehouses and \\ndistribution centers host a crit ical infrastructure known as racking for stock storage. Un-\\nnoticed damage to pallet racking can pave the way for signi ﬁcant losses initiated by rack-\\ning collapse leading to wasted/damaged stock, ﬁnancial implications, operational losses, \\ninjured employees, and worst-case, loss of lives [93]. Due to the ine ﬃciencies of the con-\\nventional racking inspection mechanisms, such  as human-led annual inspection resulting \\nin labor costs, bias, fatigue, and mechanical pr oducts, such as rackguar ds [94] lacking clas-\\nsiﬁcation intelligence, CNN-based automated de tection seems to be a promising alterna-\\ntive. \\nRealizing the potential, Hussain et al. [95] inaugurated research into automated pallet \\nracking detection via computer vision. After presenting their initial research based on the \\nMobileNet-V2 architecture, the authors recent ly proposed the implementation of YOLO-\\nv7 for automated pallet racking inspection [96]. The selection of the architecture was in-\\nline with the stringent requirements of production ﬂoor deployment, i.e., edge device de-\\nployment, placed onto an oper ating forklift, requiring real-time detection as the forklift \\napproaches the racking. Evaluating the perfor mance of the proposed solution on a real \\ndataset, the authors claimed an impressive pe rformance of 91.1% mAP running at 19 FPS. \\nFigure 17. Proposed parallel network structure [91].\\nJianting Shi et al. [ 92] cite the manufacturing process of steel production as the reason\\nfor various defects originating on the steel surface, such as rolling scale and patches. The\\nauthors state that the small dimensions of the defects as well as the stringent detection\\nrequirements make the quality inspection process a challenging task. Therefore, the authors\\npresent an improved version of YOLO-v5 by incorporating an attention mechanism for\\nfacilitating the transmission of shallow features from the backbone to the neck, preserving\\nthe defective regions, in addition to k-means clustering of anchor boxes for addressing\\nthe extreme aspect ratios of defective targets within the dataset. The authors state that the\\nimproved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the\\noriginal architecture achieved 81.78% mAP at 52 FPS.\\n3.4. Pallet Racking Defect Inspection\\nA promising application with significant deployment scope in the warehousing\\nand general industrial storage centers is automated pallet racking inspection. Ware-\\nhouses and distribution centers host a critical infrastructure known as racking for stock\\nstorage. Unnoticed damage to pallet racking can pave the way for significant losses\\ninitiated by racking collapse leading to wasted/damaged stock, financial implications,\\noperational losses, injured employees, and worst-case, loss of lives [ 93]. Due to the\\ninefficiencies of the conventional racking inspection mechanisms, such as human-led\\nannual inspection resulting in labor costs, bias, fatigue, and mechanical products, such\\nas rackguards [ 94] lacking classification intelligence, CNN-based automated detection\\nseems to be a promising alternative.\\nRealizing the potential, Hussain et al. [ 95] inaugurated research into automated pallet\\nracking detection via computer vision. After presenting their initial research based on the\\nMobileNet-V2 architecture, the authors recently proposed the implementation of YOLO-\\nv7 for automated pallet racking inspection [ 96]. The selection of the architecture was\\nin-line with the stringent requirements of production ﬂoor deployment, i.e., edge device\\ndeployment, placed onto an operating forklift, requiring real-time detection as the forklift', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11222c7e-4de4-4f75-972d-dc10cecd29fe', embedding=None, metadata={'page_label': '19', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 19 of 25\\napproaches the racking. Evaluating the performance of the proposed solution on a real\\ndataset, the authors claimed an impressive performance of 91.1% mAP running at 19 FPS.\\nTable 4 presents a comparison of the present research in this emerging ﬁeld. Although\\nmask R-CNN presents the highest accuracy, which is a derivative of the segmentation\\nfamily of architectures with signiﬁcant computational load, this makes it an infeasible\\noption for deployment. Whereas the proposed approach utilizing YOLO-v7 achieved\\nsimilar accuracy compared to MobileNet-V2, whilst requiring signiﬁcantly less training\\ndata along with inferencing at 19 FPS.\\nTable 4. Racking domain research comparison.\\nResearch Architecture Dataset Size Accuracy FPS\\n[95] MobileNet-V2 19,717 92.7% -----\\n[96] YOLO-v7 2095 91.1% 19\\n[97] Mask-RCNN 75 93.45% -----\\n4. Discussion\\nThe YOLO family of object detectors has had a signiﬁcant impact on improving the\\npotential of computer vision applications. Right from the onset, i.e., the release of the\\nYOLO-v1 in 2015, signiﬁcant breakthroughs were introduced. YOLO-v1 became the ﬁrst\\narchitecture combining the two conventionally separate tasks of bounding box prediction\\nand classiﬁcation into one. YOLO-v2 was released in the following year, introducing archi-\\ntectural improvements and iterative improvements, such as batch normalization, higher\\nresolution, and anchor boxes. In 2018, YOLO-v3 was released, an extension of previous\\nvariants with enhancements including the introduction of objectness scores for bounding\\nbox predictions added connections for the backbone layers and the ability to generate\\npredictions at three different levels of granularity, leading to improved performance on\\nsmaller object targets.\\nAfter a short delay, YOLO-v4 was released in April 2020, becoming the ﬁrst variant of\\nthe YOLO family not to be authored by the original author Joseph Redmon. Enhancements\\nincluded improved feature aggregation, gifting of the ‘bag of freebies’, and the mish\\nactivation. In a matter of months, YOLO-v5 entered the computer vision territory, becoming\\nthe ﬁrst variant to be released without being accompanied by a paper release. YOLO-v5\\nbased on PyTorch, with an active GitHub repo further delineated the implementation\\nprocess, make it accessible to a wider audience. Focused on internal architectural reforms,\\nYOLO-v6 authors redesigned the backbone (EfﬁcientRep) and neck (Rep-PAN) modules,\\nwith an inclination toward hardware efﬁciency. Additionally, anchor-free and the concept\\nof decoupled head was introduced, implying additional layers for feature separation from\\nthe ﬁnal head, which is empirically shown to improve the overall performance. The authors\\nof YOLO-v7 also focused on architectural reforms, considering the amount of memory\\nrequired to keep layers within memory and the distance required for gradients to back-\\npropagate, i.e., shorter gradients, resulting in enhanced learning capacity. For the ultimate\\nlayer aggregation, the authors implemented E-ELAN, which is an extension of the ELAN\\ncomputational block. The advent of 2023 introduced the latest version of the YOLO family,\\nYOLO-v8, which was released by Ultralytics. With an impending paper release, initial\\ncomparisons of the latest version against predecessors have shown promising performance\\nwith respect to throughput when compared to similar computational parameters.\\n4.1. Reason for Rising Popularity\\nTable 5 presents a summary of the reviewed YOLO variants based on the underlying\\nframework, backbone, average-precision (AP), and key contributions. It can be observed\\nfrom Table 3 that as the variants evolved there was a shift from the conservative Darknet\\nframework to a more accessible one, i.e., PyTorch. The AP presented here is based on\\nCOCO-2017 [ 63] with the exception of YOLO-v1/v2, which are based on VOC-2017 [ 39].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5efb5ab-b6dc-4481-b1be-c7f439a32ccf', embedding=None, metadata={'page_label': '20', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 20 of 25\\nCOCO-2017 [ 63] consists of over 80 objects designed to represent a vast array of regularly\\nseen object. It contains 121,408 images resulting in 883,331 object annotations with median\\nimage ratio of 640 ×480 pixels. It is important to note that the overall accuracy along with\\ninference capacity depends on the deployed design/training strategies, as demonstrated in\\nthe industrial surface detection section.\\nTable 5. Abstract variant comparison.\\nVariant Framework Backbone AP (%) Comments\\nV1 Darknet Darknet-24 63.4 Only detect a maximum of two objects in the same grid.\\nV2 Darknet Darknet-24 63.4Introduced batch norm, k-means clustering for anchor boxes.\\nCapable of detecting > 9000 categories.\\nV3 Darknet Darknet-53 36.2Utilized multi-scale predictions and spatial pyramid pooling\\nleading to larger receptive ﬁeld.\\nV4 Darknet CSPDarknet-53 43.5 Presented bag-of-freebies including the use of CIoU loss.\\nV5 PyTorch Modiﬁed CSPv7 55.8First variant based in PyTorch, making it available to a wider\\naudience. Incorporated the anchor selection processes into\\nthe YOLO-v5 pipeline.\\nV6 PyTorch EfﬁcientRep 52.5Focused on industrial settings, presented an anchor-free\\npipeline. Presented new loss determination mechanisms\\n(VFL, DFL, and SIoU/GIoU).\\nV7 PyTorch RepConvN 56.8Architectural introductions included E-ELAN for faster\\nconvergence along with a bag-of-freebies including\\nRepConvN and reparameterization-planning.\\nV8 PyTorch YOLO-v8 53.9Anchor-free reducing the number of prediction boxes whilst\\nspeeding up non-maximum suppression. Pending paper for\\nfurther architectural insights.\\nThe AP metric consists of precision-recall (PR) metrics, deﬁning of a positive prediction\\nusing Intersection over Union, and the handling of multiple object categories. AP provides\\na balanced overview of PR based on the area under the PR curve. IoU facilitates the\\nquantiﬁcation of similarity between predicted kpand ground truth kgbounding boxes as\\nexpressed in (8):\\nIoU =area(\\nkp∩kg)\\narea(\\nkp∪kg) (8)\\nThe rise of YOLO can be attributed to two factors. First, the fact that the architectural\\ncomposition of YOLO variants is compatible for one-stage detection and classiﬁcation\\nmakes it computationally lightweight with respect to other detectors. However, we feel\\nthat efﬁcient architectural composition by itself did not drive the popularity of the YOLO\\nvariants, as other single-stage detectors, such as MobileNets, also serve a similar purpose.\\nThe second reason is the accessibility factor, which was introduced as the YOLO\\nvariants progressed, with YOLO-v5 being the turning point. Expanding further on this\\npoint, the ﬁrst two variants were based on the Darknet framework. Although this pro-\\nvided a degree of ﬂexibility, accessibility was limited to a smaller user base due to the\\nrequired expertise. Ultralytics, introduced YOLO-v5 based on the PyTorch framework,\\nmaking the architecture available for a wider audience and increasing the potential domain\\nof applications.\\nAs evident from Table 6, the migration to a more accessible framework coupled with\\narchitectural reforms for improved real-time performance sky-rocketed. At present, YOLO-\\nv5 has 34.7 k stars, a signiﬁcant lead compared to its predecessors. From implementation,\\nYOLO-v5 only required the installation of lightweight python libraries. The architectural\\nreforms indicated that the model training time was reduced, which in turn reduced the ex-\\nperimentation cost attributed to the training process, i.e., GPU utilization. For deployment\\nand testing purposes, researchers have several routes, such as individual/batch images,\\nvideo/webcam feeds, in addition to simple weight conversion to ONXX weights for edge\\ndevice deployment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f209161-551e-4c30-940c-a95a6b39ab4e', embedding=None, metadata={'page_label': '21', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 21 of 25\\nTable 6. GitHub popularity comparison.\\nYOLO Variant Stars (K)\\nV3 9.3\\nV4 20.2\\nV5 34.7\\nV6 4.6\\nV7 8.4\\nV8 2.9\\n4.2. YOLO and Industrial Defect Detection\\nManifestations of the fourth industrial revolution can be observed at present in an\\nad-hoc manner, spanning across various industries. With respect to the manufacturing\\nindustry, this revolution can be targeted at the quality inspection processes, which are\\nvital for assuring efﬁciency and retaining client satisfaction. When focusing on surface\\ndefect detection, as alluded to earlier, the inspection requirements can be more stringent\\nas compared to other applications. This is due to many factors, such as the fact that the\\ndefects may be extremely small, requiring external spectral imaging to expose defects prior\\nto classiﬁcation and due to the fact that the operational setting of the production line may\\nonly provide a small-time window within which inference must be carried out.\\nConsidering the stringent requirements outlined above and benchmarking against the\\nprinciples of YOLO family of variants, forms the conclusion that the YOLO variants have the\\npotential to address both real-time, constrained deployment and small-scale defect detec-\\ntion requirements of industrial-based surface defect detection. YOLO variants have proven\\nreal-time compliance in several industrial environments as shown in [81,84,85,90,95] . An\\ninteresting observation arising from the industrial literature reviewed is the ability for users\\nto modify the internal modules of YOLO variants in order to take care of their speciﬁc ap-\\nplication needs without compromising on real-time compliance, for example [81,87,91,92] ,\\nintroducing attention-mechanisms for accentuation of defective regions.\\nAn additional factor, found within the later YOLO variants is sub-variants for each\\nbase architecture, i.e., for YOLO-v5 variants including YOLO-v5-S/M/L, this corresponds\\nto different computational loads with respect to the number of parameters. This ﬂexibility\\nenables researchers to consider a more ﬂexible approach with the architecture selection\\ncriteria based on the industrial requirements, i.e., if real-time inference is required with less\\nemphasis on optimal mAP , a lightweight variant can be selected, such as YOLO-v5-small\\nrather than YOLO-v5-large.\\n5. Conclusions\\nIn conclusion, this work is the ﬁrst of its type focused on documenting and reviewing\\nthe evolution of the most prevalent single-stage object detector within the computer vision\\ndomain. The review presents the key advancements of each variant, followed by imple-\\nmentation of YOLO architectures within various industrial settings focused on surface\\nautomated real-time surface defect detection.\\nFrom the review, it is clear as the YOLO variants have progressed, latter versions in\\nparticular, YOLO-v5 has focused on constrained edge deployment, a key requirement for\\nmany manufacturing applications. Due to the fact that there is no copyright and patent\\nrestrictions, research anchored around the YOLO architecture, i.e., real-time, lightweight,\\naccurate detection, can be conducted by any individual or research organization, which has\\nalso contributed to the prevalence of this variant.\\nWith YOLO-v8 released in January 2023, showing promising performance with respect\\nto throughput and computational load requirements, it is envisioned that 2023 will see\\nmore variants released by previous or new authors focused on improving the deployment\\ncapacity of the architectures with respect to constrained deployment environments.\\nWith research organizations, such as Ultralytics and Meituan Technical Team taking\\na keen interest in the development of YOLO architectures with a focus on edge-friendly', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc11b85c-8b96-4135-9326-36c37ede17a8', embedding=None, metadata={'page_label': '22', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 22 of 25\\ndeployment, we anticipate further technological advancements in the architectural footprint\\nof YOLO. To cater for constrained deployment, these advancements will need to focus on\\nenergy conservation whilst maintaining high inference rates. Furthermore, we envision\\nthe proliferation of YOLO architectures into production facilities to help with quality\\ninspection pipelines as well as providing stimulus for innovative products as demonstrated\\nby [96] with an automated pallet racking inspection solution. Along with integration\\ninto a diverse set of hardware and IoT devices, YOLO has the potential to tap into new\\ndomains where computer vision can assist in enhancing existing processes whilst requiring\\nlimited resources.\\nFunding: This research received no external funding.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nReferences\\n1. Zhang, B.; Quan, C.; Ren, F. Study on CNN in the recognition of emotion in audio and images. In Proceedings of the 2016\\nIEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), Okayama, Japan, 26–29 June 2016.\\n[CrossRef]\\n2. Pollen, D.A. Explicit neural representations, recursive neural networks and conscious visual perception. Cereb. Cortex 2003 ,13,\\n807–814. [CrossRef] [PubMed]\\n3. Using artiﬁcial neural networks to understand the human brain. Res. Featur. 2022 . [CrossRef]\\n4. Improvement of Neural Networks Artiﬁcial Output. Int. J. Sci. Res. (IJSR) 2017 ,6, 352–361. [CrossRef]\\n5. Dodia, S.; Annappa, B.; Mahesh, P .A. Recent advancements in deep learning based lung cancer detection: A systematic review.\\nEng. Appl. Artif. Intell. 2022 ,116, 105490. [CrossRef]\\n6. Ojo, M.O.; Zahid, A. Deep Learning in Controlled Environment Agriculture: A Review of Recent Advancements, Challenges and\\nProspects. Sensors 2022 ,22, 7965. [CrossRef] [PubMed]\\n7. Jarvis, R.A. A Perspective on Range Finding Techniques for Computer Vision. IEEE Trans. Pattern Anal. Mach. Intell. 1983 ,P AMI-5 ,\\n122–139. [CrossRef]\\n8. Hussain, M.; Bird, J.; Faria, D.R. A Study on CNN Transfer Learning for Image Classiﬁcation. 11 August 2018. Available online:\\nhttps://research.aston.ac.uk/en/publications/a-study-on-cnn-transfer-learning-for-image-classiﬁcation (accessed on 1 January\\n2023).\\n9. Yang, R.; Yu, Y. Artiﬁcial Convolutional Neural Network in Object Detection and Semantic Segmentation for Medical Imaging\\nAnalysis. Front. Oncol. 2021 ,11, 638182. [CrossRef]\\n10. Haupt, J.; Nowak, R. Compressive Sampling vs. Conventional Imaging. In Proceedings of the 2006 International Conference on\\nImage Processing, Las Vegas, NV , USA, 26–29 June 2006; pp. 1269–1272. [CrossRef]\\n11. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\\nconvolutional neural networks. Pattern Recognit. 2018 ,77, 354–377. [CrossRef]\\n12. Perez, H.; Tah, J.H.M.; Mosavi, A. Deep Learning for Detecting Building Defects Using Convolutional Neural Networks. Sensors\\n2019 ,19, 3556. [CrossRef]\\n13. Hussain, M.; Al-Aqrabi, H.; Hill, R. PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection\\nwithin a Photovoltaic Manufacturing Facility. Energies 2022 ,15, 8667. [CrossRef]\\n14. Hussain, M.; Dhimish, M.; Holmes, V .; Mather, P . Deployment of AI-based RBF network for photovoltaics fault detection\\nprocedure. AIMS Electron. Electr. Eng. 2020 ,4, 1–18. [CrossRef]\\n15. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Parkinson, S. Exudate Regeneration for Automated Exudate Detection in\\nRetinal Fundus Images. IEEE Access 2022 . [CrossRef]\\n16. Hussain, M.; Al-Aqrabi, H.; Hill, R. Statistical Analysis and Development of an Ensemble-Based Machine Learning Model for\\nPhotovoltaic Fault Detection. Energies 2022 ,15, 5492. [CrossRef]\\n17. Singh, S.A.; Desai, K.A. Automated surface defect detection framework using machine vision and convolutional neural networks.\\nJ. Intell. Manuf. 2022 ,34, 1995–2011. [CrossRef]\\n18. Weichert, D.; Link, P .; Stoll, A.; Rüping, S.; Ihlenfeldt, S.; Wrobel, S. A review of machine learning for the optimization of\\nproduction processes. Int. J. Adv. Manuf. Technol. 2019 ,104, 1889–1902. [CrossRef]\\n19. Wang, J.; Ma, Y.; Zhang, L.; Gao, R.X.; Wu, D. Deep learning for smart manufacturing: Methods and applications. J. Manuf. Syst.\\n2018 ,48, 144–156. [CrossRef]\\n20. Weimer, D.; Scholz-Reiter, B.; Shpitalni, M. Design of deep convolutional neural network architectures for automated feature\\nextraction in industrial inspection. CIRP Ann. 2016 ,65, 417–420. [CrossRef]\\n21. Kusiak, A. Smart manufacturing. Int. J. Prod. Res. 2017 ,56, 508–517. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93657341-9f3d-4b77-aec7-f40af5adde4c', embedding=None, metadata={'page_label': '23', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 23 of 25\\n22. Yang, J.; Li, S.; Wang, Z.; Dong, H.; Wang, J.; Tang, S. Using Deep Learning to Detect Defects in Manufacturing: A Comprehensive\\nSurvey and Current Challenges. Materials 2020 ,13, 5755. [CrossRef]\\n23. Soviany, P .; Ionescu, R.T. Optimizing the Trade-Off between Single-Stage and Two-Stage Deep Object Detectors using Image\\nDifﬁculty Prediction. In Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for\\nScientiﬁc Computing (SYNASC), Timisoara, Romania, 20–23 September 2018. [CrossRef]\\n24. Du, L.; Zhang, R.; Wang, X. Overview of two-stage object detection algorithms. J. Phys. Conf. Ser. 2020 ,1544 , 012033. [CrossRef]\\n25. Sultana, F.; Suﬁan, A.; Dutta, P . A Review of Object Detection Models Based on Convolutional Neural Network. In Advances in\\nIntelligent Systems and Computing ; Springer: Singapore, 2020; pp. 1–16. [CrossRef]\\n26. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single shot multibox detector. In Proceedings of\\nthe Computer Vision—ECCV 2016, Amsterdam, The Netherlands, 11–14 October 2016; pp. 21–37. [CrossRef]\\n27. Fu, C.Y.; Liu, W.; Ranga, A.; Tyagi, A.; Berg, A.C. DSSD: Deconvolutional Single Shot Detector. arXiv 2017 , arXiv:1701.06659.\\n28. Cheng, X.; Yu, J. RetinaNet with Difference Channel Attention and Adaptively Spatial Feature Fusion for Steel Surface Defect\\nDetection. IEEE Trans. Instrum. Meas. 2020 ,70, 2503911. [CrossRef]\\n29. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Uniﬁed, Real-Time Object Detection. In Proceedings of the\\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788.\\n[CrossRef]\\n30. Wang, Z.J.; Turko, R.; Shaikh, O.; Park, H.; Das, N.; Hohman, F.; Kahng, M.; Chau, D.H.P . CNN Explainer: Learning Convolutional\\nNeural Networks with Interactive Visualization. IEEE Trans. Vis. Comput. Graph. 2020 ,27, 1396–1406. [CrossRef] [PubMed]\\n31. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep convolutional neural networks. Commun. ACM 2017 ,\\n60, 84–90. [CrossRef]\\n32. Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2014 , arXiv:1409.1556.\\n33. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P .; Reed, S.; Anguelov, D.; Rabinovich, A. Going deeper with convolutions. In Proceedings\\nof the Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 12 June 2015.\\n34. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV , USA, 30 June 2016.\\n35. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Region-Based Convolutional Networks for Accurate Object Detection and\\nSegmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2015 ,38, 142–158. [CrossRef]\\n36. Girshick, R. Fast R-CNN. In Proceedings of the International Conference on Computer Vision, Santiago, Chile, 7–13 December\\n2015.\\n37. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. Trans.\\nPattern Anal. Mach. Intell. 2017 ,39, 1137–1149. [CrossRef]\\n38. Vidyavani, A.; Dheeraj, K.; Reddy, M.R.M.; Kumar, K.N. Object Detection Method Based on YOLOv3 using Deep Learning\\nNetworks. Int. J. Innov. Technol. Explor. Eng. 2019 ,9, 1414–1417. [CrossRef]\\n39. Everingham, M.; Van Gool, L.; Williams, C.K.I.; Winn, J.; Zisserman, A. The Pascal Visual Object Classes (VOC) Challenge. Int. J.\\nComput. Vis. 2009 ,88, 303–338. [CrossRef]\\n40. Shetty, S. Application of Convolutional Neural Network for Image Classiﬁcation on Pascal VOC Challenge 2012 dataset.\\narXiv 2016 , arXiv:1607.03785.\\n41. Felzenszwalb, P .F.; Girshick, R.B.; McAllester, D.; Ramanan, D. Object Detection with Discriminatively Trained Part-Based Models.\\nIEEE Trans. Pattern Anal. Mach. Intell. 2009 ,32, 1627–1645. [CrossRef] [PubMed]\\n42. Chang, Y.-L.; Anagaw, A.; Chang, L.; Wang, Y.C.; Hsiao, C.-Y.; Lee, W.-H. Ship Detection Based on YOLOv2 for SAR Imagery.\\nRemote Sens. 2019 ,11, 786. [CrossRef]\\n43. Liao, Z.; Carneiro, G. On the importance of normalisation layers in deep learning with piecewise linear activation units. In\\nProceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), New York, NY, USA, 7–10 March\\n2016. [CrossRef]\\n44. Garbin, C.; Zhu, X.; Marques, O. Dropout vs. batch normalization: An empirical study of their impact to deep learning. Multimed.\\nTools Appl. 2020 ,79, 12777–12815. [CrossRef]\\n45. Li, G.; Jian, X.; Wen, Z.; AlSultan, J. Algorithm of overﬁtting avoidance in CNN based on maximum pooled and weight decay.\\nAppl. Math. Nonlinear Sci. 2022 ,7, 965–974. [CrossRef]\\n46. Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20–25 June 2009.\\n47. Xue, J.; Cheng, F.; Li, Y.; Song, Y.; Mao, T. Detection of Farmland Obstacles Based on an Improved YOLOv5s Algorithm by Using\\nCIoU and Anchor Box Scale Clustering. Sensors 2022 ,22, 1790. [CrossRef]\\n48. Ahmed, M.; Seraj, R.; Islam, S.M.S. The k-means Algorithm: A Comprehensive Survey and Performance Evaluation. Electronics\\n2020 ,9, 1295. [CrossRef]\\n49. Redmon, J. Darknet: Open Source Neural Networks in C. 2013. Available online: https://pjreddie.com/darknet (accessed on\\n1 January 2023).\\n50. Furusho, Y.; Ikeda, K. Theoretical analysis of skip connections and batch normalization from generalization and optimization\\nperspectives. APSIP A Trans. Signal Inf. Process. 2020 ,9, e9. [CrossRef]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85990255-969c-4437-850a-e8451a4ed53c', embedding=None, metadata={'page_label': '24', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 24 of 25\\n51. Machine-Learning System Tackles Speech and Object Recognition. Available online: https://news.mit.edu/machine-learning-\\nimage-object-recognition-918 (accessed on 1 January 2023).\\n52. Bochkovskiy, A.; Wang, C.Y.; Liao HY, M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020 ,\\narXiv:2004.10934.\\n53. Tan, M.; Le, Q. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the International\\nConference on Machine Learning (ICML), Long Beach, CA, USA, 9–15 June 2019.\\n54. Huang, G.; Liu, Z.; Van Der Maaten, L.; Weinberger, K.Q. Densely connected convolutional networks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 4700–4708.\\n55. Lin, T.Y.; Doll ár, P .; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 2117–2125.\\n56. Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path aggregation network for instance segmentation. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18–23 June 2018; pp. 8759–8768.\\n57. He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. IEEE Trans.\\nPattern Anal. Mach. Intell. 2015 ,37, 1904–1916. [CrossRef]\\n58. Zheng, Z.; Wang, P .; Liu, W.; Li, J.; Ye, R.; Ren, D. Distance-IoU Loss: Faster and better learning for bounding box regression. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), New York, NY, USA, 7–12 February 2020.\\n59. Misra, D. Mish: A self regularized nonmonotonic neural activation function. arXiv 2019 , arXiv:1908.08681.\\n60. Yao, Z.; Cao, Y.; Zheng, S.; Huang, G.; Lin, S. Cross-Iteration Batch Normalization. arXiv 2020 , arXiv:2002.05712.\\n61. Ultralytics. YOLOv5 2020. Available online: https://github.com/ultralytics/yolov5 (accessed on 1 January 2023).\\n62. Jocher, G.; Stoken, A.; Borovec, J.; Christopher, S.T.A.N.; Laughing, L.C. Ultralytics/yolov5: v4.0-nn.SiLU() Activations, Weights\\n& Biases Logging, PyTorch Hub Integration. Zenodo 2021 . Available online: https://zenodo.org/record/4418161 (accessed on\\n5 January 2023).\\n63. Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P .; Ramanan, D.; Zitnick, C.L. Microsoft coco: Common objects in context. In\\nProceedings of the European Conference on Computer Vision, Zurich, Switzerland, 6–12 September 2014.\\n64. Tan, M.; Pang, R.; Le, Q.V . EfﬁcientDet: Scalable and Efﬁcient Object Detection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, Seattle, WA, USA, 13–19 June 2020.\\n65. Li, C.; Li, L.; Jiang, H.; Weng, K.; Geng, Y.; Li, L.; Wei, X. YOLOv6: A Single-Stage Object Detection Framework for Industrial\\nApplications. arXiv 2022 , arXiv:2209.02976.\\n66. Ding, X.; Zhang, X.; Ma, N.; Han, J.; Ding, G.; Sun, J. Repvgg: Making vgg-style convnets great again. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 13733–13742.\\n67. Zhang, H.; Wang, Y.; Dayoub, F.; Sunderhauf, N. Varifocalnet: An iou-aware dense object detector. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 8514–8523.\\n68. Li, X.; Wang, W.; Wu, L.; Chen, S.; Hu, X.; Li, J.; Yang, J. Generalized focal loss: Learning qualiﬁed and distributed bounding\\nboxes for dense object detection. Adv. Neural Inf. Process. Syst. 2020 ,33, 21002–21012.\\n69. Gevorgyan, Z. Siou loss: More powerful learning for bounding box regression. arXiv 2022 , arXiv:2205.12740.\\n70. Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; Shen, C. Channel-wise knowledge distillation for dense prediction. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 11–17 October 20221; pp. 5311–5320.\\n71. Solawetz, J.; Nelson, J. What’s New in YOLOv6? 4 July 2022. Available online: https://blog.roboﬂow.com/yolov6/ (accessed on\\n1 January 2023).\\n72. Wang, C.Y.; Bochkovskiy, A.; Liao HY, M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\\narXiv 2022 , arXiv:2207.02696.\\n73. Ge, Z.; Liu, S.; Wang, F.; Li, Z.; Sun, J. YOLOX: Exceeding YOLO series in 2021. arXiv 2021 , arXiv:2107.08430.\\n74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Uniﬁed network for multiple tasks. arXiv 2021 ,\\narXiv:2105.04206.\\n75. Wu, W.; Zhao, Y.; Xu, Y.; Tan, X.; He, D.; Zou, Z.; Ye, J.; Li, Y.; Yao, M.; Dong, Z.; et al. DSANet: Dynamic Segment AggrDSANet:\\nDynamic Segment Aggregation Network for Video-Level Representation Learning. In Proceedings of the MM ’21—29th ACM\\nInternational Conference on Multimedia, Virtual, 20–24 October 2021. [CrossRef]\\n76. Li, C.; Tang, T.; Wang, G.; Peng, J.; Wang, B.; Liang, X.; Chang, X. BossNAS: Exploring Hybrid CNN-transformers with Block-\\nwisely Self-supervised Neural Architecture Search. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision, Online, 11–17 October 2021. [CrossRef]\\n77. Dollar, P .; Singh, M.; Girshick, R. Fast and accurate model scaling. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), Nashville, TN, USA, 20–25 June 2021; pp. 924–932.\\n78. Guo, S.; Alvarez, J.M.; Salzmann, M. ExpandNets: Linear over-parameterization to train compact convolutional networks. Adv.\\nNeural Inf. Process. Syst. (NeurIPS) 2020 ,33, 1298–1310.\\n79. Ding, X.; Zhang, X.; Zhou, Y.; Han, J.; Ding, G.; Sun, J. Scaling up your kernels to 31 ×31: Revisiting large kernel design in CNNs.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 18–24\\nJune 2022.\\n80. Jocher, G.; Chaurasia, A.; Qiu, J. YOLO by Ultralytics. GitHub. 1 January 2023. Available online: https://github.com/ultralytics/\\nultralytics (accessed on 12 January 2023).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a18b59b2-030e-4183-b394-b0ed1185902a', embedding=None, metadata={'page_label': '25', 'file_name': 'yolov8.pdf', 'file_path': '/content/data/yolov8.pdf', 'file_type': 'application/pdf', 'file_size': 6049935, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machines 2023 ,11, 677 25 of 25\\n81. Jin, R.; Niu, Q. Automatic Fabric Defect Detection Based on an Improved YOLOv5. Math. Probl. Eng. 2021 ,2021 , 1–13. [CrossRef]\\n82. NVIDIA Jetson TX2: High Performance AI at the Edge, NVIDIA. Available online: https://www.nvidia.com/en-gb/autonomous-\\nmachines/embedded-systems/jetson-tx2/ (accessed on 30 January 2023).\\n83. NVIDIA TensorRT. NVIDIA Developer. 18 July 2019. Available online: https://developer.nvidia.com/tensorrt (accessed on\\n5 January 2023).\\n84. Dlamini, S.; Kao, C.-Y.; Su, S.-L.; Kuo, C.-F.J. Development of a real-time machine vision system for functional textile fabric defect\\ndetection using a deep YOLOv4 model. Text. Res. J. 2021 ,92, 675–690. [CrossRef]\\n85. Lin, G.; Liu, K.; Xia, X.; Yan, R. An Efﬁcient and Intelligent Detection Method for Fabric Defects based on Improved YOLOv5.\\nSensors 2022 ,23, 97. [CrossRef] [PubMed]\\n86. Liu, Z.; Tan, Y.; He, Q.; Xiao, Y. SwinNet: Swin Transformer Drives Edge-Aware RGB-D and RGB-T Salient Object Detection. IEEE\\nTrans. Circuits Syst. Video Technol. 2021 ,32, 4486–4497. [CrossRef]\\n87. Zhang, M.; Yin, L. Solar Cell Surface Defect Detection Based on Improved YOLO v5. IEEE Access 2022 ,10, 80804–80815. [CrossRef]\\n88. Binomairah, A.; Abdullah, A.; Khoo, B.E.; Mahdavipour, Z.; Teo, T.W.; Noor, N.S.M.; Abdullah, M.Z. Detection of microcracks\\nand dark spots in monocrystalline PERC cells using photoluminescene imaging and YOLO-based CNN with spatial pyramid\\npooling. EPJ Photovolt. 2022 ,13, 27. [CrossRef]\\n89. Sun, T.; Xing, H.; Cao, S.; Zhang, Y.; Fan, S.; Liu, P . A novel detection method for hot spots of photovoltaic (PV) panels using\\nimproved anchors and prediction heads of YOLOv5 network. Energy Rep. 2022 ,8, 1219–1229. [CrossRef]\\n90. Yang, D.; Cui, Y.; Yu, Z.; Yuan, H. Deep Learning Based Steel Pipe Weld Defect Detection. Appl. Artif. Intell. 2021 ,35, 1237–1249.\\n[CrossRef]\\n91. Ma, Z.; Li, Y.; Huang, M.; Huang, Q.; Cheng, J.; Tang, S. A lightweight detector based on attention mechanism for aluminum strip\\nsurface defect detection. Comput. Ind. 2021 ,136, 103585. [CrossRef]\\n92. Shi, J.; Yang, J.; Zhang, Y. Research on Steel Surface Defect Detection Based on YOLOv5 with Attention Mechanism. Electronics\\n2022 ,11, 3735. [CrossRef]\\n93. CEP , F.A. 5 Insightful Statistics Related to Warehouse Safety. Available online: www.damotech.com (accessed on 11 January 2023).\\n94. Armour, R. The Rack Group. Available online: https://therackgroup.com/product/rack-armour/ (accessed on 12 January 2023).\\n95. Hussain, M.; Chen, T.; Hill, R. Moving toward Smart Manufacturing with an Autonomous Pallet Racking Inspection System\\nBased on MobileNetV2. J. Manuf. Mater. Process. 2022 ,6, 75. [CrossRef]\\n96. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Alsboui, T. Domain Feature Mapping with YOLOv7 for Automated Edge-Based\\nPallet Racking Inspections. Sensors 2022 ,22, 6927. [CrossRef] [PubMed]\\n97. Farahnakian, F.; Koivunen, L.; Makila, T.; Heikkonen, J. Towards Autonomous Industrial Warehouse Inspection. In Proceedings of\\nthe 2021 26th International Conference on Automation and Computing (ICAC), Portsmouth, UK, 2–4 September 2021. [CrossRef]\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "L8et2P85eVRz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_wrapper_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taRIIjlNeVOl",
        "outputId": "9f0e78db-37b7-4e8a-cfea-924ea2695027"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='<|USER|>{query_str}<|ASSISTANT|>')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASHDE-JqeVMJ",
        "outputId": "82b5a289-48b3-41e4-cfeb-c7a19d6125fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "id": "Kz235zWAeVJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "a8f9cb2b35e44e149548d50431a23b9c",
            "ee377c4143c743d581c4db4d27086f35",
            "068d08ccab304b47a124b82f5115ff7b",
            "30881a608e15467eaa3cfc6ea64acb38",
            "e03ca9aa203f4c22bcd4c3a261aca09a",
            "62b354dcfee64d7bb6646a6d98284114",
            "7ba1a064b72b4c92a58aa2612f29c042",
            "744b8dd978034ffe81df1dbc0194e82f",
            "bf239e83cad44d96ab00fc7ce614231a",
            "6d6ab8f7656646c9aa2a250d3453769e",
            "8dcafbc1fd6740038647371aba4c607b",
            "fd47cf7391be4c43b71e5b3fd9938f48",
            "4a5c98f078984dc4a5ccb459e467a9cd",
            "cad63e67805441fca7af6c051936f2bd",
            "9d791d49d4794628aa8a8c1cc835e11d",
            "eff76c7f4fba4f8da19658b5a7b682e0",
            "43c5c3cd0d8f4a469b1aee6154994abb",
            "6432bdf4c2e541e29a12f995f202a5f0",
            "6b814d97acd74171a9181dada5f3a775",
            "0d51d9e867234ebb991a07f31551ef87",
            "817b002cb9a249a58a4a11c9626799e0",
            "c25c36b4119b4fdab0656a1356f73c46",
            "1c6e9ca76a844285aa773e6f9b02b0ce",
            "16292ece9b584400b66f151a4523fe6a",
            "1183c292648544babe1b032430db9cbd",
            "a0c4e42b15144aa493dd109f8d394357",
            "888eefcdca6544b9ab9f4d7df6a76ea2",
            "a59094d1da7d4aaeb3ff82d227229c13",
            "9dd483b5e5f247d3a47ce4512de7215f",
            "4794d480ad884c88a002c0660d4c8583",
            "62763a1b14cc4f4781a1baf18de43b94",
            "5bef0b440ba043348c57f2bda035f395",
            "312882b5050b4cdda0636a9751016e62",
            "29243d0f59e84502a3e0643446d589f5",
            "22abe88b308a4de987423b3ea0fe6128",
            "5286134832a546eb950166f950a94e66",
            "3b52a4ec85114618b64a352b3e41bc1c",
            "f04cea22cfa54a17af9760648cf09b2d",
            "0577905a762f4412bb4260bfbd1e0e9d",
            "79e7b3cfc63b4c00b7d560987f1f2bb0",
            "97668a9b92c14c2c8ad6ff622810f25e",
            "654f62c9118344d793b2f736c1e3fede",
            "0781aceca69140ed966dab4272b71b1f",
            "c2aafdcae82440aeaeae09ebf51fdcef",
            "a3fc4253802d42ed8ecdf91275ecb715",
            "87ee448b639b43d7a980251c571290be",
            "01fe733dfd254bc7a914a079f2db14cd",
            "541abf8e6a614ec08a919c88d74fe947",
            "0f9bb3e5892e4b64807f99c1514540d3",
            "98cd0e428be8448d8d012ecd35384de9",
            "7536299d59a34c8bb70017b08aae482d",
            "b1b0006d41fb42199873fb3cdddf5145",
            "0370cff182d4415887e61c47483b640c",
            "59b43a7dc05c4b718f24477011a0f537",
            "a980963378294f2aacc905054e702ba9"
          ]
        },
        "outputId": "9d0a70be-bc9e-4dc4-afa9-c2469980ce6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8f9cb2b35e44e149548d50431a23b9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd47cf7391be4c43b71e5b3fd9938f48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c6e9ca76a844285aa773e6f9b02b0ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29243d0f59e84502a3e0643446d589f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3fc4253802d42ed8ecdf91275ecb715"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "b22f5ff3da3041c1ae9482e42e2b3bb7",
            "1f377580690646969b500b24ecb37edb",
            "1010ab26744643f4b88a904c0784c941",
            "f0c94cdc0bd547f7b32268344013731c",
            "2fa5af096e5f430391e048500c36a35d",
            "d3119f5598f64701b01af690b235223b",
            "7c7ce26c9e044398b657e624c1b6a5d7",
            "e088fe70ac994d6e94ecfafba101a8ab",
            "0d8d15bd031f4b379335b528f315d2cd",
            "fdf1f7129fa943f39996ca6f799b7673",
            "7b54a04e6d39450391929b07c3dd365e",
            "c216ba59549d491b9ceb8a42a50e6bbb",
            "138397e453474c20a6553bf238a43024",
            "5df041451eb640ee92533558c952fcf6",
            "dcfb39184c6d43008d836944b0ddad34",
            "ddebd729c9074bb695c6a91bcbd578b3",
            "eadc72a7756f4b52bdb85ecdcbc2d6ed",
            "1a44834fd1b945548ef15839f0ad2aaf",
            "63bf699a980643e4851dd69b6f4b0b6b",
            "d7253b9ac1764a058259fb7d99f903ec",
            "aaee2f8634434291a2c92151c577d437",
            "7c7c0d1b020b40caabfe2252739c90eb",
            "f36014966226472e898207cc0fb65ea1",
            "eeac7543c8f94f29827e51595a10b476",
            "7a364f1e8224471eab7108221eb5cd46",
            "8fd0b85371284cb497b1f4e42a3c811f",
            "b004847dcb6640849625802a052bad0e",
            "46ed0e2e780f463bbe35ce84be434398",
            "b4cb1adf7273413d9a1dc3cbbdb1baa6",
            "dba4fdf314404cf290f71f98f33a2c6f",
            "07c4564089b94a01932223b3fad07d51",
            "d0d53c60a032425dbfb6058bf0849fd1",
            "3be665357b9f43829e41bd2763a741f7",
            "973478734a784aef912738f43551725e",
            "e226bfc1298444d28456652d9b2efa1a",
            "ca8fbec8e5834421bf44a337b773714b",
            "2d82f65fb3b24293b57bc3d44ef482a2",
            "3c7d8ca869e840f8bcaacc7447220f7b",
            "837d061475854fae9fbd57c548c494c4",
            "a60a523e684b46c4ad2f81313d0b2640",
            "ea0c97700143494abb4d88a0b19294fb",
            "6ad1479208f848e2b4a46b21b596e377",
            "c6e462cbe418476bbb677f02a4e4dd78",
            "7246c70d878b4048a3d63b775a2a54e9",
            "270ea055b2fb4848ab63a62e3a85392a",
            "fabac93ec5fc460790dbaa33b2122717",
            "1580ec29b0a84bd4914ad893c87a5353",
            "8e93eba9bfd34f85bb29609a85b49174",
            "087f2d29ac984a678eb4a34921e4f329",
            "edb91ae58b6f4c2b9e95e3ab38b0c19b",
            "c7459b927f40463fbd5283a8acd4dc69",
            "111bc504de2042b499b8a6c76c6d2852",
            "c52292951dd44354a375da554a492977",
            "e3460908596d4292bfd90dad5e41dee5",
            "a198c4d4c2364b2f85c1cb332f90d948",
            "6afb567b09aa4c19864565ba2ccf0fb2",
            "4172cf7346114239ae2fa50972fc6c33",
            "ea21e8e9841c48e4bbafbbd041a21d33",
            "0ef976de84914e548a7d2e059017234d",
            "bc495d0b824c4045be489eb031d31dde",
            "5ab46b43926b4b049c4c7b245c652cbd",
            "6e8ecfadcdce4e968aae7b3c15af1474",
            "5e4ba885a4614efaa9862fc0d46dd17f",
            "ff9e44e684804c5cae0aef1de23af464",
            "f3312663d1f945a990039420f05771fc",
            "65109c01e55b4c19b8395f92abebde6e",
            "2fdb571d5cf14219857669bc8cff59ec",
            "c2794bc9bc5f4a96842d528f606bfa73",
            "20b551b9cd41492cade95e1b9e0074cb",
            "530cd26d2cb04ad09d5d47eb413dc8e3",
            "1e189a54e5124782a8365ff2afc0b1ca",
            "960201e740f845debc988c5ce2d74629",
            "d52022dbe13f4f6dbb0f48f501ce6a76",
            "95adb1ea43624144b160dd3a864a8cc7",
            "810ab011bb8641cf8e3c86bef3605b8f",
            "88eef6b529cd4578a37c1d19161f4ca7",
            "58cfe383e88c4f5eb12612c280472e46",
            "9fb4b330c6a745a48acf4da1c141e1fe",
            "d4612d84e35c42719139e4f008c5d615",
            "81ba92c0a7084ff983371dabf58296cb",
            "3d3da872a21d47b387d9157472c1539e",
            "39abb4598e614585b028d8a1bd47fba6",
            "30b7851b24704cc6b1bdb0b3c1e5ef73",
            "2355b8a45eab4db0ac11c4044c20c29f",
            "9ce683e6a2ca4dbb898b2c32ea924cfc",
            "5731bc50a9114cca95f6acfef67c97c1",
            "df4d214f733845dbb1f3bd3bbe08e8ca",
            "a5a1e570e1c844d5a5a9d61ddd9c624a",
            "7817415f639d493683fb8247f87da3a4",
            "b6aefadaa9254db586847458f89b39b3",
            "48335256df304f75b1b4d3e93329b09a",
            "79e007c3ff5d4af8969bc9ce9c616e2a",
            "56ec0c13ffa9446a88e8c78e6f175689",
            "278dc7d1557846ada54a411c5c0a4c53",
            "9303acd941f7402e85b487a9f9ccdfe4",
            "201a6043ae004758818f1186f1d2a18e",
            "1dc9a157b7004c478255a2a5be64c959",
            "fba8176d60074d2cbf2e122cda888013",
            "5edbcca9151e402b99e9a6b0ae70ec4f",
            "72bc32c5075540d788e55cbc187aeeea",
            "0b179c0f498343628e4535ab4714f6ac",
            "c810d2cfbd964193a42272d82be9805f",
            "2a75ea8d37b842c1b03a9f44903ddb78",
            "4b77186c49e84bb1bbc4ab25a74c9f8d",
            "5568148f20d84882b25e46d460fac08f",
            "61923eac48fc4375b9b8e42c26d5c4ca",
            "b7b0cea596254a43b8e0bb0b62ac7bbe",
            "066d3a82e24b4e7aa32ef05502b36145",
            "28d69525baab4b04bc78e350a99ba3fb",
            "a5b67502488a4a4bac9c67f4f7097417",
            "7f656eeada4b48979c66ef417b66f434",
            "e68ee4798b81440b810c9b95343b5ee2",
            "29c9784e30d9411896b951b680fa4078",
            "6eae7883f179433bae03cac7e1f2f6ac",
            "f3c8ff9ca9004f41b97e742239441528",
            "0f4d0d8fefdd4508a1ca2fd0bd01ba85",
            "b482a9bde74c431f9733e21eb66636e1",
            "fff559e7feeb4e99a3726022d09caea6",
            "ff441e52ba0f4a5a87264f2b1b60a034",
            "c53b626023ac49e481433db38b53c8d8",
            "32494e3ad96344739e6dfc41b18dd100"
          ]
        },
        "id": "jmoOM6DYjdKm",
        "outputId": "1be923bf-eb43-42a2-c2dd-1da5a51ffd95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b22f5ff3da3041c1ae9482e42e2b3bb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c216ba59549d491b9ceb8a42a50e6bbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f36014966226472e898207cc0fb65ea1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973478734a784aef912738f43551725e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "270ea055b2fb4848ab63a62e3a85392a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6afb567b09aa4c19864565ba2ccf0fb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fdb571d5cf14219857669bc8cff59ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fb4b330c6a745a48acf4da1c141e1fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7817415f639d493683fb8247f87da3a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72bc32c5075540d788e55cbc187aeeea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f656eeada4b48979c66ef417b66f434"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "2uNoA_ZElNpe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "id": "-4LfM2IalQrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41168027-8982-40ca-c59f-b258185ba3b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7834bdb4d960>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7834bdb4d960>, id_func=<function default_id_func at 0x783592b9a3b0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x783591c30970>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7834bdb4d960>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
        "\n",
        "index"
      ],
      "metadata": {
        "id": "C9Qa4gRfoHzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de85408-dff6-4b3c-a013-9751e2f30c2b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7834bdb4f490>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine()\n",
        "\n",
        "response=query_engine.query(\"what is object Detection in yolo\")"
      ],
      "metadata": {
        "id": "Fj89TFc1oHv9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "1J_9JhOjoHtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ba9f08-8545-4436-b9e3-e37750e5a60d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object detection in YOLO is the process of locating and classifying objects within an image or video stream. It involves identifying the position and class of objects within the image, and can be used for a variety of applications such as autonomous driving, surveillance, and robotics. YOLO uses a single neural network to predict bounding boxes around objects in an image, along with the class probabilities of those objects. The network takes the input image and outputs a set of bounding boxes, each with a class probability associated with it. The bounding boxes are then used to identify the location and class of objects within the image.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is YOLO?\")"
      ],
      "metadata": {
        "id": "TKTbc6v0oHqr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "nNxbbiBooS9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab7b663-1f00-4c24-8cdc-afec2eaac61d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO (You Only Look Once) is a real-time object detection system that uses a single neural network to predict bounding boxes around objects in an image. It was introduced in 2016 by Joseph Redmon and Ali Farhadi. YOLO uses a single neural network to predict bounding boxes around objects in an image, rather than using multiple networks for different object types, as in traditional object detection systems. This allows YOLO to detect objects in real-time, making it suitable for applications such as self-driving cars and surveillance systems. YOLO has been shown to be highly accurate and efficient, and has become a popular choice for object detection tasks.\n"
          ]
        }
      ]
    }
  ]
}